\newacronym{ctan}{CTAN}{Comprehensive TeX Archive Network}
\newacronym{faq}{FAQ}{Frequently Asked Questions}
\newacronym{pdf}{PDF}{Portable Document Format}
\newacronym{svn}{SVN}{Subversion}
\newacronym{wysiwyg}{WYSIWYG}{What You See Is What You Get}

\newglossaryentry{texteditor}
{
	name={editor},
	description={A text editor is a type of program used for editing plain text files.}
}

\part{Overview}

\chapter{Introduction}

Biochemistry lies at the root of complex biological systems that describe the machinery of life.
In order to understand how we work, we must first understand the complete cascade of events that begin at the atomic level.
Because biological systems span several scales of space and time as well as scientific fields, such as biology, chemistry, mathematics, or medicine, it is crucial to communicate advances in biochemistry efficiently between experts with different scientific backgrounds.
Moreover, there is also a growing interest from the general audience to understand how living organisms work.

\begin{figure}
	\centering
	\includegraphics[width=0.80\linewidth]{graphics/goodsell_mitochondrion}
	\caption{A painting made by David S. Goodsell of a cross-section revealing the internals of a mitochondrion. The view focuses on the outer and inner membrane of the organelle (green). The genome is shown in yellow, and the remaining proteins are both coloured according to their type and location. Note how the Goodsell arranged the scene to ensure that all the key elements are visible in a single image, and how the visual mapping of shapes and colors contribute to the clarity of the image.}
	\label{fig:david-s}
\end{figure}

Visual communication is undeniably efficient for educating a non-expert audience about how physiological processes function.
A quick glance into physiological textbooks is enough to realize that they would be close to useless without any illustrations.
Illustrations, such as the ones made by David Goodsell, are often used to convey information in such textbooks.
The illustrator likes to depict entire sceneries on the mesoscale levels, as shown in Figure~\ref{fig:david-s}, which would be impossible to observe otherwise in such detail using current microscopy instruments.
His paintings rightfully balance scientific accuracy and clarity, which makes them very popular because they are accessible to a large audience.

The realization of such illustrations, however, is very laborious and requires highly skilled individuals.
The first step of the creation process consists of gathering knowledge from the scientific literature, in order to fully understand the process that is to be depicted.
This task demands a thorough knowledge of biology, as scientific articles are intended to be read by experts and peers.
Based on gathered information, the illustrator will decide how to compose the scene, i.e., which macromolecular structures should be present, where should they be located, in which quantities, and what behaviour should they exhibit.

The second step of the creation process is the drawing. 
It is important not to confuse the work of a scientific illustrator with the work of an artist.
Although they both aim at conveying a message or an idea, the artist has the freedom to hide his message behind a curtain of abstraction.
On the contrary, an illustrator has to convey a message as clearly as possible in order to expose scientific concepts to an uninformed audience.
This concretely means that the illustrator is bound to a set of logical guidelines in terms of composition, lighting, color-coding or storytelling.

While some illustrators prefer working with paper and pencil or 2D composition software such as Adobe Photoshop, the new generation of illustrators grew with 3D rendering and animation packages such as Autodesk Maya, Maxon Cinema4D or Blender, made mainstream by the popularization of digital effects in the movie industry.
The use of such computer aided design tools greatly facilitates the drawing of three dimensional shapes.
Perspective and lighting effects, for instance, are automatically handled by the software, thus leaving artists more time to work on other aspects such as material design, composition, or post-processing.
Since less time is spent working on single images, it is also less cumbersome to produce animated stories. 
Consequently, over the last decade, 3D animation became an increasingly popular means of visual communication for cell biology.

A famous animated educational material is "The Inner Life of the Cell" \cite{inner2006} realized in 2006 by the XVIVO medical illustration studio, which was appointed by Harvard University.
This short animated video beautifully depicts in less than 10 minutes, the logical sequence of events that describe complex biochemical processes of a living cell.
The shape of macromolecules are based on real structural information available from public databases and their behaviour based on the most recent knowledge of cell biology.
Furthermore, environments surrounding each event is also accurately depicted to provide important information about location and scale.
This outstanding work took an entire team of experts and 14 months to produce, which is a good average production time for an educational material of this length.
Unfortunately, on top of being time-consuming, the production of such films is also very costly, which limits the accessibility and availability of such visualization.

Another type of media that has a great potential for educational purposes, is interactive applications.
Compared to movies, interactive titles, such as computer games, are able to keep the player engaged with educational content using the traditional reward system present in many computer games.
Immune Attack~\cite{immune2008} and Sim Cell~\cite{simcell2013} are two famous examples of edutainment titles whose goal is to reveal the functioning of living cells through accomplishment of actions that are part of physiological processes.
Promising new VR devices have also emerged over the last years, and are now paving the way for more exciting and engaging user experiences that could have a great educational outreach.
However, the production of high quality interactive content, similarly to animated films, is also a lengthy and costly enterprise, as technicians and programming experts are also needed in addition to the team.

Interactive applications may also have an educational purpose without necessarily introducing gameplay mechanisms or score-based rewards. 
Interactive map applications, such as Google Earth~\cite{gearth2001}, are a good example.
Unlike static maps, these applications enable on-demand access to specific information.
Through a set of 2D interactions such as zoom, rotate, and pan, or 3D interactions such as tilt, the user is free to navigate to whichever part of earth that he deems interesting.
It also features multiple zoom levels from planets down to the size of building, houses or even cars.
Another strong advantage of the platform is crowd-based collaboration.
Three dimensional data obtained from scans of entire cities can be provided by the municipalities and added to the platform for in-depth city architecture exploration.
Finally, the platform is not only bound to static representation of earth, dynamic data such as traffic or meteorology provided by third party applications can also be added in the platform.
The final outcome is a system that enables omniscient and three dimensional observation of the planet and its dynamics, based on available data.
The educational outreach of this software is undeniable as it transcend all types of media previously used, and provides unconstrained access to multiple types of information at once. 

To our knowledge, the concept of reproducing an observable 3D virtual environment as such has not yet been transposed to the level of an entire cell.
In order to achieve this enterprise one would need access to important data such as the three dimensional structures of macromolecules and greater ensembles that form the compartments and various organelles of the cells.
Cells also carry important functions expressed in biological systems and represented as reaction networks between micro and macromolecules.
This input information is used by scientists to model parts of living cells and produce datasets that describe the dynamic behaviour of molecules such as concentration charts, reactions events and 3D diffusion patterns.
Fortunately, a large amount of biological knowledge is already available via online public databases.
Concerning structural data, the Protein Data Bank~\cite{bernstein1977protein}, for instance, is a project that aims at grouping every known protein structures in a large public data base.
Thanks to this resource, it is trivial to access the atomic structure of a very large number of proteins and use it for illustrative purposes.
Concerning procedural data, Ecocyc~\cite{keseler2005ecocyc}, is another example of a public database that aim at gathering extensive procedural descriptions of the biological systems that are ongoing in the E. Coli bacterium.
Based on these descriptions, biologists have also managed to run partial and whole-cell simulation of the systems occurring inside these species on super computers, and they also made the results publicly available~\cite{karr2014wholecellsimdb}.
Similarly to satellite data, or traffic data in Google Earth, the data present in these databases is steadily updated with most recent scientific knowledge by a large crowd of researchers.

To address the limitations of traditional scientific animations workflows, which are time-consuming and expensive, we envision a new type of solution that would be more streamlined and relying on scientific data.
Since the visualized output would directly derive from the data, it would become much less cumbersome to create new content or update the existing ones with new information based on recent scientific discoveries.
Additionally, we also aim at providing the means to explore the 3D space in real-time across multiple scales, and also to interact with physiological properties of the environment in order to keep the subject even more engaged with the content. 
Despite the large quantity of available data, there is yet no solution that could generate a comprehensible digital cell based on this data, which would be both dynamic and multi-scale.
What is truly needed is a solution that would collect and use all this data and enable real-time visualization and interaction with the showcased models. 
Although data-collection and real-time rendering are important aspects, this ambitious enterprise also comprises many interesting visualization-related challenges.
In this thesis, we present the methodology that we employed to address the various challenges that we have encountered along the way.

\section{Scope and Contributions}



The central vision of this project is to develop a technology that would provide interactive visualization of digitally reproduced cellular lifeforms in order to spread state-of-the-art knowledge of cell biology more easily to the broad audience.
Similarly to map navigation tools, we want to offer boundless exploration through the environment and massive zooming from the tiniest atom to the entire organism, so that the audience can query any region of a cell and learn about it on-demand, see Figure \ref{fig:teasernew}.
We also envision the whole model to be dynamic in order to explain the complex machinery that allow the cells to sustain, function, and reproduce.

The scenario that explains how physiological processes work is far from linear, as these processes are designed to respond to multiple type of environmental changes.
Therefore, we also wish to provide the means to interact with the functions of life in terms of changing environmental conditions in order to observe and learn how the life-form responds to such change. 
To achieve this vision, our approach is to integrate up-to-date biological knowledge from several online databases that contain frequently updated scientific findings related to a particular organism. 
For example, one database will provide us with the structural description on a microscopic spatial scale, another database will provide information on an atomic detail, yet another database will provide us with information on physiology such as reaction networks and life-cycle simulations.
A good starting point for our enterprise would be to begin with small unicellular organisms such as E.coli bacterium or Mycoplasma mycoides.
These are so-called model organisms because we already have an extensive knowledge about their functioning and structure, so the integration of this information into a visual depiction seems within reach. 

In this thesis are presented different research projects that were all driven by this vision.
To illustrate how the different contributions of this PhD project fit together, these are laid out in a flow diagram starting from input data to final visual output, and shown in Figure~\ref{master-diagram}.
The figure also contains a list of relevant first authorship publications referenced in the diagram.
Second authorship contributions are also included in this diagram but are not discussed in this thesis.
The contributions are encapsulated into blocks that correspond to the stages of a classic computer visualization pipeline, and which are described in the following sections.
It is worth mentioning that all the pieces of software associated with these contributions were developed with the same visualization framework, cellVIEW, with the ambition to compile all the work we achieved in a unified solution.
In 2016, cellVIEW was also awarded Best Technical Solution at the Austrian Computer Graphics Awards, during the PIXELvienna conference.
Executable version and source code are freely available and can be downloaded here: https://www.cg.tuwien.ac.at/research/projects/illvisation/cellview/cellview.php.


\begin{figure}
\centering
\includegraphics[width=0.99\linewidth]{graphics/teaser_new}
\caption{Multi-scale visualization of the HIV virus with cellVIEW, a program developed in the course of this PhD project. This image show the massive zooming capabilities from entire organisms down to single atoms, while also revealing intermediate
	 structures such as organelles, compartments and macromolecules.}
\label{fig:teasernew}
\end{figure}


%\begin{figure}
%\centering
%\includegraphics[width=0.7\linewidth]{graphics/Picture4}
%\caption{}
%\label{fig:picture4}
%\end{figure}

\begin{figure*}
	\centering	
	\caption{The flow diagram describing the visualization pipeline which we designed for this PhD project. 
		The contributions of this thesis are placed along the workflow, and the numbering corresponds to the publication listing below the diagram.}
	
	\subfloat
	{
		\centering
		\includegraphics[width=1\linewidth]{"graphics/master_diagram"}
	}	
	\subfloat
	{		
		This thesis is based on the following publications:		
		\begin{list}{}{}			
			\label{fig:list-contributions}
			\item[\textbf{A}]\textbf{Mathieu Le Muzic}, Julius Parulek, Anne-Kristin Stavrum, and Ivan Viola.
			Illustrative Visualization of Molecular Reactions Using Omniscient Intelligence and Passive Agents. 
			In \textit{Computer Graphics Forum}, pages 141–150, 2014. 
			
			\item[\textbf{B}]\textbf{Mathieu Le Muzic}, Manuela Waldner, Julius Parulek, and Ivan Viola.
			Illustrative Timelapse: a Technique for Illustrative Visualization of Particle-based Simulations. 
			In \textit{IEEE Pacific Visualization Symposium} (PacificVis), pages 247–254, 2015.
			
			\item[\textbf{C}]\textbf{Mathieu Le Muzic}, Ludovic Autin, Julius Parulek, and Ivan Viola.
			cellVIEW: a Tool for Illustrative and Multi-scale Rendering of Large Biomolecular Datasets. 
			In \textit{Proceedings of the Eurographics Workshop on Visual Computing for Biology and Medicine}, pages 61–70, 2015.
			
			\item[\textbf{D}]\textbf{Mathieu Le Muzic}, Peter Mindek, Johannes Sorger, Ludovic Autin, David S. Goodsell, and Ivan Viola.
			Visibility Equalizer: Cutaway Visualization of Mesoscopic Biological Models. 
			In \textit{Computer Graphics Forum}, pages 161–170. 2016.				
		\end{list}			
		
		The following article are also related to this thesis:	
		\begin{list}{}{}			
			\item[\textbf{I}]Manuela Waldner, \textbf{Mathieu Le Muzic}, Matthias Bernhard, Werner Purgathofer, and Ivan Viola. 
			Attractive Flicker: Guiding Attention in Dynamic Narrative Visualizations. 
			In \textit{IEEE Transactions on Visualization and Computer Graphics}, pages 256–265, 2014.	
			
			\item[\textbf{II}]Nicholas Waldin, \textbf{Mathieu Le Muzic}, Manuela Waldner, Eduard Gröller, David S. Goodsell, Autin Ludovic, and Ivan Viola. 
			Chameleon: Dynamic Color Mapping for Multi-Scale Structural Biology Models. 
			In \textit{Proceedings of the Eurographics Workshop on Visual Computing for Biology and Medicine}, pages 53–62, 2016.						
		\end{list}		
	}	
	\label{master-diagram}	
\end{figure*}

%\pagebreak

\subsection{Preparing The Raw Data}

In our particular use case, the input data is coming from various sources, and needs to be unified before being visualized.
Structural information of large number of macromolecules, is publicly available via online databases.
Additionally, it is also possible to obtain a valid spatial arrangement of large ensembles of proteins that form mesoscale models for entire cells.
The trajectory and reaction history of individual particles for a given biological system may be obtained by modeling approaches called agent-based modelling.
This data can thus be combined with structural information in order to reproduce a similar visual output as in animated movies.

However, particle-based modeling has a high computational footprint, which prohibits us from interacting with the simulation in real-time since the data must be precomputed.
Quantitative modelling, such as kinetic modeling, is much more lightweight to compute but only provides quantitative and relational information, and the spatial component is entirely missing from the model description.
Therefore, we have developed a new type of solution, which is able to generate 3D particle animations, driven by the results of a quantitative simulation, which is described in \textbf{Paper A}.
The light computational footprint of quantitative models thus allows fast in-situ visualization of particle trajectories and interactions, which was not achievable up to that point.

\subsection{Filtering}

Input data processed by the visualization pipeline consists of positions of single atoms for the macromolecular structures, on the one hand, and the trajectory data for entire groups of molecules, on the other hand.
Because of the chaotic nature of the forces driving the motion of molecules inside living cells, the raw visualization of such data often results in an overly complex visual output, which is challenging to comprehend.
Thus, it is important to filter out irrelevant and redundant features such as excessively erratic motion and occluding elements to reveal important underlying information buried in the chaos.

Trajectory data is often visualized in fast-forward to shorten the viewing of overly long sequences, which also speeds up the motion of individual particles.
Consequently, it is almost impossible to keep track of individual elements and observe key reaction events that enable that same process. 
Indeed, rates of physiological processes operate at a much larger time scale than the movement of individual particles.
With direct visualization, one can observe only one of these scales, either the physiological process or the behaviour of individual particles.
In \textbf{Paper B}, we have investigated how to simultaneously convey two phenomena that reside at different temporal scale levels. 
In particular, we have aimed at developing a technique that can show the complexity of diffusion in fast-forward and simultaneously allows viewers to see physiologically-relevant events that would not be observable at such temporal resolution.

Another issue that derives from densely crowded environments is the presence of a large number of bodies that may obstruct the view to key macromolecules and important reaction events. 
A characteristic of molecular bodies is that many of them actually share the same atomic structure.
Our technique, called Visibility Equalizer, and described in \textbf{Paper D}, allows the user to see inside dense arrangements of proteins, by providing an explicit control over the visibility of entire groups of molecules sharing a similar structure. 
Rather than completely displaying or removing entire sets of proteins, we introduced the concept of fuzzy visibility, which allows reducing the concentration of visible elements of a given type, thus revealing the internals of a cell while preserving important contextual information.

\subsection{Mapping}

The next operation of the pipeline is the mapping, which determines visual properties such as shape or color.
Because the data we want to visualize features multiple scales, from single atoms and up to entire cells, it is important to adapt the visual representation accordingly to ensure an optimal comprehension of the scene at any given zoom level.
A level-of-detail scheme is an optimization technique often used in computer graphics and visualization to accelerate the rendering.
The principle consists of progressively switching between simplified shape proxies as the camera distances itself from objects.
In the case of molecular visualization, level of detail is two fold, on the one hand, it allows to speed up performance, and on the other hand it can also be utilized to filter out high frequency details, as molecules tend to have complex shapes, which may clutter the view when observing molecular landscapes in their entirety.
In \textbf{Paper A} and \textbf{Paper C} we have explored the use of level-of-detail schemes, specifically designed for macromolecular structures. 

Color is a strong visual cue that is extensively used in molecular visualization.
In large scenes comprising many macromolecular elements, color coding can have multiple folds.
It can either be used to discriminate atoms with different physio-chemical properties, such as charge or hydrophobicity, structural properties such as the type of atom, amino-acid, chain, protein, or even spatial properties such as the membership of element to a given subregion of an organism.
Often these properties can only be optimally observed at one single zoom level.
In \textbf{Paper II} we developed a novel dynamic coloring approach that optimises the color coding based on the current zoom level in order to ensure that only the most relevant information is revealed when exploring multi-scale atomic structures.

\subsection{Dynamic Visual Guidance}

Traditional focus+context approaches are commonly utilized to highlight key interactions between molecular agents when observing an ongoing biological system.
However, these may fail when observing the results on large projection displays because they should be effective both for the foveal as well as the peripheral vision, especially when viewing large ensembles of particles that exhibit very chaotic motion patterns.
Therefore, we have investigated how to guide the viewer to interesting events in a dense dynamic scene of interacting molecules that are presented on such type of display.
In \textbf{Paper I}, we have developed a special type of dynamic guidance based on subtle flicker that is effective at guiding the viewer's gaze towards interesting events, is unobtrusive, does not use any visual variables that encode the data, and incurs only a minimal visual modification to the presented scene.
%We have demonstrated the usefulness of the new guidance technique on following a reaction pathway in the context of a dynamic diffusing environment that would normally fully distract the audience.  

\subsection{Rendering}

The rendering is arguably the most crucial part of our visualization pipeline, as it is meant to display challengingly large and dynamic molecular datasets at a high frequency refresh rate.
Realistic atomic structures of entire cells consist of thousands to millions of macromolecules, themselves composed of a few thousands atoms each, resulting in an memory footprint that exceeds the capacity of today's high end graphics hardware.
Fortunately, many molecules present in these scenes share the same structure, which allows us to utilize the concept of instancing.
Every structure of a particular protein macromolecule is stored on the graphics hardware (GPU) only once, while the positions and orientations of the molecules are stored separably.
Instancing is useful for reducing the size of redundant datasets, but also helps to reduce the number of necessary draw calls, which tremendously accelerates the performance when rendering hundreds of thousands of elements. 
When it comes to the geometric representation of a single macromolecule, we model and render it as a set of atom spheres drawn using 2D impostors, which have a much lower vertex count than 3D tessellated spheres for the same visual output.
We also utilize a level-of-detail scheme to dynamically switch between proxies according to the distance of molecules to the camera, which is presented in \textbf{Paper A}.
In \textbf{Paper C}, further acceleration techniques have been designed to optimize the rendering speed even further, such as occlusion culling, as well as a new rendering pipeline dedicated to large and linear fibre structures.

\section{Contributions of Co-authors}

All manuscripts that constitute this thesis were written during the PhD project and the author of this thesis is also their main author. 
The first author has put the scattered thoughts of the entire research team and realized a concrete and meaningful technology out of it.
He was responsible for the development of cellVIEW as well as the prototyped software of the technologies presented in the papers.
Ivan Viola, the main supervisor of this thesis, coauthored all manuscripts. 
Viola is the primary investigator of the research team, and many interesting concepts presented here emerged from his imagination long before the project was even funded. 
He also provided indispensable mentorship, and high level ideas for a fruitful research direction thus contributing to the crystallization of ideas throughout the projects.
\textbf{Paper A, B, C}, were all co-authored by Julius Parulek who helped with the conception and implementation of the rendering pipeline, and he also provided valuable advices and support during the writing of manuscripts.
He also helped porting the prototyped technology to the Unity 3D game engine. 
\textbf{Paper A} received the participation of Anne-Kristin Stavrum who joined the project as a biologist and was given the task to conceive physiological models utilised to prototype the presented technology.
\textbf{Paper B} was also co-authored by Manuela Waldner who helped designing and run the user studies, provided insightful inspiration, and also actively participated in the writing of the article. 
\textbf{Paper C} was co-authored by Ludovic Autin, who conceived the showcased models, helped with the development of cellVIEW, and also provided support and insightful advices.
\textbf{Paper D} was co-authored by Peter Mindek as second first author since both authors were deemed to have contributed equally to the success of this article, this mention is also present in the publication.
\textbf{Paper D} also received additional help from Johannes Sorger for the writing of the manuscript, and the design choices were also influenced by the feedback we received from Ludovic Autin and David S. Goodsell.

\section{Thesis Structure}

This thesis consists of two parts.
The first part summarizes individual contributions and findings and also aims at describing how these single pieces fit together as part of a bigger picture.
The second part contains the published articles.
Chapter~\ref{sec:section2} follows the first introductory chapter with an overview of previous work related to the visualization of structural and systems biology models, with an emphasis on multi-scale visualization. 
A more detailed overview of related works is contained in the individual papers in the second part of this thesis. 
Following the related work, we provide in-depth details about the contributions, starting with the visualization of strictly static and structural information in Chapter~\ref{sec:section3}.
In Chapter~\ref{sec:section4}, we present the work that is concerned with the visualization of large-scale structures enhanced with dynamic procedural information obtained from real scientific data. 
Finally, we conclude the first part of the thesis in Chapter~\ref{sec:section5}.

\chapter{Background and Related Work}
\label{sec:section2}

In biochemistry, there exists two distinct experimental protocols, respectively called dry and wet laboratories.
Wet laboratories are where chemical agents are physically manipulated and then observed.
Dry laboratories are where computational or mathematical methods are employed for the modelling and analysis of biochemical processes. 
Over the last decades, the use of \textit{in-silico} experiments (dry laboratories) have significantly increased due to the development of new software and the decreasing costs of super-computers.
%Due to computational or human limitations, it may be often impractical to accurately simulate a given process in its entirety.
Despite being often criticized for being too approximate, dry laboratory experiments represent a valuable source of information for researchers nonetheless.

In 2013 Martin Karplus, Michael Levitt and Arieh Warshel were awarded the Nobel prize in Chemistry for their work on theoretical modelling for complex chemical systems~\cite{karplus2014development}.
Their work highlights the importance of theoretical modelling as a tool to complement experimental techniques as wet-lab experiments are usually complex and expensive to conduct.
The analysis of theoretical modeling brings researchers the necessary guidance to formulate new hypotheses, which can be later on verified in wet laboratories, thus saving the time and money needed to run too many wet lab experiments.
As a result of the increasing popularity of dry lab experiments, a significant amount of data has already been gathered and produced.

Data is often stored in digital format and may be shared with peers via online databases.
Structural biology and Systems biology are branches of molecular biology that both heavily rely on computational methods.
Structural biology informs us about how things look, i.e., what is the atomic structure of a protein, while systems biology informs us about how things work, i.e., what are the micro and macromolecular interactions that influence the functioning of living organisms.

\section{Visualization of Biological Structures}

Structural biology is the branch of biology that is concerned by the structure of biological macromolecules, such as proteins or DNA, for example, and focuses on understanding the relationship between the structure of molecules and their function.
Data acquisition methods, such as X-ray crystallography, are commonly used to read the atomic structure of proteins, i.e., the positions or atoms, their type, and the type of bonds between them.
Acquired atomic structures are often stored in digital files and shared via public data bases, such as the Protein Data Bank~\cite{bernstein1977protein}, to facilitate collaboration among peers.
This information is then processed to decrypt underlying important structural information, and also used to run molecular dynamics (MD) simulations, which aim at reproducing atomic interactions and forces to observe the actual behaviour of macromolecules over time.
Visualization is an important component of this discipline, because atoms are arranged and assembled in three dimensional space and therefore, a visual representation is often required.
Biologists developed several types of representation to illustrate molecular structures, and are supported by mainstream visualization packages such as VMD~\cite{humphrey1996vmd} or PyMol~\cite{PyMOL}.

\begin{figure}
	
	\centering
	\includegraphics[width=.90\textwidth]{graphics/Picture13}
	
	\caption{Different types of representation used in molecular visualization. (a) The ribbon diagram reveals structural information hidden inside the structure, such as the formation of sheets or helices along the protein chains. (b) The van der Waals representation, also called space filling, renders individual atoms as 3D spheres. (c) The stick model represents the bonds betweens two atoms with lines, but unlike space filling it does not encode the atomic radius. (d) The surface representation wraps the entire molecule with a tight hull that facilitates the detection of cavities that may host important reaction sites.}
	\label{fig:representations}
	
\end{figure}

%\begin{figure}
%	\centering
%	
%	\begin{subfigure}{.5\textwidth}
%		\centering
%		\includegraphics[width=.99\linewidth]{"graphics/protein1"}
%		\caption{}
%		\label{fig:sub1}
%	\end{subfigure}%
%	
%	\begin{subfigure}{.5\textwidth}
%		\centering
%		\includegraphics[width=.99\linewidth]{"graphics/protein2"}
%		\caption{}
%		\label{fig:sub1}
%	\end{subfigure}%
%	
%	\caption{The HIV + blood plasma dataset composed with the visibility equalizer. (a) The inside of the HIV capsid (blue proteins surrounded by yellow capsid proteins) is set a object of interest, which ensure occluders to be removed dynamically from any point of view. (b) Demonstration of the concept of "fuzzy visibility". Above the clipping plane positioned manually the user can decide to override the concentration of displayed elements. Additionally we also propose to render the ghost of the removed elements to reduce visual clutter only the first visible removed molecules are drawn.}
%	\label{fig:test2}
%\end{figure}

A popular representation among structural biologists is the secondary structure or ribbon diagram (Figure \ref{fig:representations}a), which is used to reveal properties of the protein backbones, such as sheets or helices.
The van der Waals surface (Figure \ref{fig:representations}b) is probably the most commonly understood representation and simply shows atoms as spheres whose radius corresponds to the atomic radius.
The simplest representation is the sticks model (Figure \ref{fig:representations}c), where each bond is represented as a line, and color coding is used to indicate the atom type at the line extremities or joints.
Finally, the molecular surface representation (Figure \ref{fig:representations}d) is used to show a continuous surface that closely surrounds atoms of a protein, and that also closes small holes between atoms that are not accessible by small solvent molecules.
This method was first introduced to reveal information that is not salient enough with other types of representations, such as the presence of pockets and cavities buried in the protein structure that can potentially host important reaction sites.
In scientific illustrations, the shape of a protein is an important aspect to convey as it is tightly related to its function.
Therefore the surface or space-filling representations are often preferred, because they communicate shape information more efficiently.
Furthermore, these models can easily be stored as polygon meshes, which are supported by 3D animation packages.
BioBlender~\cite{andrei2012intuitive}, Molecular Maya~\cite{mmaya}, ePMV~\cite{johnson2011epmv}, are examples of plugins for animation packages that were specifically developed to ease the loading and rendering of molecular surface meshes in animation packages.

\begin{figure}
	\centering
	\includegraphics[width=0.85\linewidth]{graphics/cellpack_1}
	\caption{The principle of cellPACK, by courtesy of Ludovic Autin. Firstly, recipes are designed based on available information, such as protein structures, concentrations, distribution and compartment shapes. Then the software reads the recipe and generates a plausible assembly of molecules based on collision constraints, in order to generate a larger structure featuring structural information down to the level of atoms, which would not be possible to acquire otherwise with traditional methods such as X-ray crystallography.}
	\label{fig:cellpack}
\end{figure}

X-ray crystallography is limited because it cannot capture large and complex structures such as organelles, viruses, or cells in their entirety.
Electron microscopy imaging, on the other hand, still does not offer enough resolution to capture individual atoms which make the segmentation task between proteins extremely challenging.
So far, only little is known about spatial arrangement of proteins that form greater structures, and their manual modeling would be a cumbersome and time-consuming task.
To fill the mesoscale gap between atoms and cells, scientists from the Scripps Research Institute have developed cellPACK~\cite{johnson2015cellpack}, a tool to procedurally construct large mesoscale structures, such as entire viruses or cells, at atomic resolution.
cellPACK incorporates the most recent knowledge obtained from biology to generate these models, such as protein structures obtained from crystallography, concentrations and spatial distribution observed \textit{in vitro}, and 3D shape of compartments acquired from electron microscopy.

They summarize all this data in structural descriptions which they call a recipe, which is then used as input to generate entire models of viruses and cells via a packing method based on collisions constraints. 
This concept is depicted in Figure \ref{fig:cellpack}.
Their algorithm is designed to progressively insert molecules inside given compartments.
They use a spatial partitioning scheme to detect overlapping structures and find an appropriate location to insert new shapes, guaranteeing no overlap with previously inserted elements.
As an output, their tool generates a list that contains the position, rotation, and type of all the macromolecules that compose the organism.
Additionally, their method also supports packing fibre data, such as DNA, or RNA, which is stored as spline control points in the resulting file.

The initial goal of cellPACK was to generate valid protein ensembles that form organisms and that also contain atomic data in order to serve as input for large-scale molecular dynamics simulations.
Furthermore, the generated structures can also be loaded in 3D rendering and animation packages for illustration purposes.
%, thus preventing illustrator to manually place and arrange proteins when depicting a larger protein ensemble.
These large models are thus highly valuable to us, as they contain complex data that would have to be modeled manually otherwise.
They are also publicly available and can be easily updated with the most recent knowledge of cell biology.
However, the overwhelmingly large number of elements that may compose these mesoscale structures begin to truly challenge animation packages that were not designed with such constraints in mind.
While it is possible to render still images in very high quality, real-time visualization of these models is simply not possible, even with simplified surface meshes. 
This affects the productivity of those who create the models, as well as those who are using it for illustration purposes, and it also compromises the transition to the next generation of interactive scientific illustrations.

Although the polygon mesh is currently the most common shape representation supported by animation packages and game creation software, it might not always provide the best performance for large and complex datasets.
Indeed, the rendering of highly detailed meshes requires an overwhelmingly large number of polygons which can stress the rendering pipeline and video memory usage.
Reducing the number of polygons for surface meshes can help improving performance significantly, but it also removes important high frequency structural details, and for larger scenes real-time performance requirements are often still not met.
To keep up with the increasing size of atomic datasets, visualization experts developed new cutting edge techniques that do not rely on polygon meshes.
Tarini et al.~\cite{tarini2006ambient} introduced a novel visualization technique inspired from 2D billboards, a popular concept in computer games.
The technique consists of drawing camera-facing 2D sphere impostors rather than tessellated 3D spheres for rendering individual atoms.
As a result, the drawing of a molecule comprising 1000 atoms, for example, requires only 4000 vertices ---4 vertices per atom--- to form the camera facing quads, while polygon meshes would require a number of vertices up to one or two orders of magnitude higher.
Thus, they are able to interactively render large macromolecules with a much smaller computational and memory footprint.

Shortly afterwards, Lampe et al.~\cite{lampe2007two} extended the billboard technique by leveraging the programmable GPU rendering pipeline to reduce memory bandwidth usage and GPU driver overhead. 
Instead of storing the entire atomic structure of a protein on the GPU, they only store the position of amino-acids which are the building blocks of proteins.
Since there is a relatively low number of different amino-acid types, up to 20 different types, they take advantage of the multiple occurrences of these elements to reduce the number of overall bytes needed to render a protein.
Alternatively, Grottel et al.~\cite{grottel2010coherent} proposed to improve the rendering speed of large particle-based datasets by implementing occlusion culling to discard hidden particle chunks from the rendering pipeline based on the depth information obtained in the previous frame.
Hence, only the sphere impostors that are guaranteed to be visible will be processed by the graphics pipeline, thus greatly increasing the rendering performance for dense particle datasets.

Lindow et al.~\cite{lindow2012interactive} subsequently presented a novel approach which relies on ray-casting instead.
For each protein structure, they store the individual atoms in small and fitting 3D grids and upload the protein grid on the video memory.
Upon rendering, they first draw the bounding box of the grid, and subsequently, in the fragment computing program, they cast a ray for each fragment in order to find the first hit with an atom sphere.
The ray-tracing is thus performed locally for each macromolecule rather than globally for the entire scene, which means that ray-traversal routines could still be executed for proteins that are occluded and non visible in the final result.
Their method supports rendering of very large structures with up to several billion atoms.
Mesoscale landscapes usually feature a high number of individual proteins that share the same structure.
In order to spare video memory usage, which is usually restricted in size on graphics device, they also use the principle of instancing.
Instead of storing every atom of the scene on the video memory, they only upload the position and rotation of individual proteins to the video memory and upload unique protein structures only once.

Falk et al.~\cite{falk2013atomistic} extended this approach by introducing depth-based occlusion culling and used simpler grid traversal schemes to reduce computing for proteins that are located far away from the camera.
They reported being able to render sparse cytoskeleton datasets for an entire cell, with up to 25 billion atoms at 3.6 fps on modern graphic hardware.
While the presented methods only support the van Der Waals representation, a few techniques were also developed to improve the rendering of large and highly detailed molecular surfaces using GPU computing and efficient supporting structures instead of meshes ~\cite{krone2012fast}~\cite{parulek2012implicit} ~\cite{parulek2013fast} ~\cite{krone2011parallel} ~\cite{szecsi2012real}. 
However, none of these surface-based methods is yet able to compete in terms of performance with most recent van der Waals rendering methods presented by Lindow et al.~\cite{lindow2012interactive} and Falk et al.~\cite{falk2013atomistic}.

%\textbf{TODO: Read Bara's STAR report and try to enrich this section a little bit more}

\section{Visualization of Biological Systems}

\begin{figure}
\centering
\includegraphics[width=0.7\linewidth]{graphics/Citric_acid_cycle_noi}
\caption{Pathway reaction cycle describing the process of energy production during aerobic respiration and which takes place inside mitochondria, also known as TCA or Krebs cycle \cite{Krebs}. This type of procedural description informs us about the type of elements participating in metabolic processes and their role. Initially, these descriptions are used to build models and run scientific simulations. Alternatively they could also be utilized to generate the scenario of explanatory animations to inform the public.}
\label{fig:reductivetcacycle}
\end{figure}

Systems biology is the branch of biology concerned with computational or mathematical modeling of complex biological systems.
The organization of biological systems spans several scales; on the level of single cells they typically describe signalling or regulatory functions of living cells, such as energy production, gene expression, and ability to divide or die.
Such systems consist of a reaction network between molecular agents such as enzymes, metabolites, or proteins.
The reaction network is denoted as pathway, an example is provided in Figure \ref{fig:reductivetcacycle}.
Based on the pathway description, scientists reproduce the dynamics of a system \textit{in silico}, via simulation tools, and observe the changes in species concentrations over time.
The results of the simulation are then further analysed to predict and understand how these systems change over time and under varying conditions, and potentially develop solutions to health issues.
The complex reaction networks are usually described with a custom markup language, such as SBML~\cite{hucka2003systems}, and used as input for the simulation tools.
Similar to protein structures, the system descriptions are often shared with peers via public online databases.
Biologists have developed several methods to simulate the dynamics of a system.
Depending on their modus operandi the modeling approach can either be deterministic or stochastic.
Models may also feature spatial information or be purely quantitative. 

Quantitative modeling, also known as kinetic modeling, relies on the use of differential equation systems to compute the species concentrations at a given time and is therefore deterministic.
Results only vary according to the initial conditions such as concentrations and reaction rates that are predefined in the model.
Additionally, the models may also feature spatial details such as location of a species in a subregion of a cell.
This approach was the first one to be introduced and still remains very popular among systems biologists because it is reliable and computationally inexpensive.
%A typical visual output for this type of simulation is a time-concentration plot.
Another type of modeling is agent-based modeling.
This method differs greatly from the strictly mathematical approach used in kinetic modeling.
It aims at reproducing the original reaction-diffusion behaviour of biochemical agents in three dimensional space and is therefore stochastic.
This technology was primarily developed to simulate and understand complex migration pattern among animal or human populations.
The concept was then transposed to study the behaviour of chemical species as more capable computer hardware became available and affordable.
With agent-based modeling, actors of systems are virtually represented as a 3D points in space and subject to constant random motion based on diffusion speeds observed in vitro.
New elements are introduced or removed according to individual reaction events.
Reaction events are triggered based on local proximity of potential reaction partners and reaction probabilities based on the reaction rates observed \textit{in-vitro}.

Software, such as CellDesigner~\cite{funahashi2003celldesigner}, TinkerCell~\cite{chandran2009tinkercell}, and VCell~\cite{moraru2008virtual} are designed to facilitate the research process by providing a unique solution for modeling, simulation, and data analysis in a single framework.
These tools usually cover non-spatial models (quantitative modelling), except VCell which also supports the use of an external agent-based simulation modules called Smoldyn~\cite{andrews2010detailed}.
At this stage, scientists studying these models have very limited ways to see how these mathematical models of physiology behave.
They can interact with the model by specifying input parameters to the simulation and the resulting visualizations are often time-concentration plots.
Even when the simulation method produces spatial information that can be visualized, such as particle-based modeling, these tools will generally favour highly abstracted visualizations which expert users prefer and understand.
Therefore, with such visual form, it is hard to relate the models to what is visually observed in wet-lab experiments. 
In interdisciplinary physiological sciences this might hamper communication of results. 
However, the underlying data present in the models contains thorough dynamic descriptions of how these biological systems work.
These models inform us about the species present in a system, their quantities, location, diffusion speed, reaction partners, and reaction rates.
When associated with corresponding structural information this data could potentially help to digitally reproduce an illustrative and dynamic model of a cell.

Biology, medicine, and other sciences can strongly profit from a visualization of physiology in order to gain, verify, and communicate the knowledge and the hypotheses in this field.
%Additionally, dynamics simulations, when computed along with the visualization could enable online changes of simulation parameters, such as species quantities or temperature, and directly observe how it would affect the system.
While the visualization of spatial trajectory data is often not crucial for the study of metabolic pathways, in specific cases such as signalling pathways for example, such visualization might be informative to scientists that are interested in observing the spatial distribution of small signalling molecules over time.
Therefore, a few specific tools were developed to allow three dimensional visualization of particle trajectories obtained from agent-based simulation results.
CellBlender~\cite{cellblender} is a software conceived as a plug-in for the 3D animation package Blender, which allows to model, design, and visualize particle-based models computed with MCell~\cite{kerr2008fast}.
%Via the user interface, it is possible to manually setup the model parameters, which are then fed to the external simulation module.
The cell compartments of a given model are represented as 3D meshes and can be modeled or loaded via the Blender interface.
Via the custom interface of the tool, expert users can specify the model parameters, such as the species types, initial quantities, and diffusion speed.

MCell also supports 3D and 2D diffusion models for the particles.
3D diffusion is applied to elements diffusing freely inside a volume, while 2D diffusion is applied to elements that are embedded in a membrane and only diffuse along the compartment surface, such as channel proteins.
Users must then input the reactions of the model by specifying the participants, the products and the reaction probability.
Particles diffusing in 3D are also able to diffuse outside their initial compartment, and these crossing events must also be defined.
The user interface also features a multitude of advanced parameters to fine tune the modeling.
Finally, the user specifies the duration of a single step in nanoseconds, and the desired number of steps. 
The duration of one simulation step will determine the precision of the simulation.
MCell then runs the computation offline based on the model properties previously set up in CellBlender, and produces large files that contain trajectory data for each single particle and for the given number of simulation steps.
%An additional file is also produced and contains the list of all reaction events, including the ID of participating elements, the reaction location and the corresponding time step.
The trajectory data is then converted to a key-frame particle animation format which is readable in Blender.
The simulation may then be played back for real-time exploration or rendered in movies.
Additionally, it is also possible to use custom meshes to show the shapes of the molecules.
%The simulation approach, however, does not take into account the shape of the molecular structures, which are represented a simple 3D points, and thus the visualization of particle data using custom protein meshes will result in unwanted overlapping artefacts.
%To a certain extent, this information could also have an informative value for the laymen because it depicts complex processes in the form of a 3D animations.
%The viewing of actual three dimensional molecular interactions, such as the ones observed in scientific animations, could then be used as a mean to automatically generate expressive illustration of the simulated system.

\begin{figure}
\centering
\includegraphics[width=0.99\linewidth]{graphics/Teaser}
\caption{Playback of the trajectory data modeled with MCell, and rendered in cellVIEW. The model was only designed for demonstration purposes and depicts a mitochondrion from outside (left) and inside (right). The model features channel proteins diffusing on the outer membrane of the organelle (red), and small ATP molecules (green) exiting the organelle through the channel proteins. The process which only is partially depicted here is the production of energy (ATP) in the core of the organelle that is then released outside the matrix in order to be consumed elsewhere in the cell.}
\label{fig:teaser}
\end{figure}


Although this type of modeling technique was invented for scientific purposes, we envision that the generated data could also be utilized to digitally reproduce the functioning of cells for explanatory purposes.
Indeed, the resulting visualization carry important information as it allows the depiction of complex biological systems in the form of 3D diffusion and reaction animations, which can also be embedded in their environment, see Figure \ref{fig:teaser}.
Falk et al.~\cite{falk2009visualization} developed a framework to playback particle trajectories with additional overlaid information to trace the history of individual particles, such as trajectory and previously undergone reactions.
A direct approach to visualizing raw trajectory data, however, may often result in an overly cluttered view due to an overwhelmingly large number of elements diffusing randomly in every directions, and may often be close to incomprehensible, even for expert users.
To provide a clearer overview of the spatial information, Falk et al.~\cite{falk20103d} followed up their previous work and proposed a novel volume-based representation of the agents density to better observe migration pattern of a selected species.
An advantage of this approach is that it significantly reduces visual clutter and highlights important spatial properties much more efficiently.
However, this approach was only designed for a certain type of domain users, rather than for the laymen.
Although it may help reducing overall visual clutter, it also removes individual particle behaviour, which would be crucial to showcase in order to ensure that the underlying information, i.e., the actual function of each actor of the system, is perceived by the viewer.



\chapter{Rendering and Composition of Molecular Landscapes}
\label{sec:section3}

Up to this point, the rendering methods presented in the visualization literature have reached unprecedented levels of performance, in terms of size of supported datasets and rendering speed, thus enabling real-time rendering of large molecular structures generated with cellPACK~\cite{johnson2015cellpack}.
The most recent presented solution is capable of rendering 25 billion atoms at 3.6 fps in HD resolution.
However, the rendering approach fails to provide a comfortable user experience, as one expects between 24 and 60 Hz on average for interactive entertainment, and more than 75 Hz for VR content.
The rendering should also leave enough resources free for eventual additional computation, such as the physics simulation of the molecular bodies for real-time animation, for example.
Moreover, none of the techniques mentioned above have proved to efficiently support other types of molecular structures that exhibit a more complex organization, such as lipid membranes, nucleic acids or fibres, which ought to be taken into account for a precise depiction of molecular landscapes.
These are indeed more challenging to render, because the assembling blocks of these structures are considerably smaller and also more numerous than with protein data.

\begin{figure}
	\centering
	\includegraphics[width=0.82\linewidth]{"graphics/large_scene"}
	\caption{Rendering benchmark achieved with a virtual dataset made out of 250 smaller blood plasma datasets, and comprising up to 15 billion atoms in total. The software is capable of rendering the entire dataset at 60 frames per seconds, and from any point of view in the 3D scene. The level-of-detail scheme ensures a fast refresh rate when looking at the dataset in its entirety, and the occlusion queries optimize the performance when closely looking at a sub-region of the dataset.}
	\label{fig:unity-2015-06-22-01-20-37-65}
\end{figure}

In order to provide a truer depiction of micro-organisms and improve real-time user experience, we decided to investigate new rendering approaches that would address all these limitations.
A shortcoming of the volumetric approach presented by Lindow et al.~\cite{lindow2012interactive}, is that for each rendered macromolecule, additional expensive ray-traversal routines are required during the per-fragment processing, which may unbalance parallel thread execution and cause considerable bottlenecks, especially with dense scenes composed of small individual macromolecules.
We opted for an impostor-based method for the design of our rendering pipeline, which requires more individual per-fragment thread execution per macromolecule, but is far more balanced and suitable for parallel processing and also does not require expensive volume sampling operation upon rendering. 
The pipeline that we have designed directly follows-up the work of Lampe et al.~\cite{lampe2007two}.
It relies on GPU computing and aims at minimizing GPU driver-overhead caused when issuing too many draw commands to the GPU.
We also optimized the rendering by adapting well-known computer graphics techniques, such as level of detail, instancing, and occlusion culling, allowing us to render up to several billion atoms with a steady 60 Hz refresh rate, see Figure \ref{fig:unity-2015-06-22-01-20-37-65}.
The rendering solution which we developed, dubbed cellVIEW, was implemented with a popular game engine, and the rendering pipeline was embedded in the core of the engine.
The project received help from the researchers of the Scripps Research Institute, namely Ludovic Autin, who provided support to import cellPACK models in cellVIEW.
Results of the visualization of cellPACK models with cellVIEW are shown in Figure \ref{fig:cellVIEWvsGoodsell}.

\begin{figure}
	\centering
	\begin{subfigure}[b]{0.99\textwidth}
		\includegraphics[width=1\linewidth]{graphics/hiv}
	\end{subfigure}
	
	\begin{subfigure}[b]{0.99\textwidth}
		\includegraphics[width=1\linewidth]{graphics/Picture10}
	\end{subfigure}
	\caption{Handmade Illustrations of David S. Goodsell (left) next to images rendered with cellVIEW and captured in real-time. The depicted organisms are the HIV surrounded by blood plasma proteins (a) and Mycoplasma (b), which is a small bacterium. The datasets were modeled with cellPACK and are still work in progress, the RNA contained inside the HIV capsid, for example, is not yet present, as well as the bi-lipid membrane of Mycoplasma. Real-time rendering is very helpful during the modeling process because it allows to quickly verify and validate of the models, which renders the modeling process much less cumbersome. Additionally, it becomes less time consuming to create illustrations since the system also allows us to quickly compose the scene with the visibility equalizer to ensure that important elements are visible without manually modifying the position of the macromolecules.}
	\label{fig:cellVIEWvsGoodsell}
\end{figure}

A particularity of the scenes generated with cellPACK is that they aim at reproducing molecular crowding which can be observed \textit{in vitro} and results in dense concentrations of macromolecules.
So far, state-of-the-art methods that are able to render up to several billion atoms interactively have only showcased protein datasets with a low population density, which means that less macromolecules are present in the viewport at once.
Unlike previous work, cellVIEW was specifically designed to render large scale and realistic datasets featuring accurate protein densities.
Besides increasing the computational complexity of the rendering, denser scenes may also cause major occlusion issues because important internal structures, such as DNA for example, may be hidden by surrounding elements.
Hence, we also propose a custom scene composition pipeline to adjust the visibility of proteins, while preserving important contextual information.
%The pipeline is also engineered to leverage the power of GPU computing in order to provide a responsive user experience even with a large number of macromolecules.

%\begin{figure}
%\centering
%\includegraphics[width=0.7\linewidth]{graphics/screen_4000x4000_2015-08-10_11-20-06}
%\caption{}
%\label{fig:screen4000x40002015-08-1011-20-06}
%\end{figure}
%

\section{Level of Detail}

\begin{figure}
	\centering
	\includegraphics[width=0.99\linewidth]{graphics/lod}
	\caption{The level-of-detail concept (LOD) applied to macromolecules. Using a clustering algorithm, simplified atomic structures are generated, which comprise fewer and larger spheres instead of atoms, called \textit{meta-atoms}.
		Impostors are used to generate the spheres and thus the computational footprint for rendering the proxies is greatly reduced, down to only a few dozen of spheres for the most simplified proxies.}
	\label{fig:lod}
\end{figure}

Level of detail (LOD) is a method often used in computer games to cope with the limited polygon budget of real-time applications.
The principle consists of drawing simpler mesh representations for distant objects since they have a lower pixel coverage compared to objects nearer to the camera, and therefore less details can be shown.
From an original 3D model with a high number of polygons, proxy models are generated to create an atlas of meshes with gradually simplified geometries, as visually explained in Figure \ref{fig:lod}.
Upon rendering of a model, the LOD proxy is then selected based on the distance to the camera.
%The further away an object lies, the less details can actually be observed and thus less polygons should be rendered.

This concept was applied to molecular visualization by Parulek et al.~\cite{parulek2014continuous} who presented a continuous level-of-detail scheme for molecular surface rendering.
Molecular surfaces are useful for scientific exploration of cavities and pockets.
Because computing the surface for dynamic molecular datasets in real-time is expensive, they propose to restrict the computation of high resolution surface details to a subset of the macromolecule located near the camera.
For the most distant regions they simplify the atomic structure to reduce the computation times, which also reduces high frequency surface details.
They use clustering algorithms to simplify the atomic structure of a protein with fewer and larger spheres, which we refer to as meta-atoms.
%Since the structure comprises less spheres to contribute to the surface, the computation of the distance field is much less expensive, but still preserves low-frequency shape details.
We also use clustering to simplify protein structures, but we directly render the spheres resulting from the clustering as 2D impostors instead of computing the surface.
Clustering allows a reduction of the number of spheres from 75\% for the first LOD proxy up to 99\% for the most simplified proxies.
We also use different shading materials for original atomic structures and the proxies.
For proteins closer to the camera and showing the entire atomic data, we highlight the surface details using high-frequency illumination.
For proteins located further away that are showing only simplified structures, we only highlight low frequency shape details to make the meta-atoms less salient and the overall shape smoother.

A naive rendering strategy would be to issue a single draw command per macromolecule.
However, each draw command will cause a small latency due to GPU-driver overhead, regardless the number of rendered spheres.
When dealing with complex scenes, the accumulation of the GPU-driver latency would cause a severe bottleneck, which would simply forbid real-time rendering.
With legacy GPU-instancing, one may group proteins sharing a similar structure and LOD proxy in a single draw command.
In most complex use-cases, however, there may be up to several thousand different macromolecule types, and half of dozen of LOD levels.
Issuing that many draw operations would thus unnecessarily compromise the efficiency of the rendering pipeline.
Therefore, we developed an optimized rendering pipeline which is able to render an entire set of macromolecules with different structures and LOD proxies in a single draw command, thus removing GPU-driver entirely.

\section{Instancing}

Instanced drawing is a concept widely used in computer graphics that aims at reducing the memory bandwidth and footprint, as well as reducing the GPU-driver latency caused by a large number of draw commands.
This concept was applied by Lampe et al.~\cite{lampe2007two} who used the geometry shader stage to instantiate the atomic structure of entire amino acids, also called residues, directly from the GPU rasterization pipeline.
Amino-acids are the building blocks of proteins, and there are around 20 different types of amino acid.
They initially store the atomic structures of each residue type in the video memory in a dedicated buffer, and they also store in separate buffers the position, rotation, and type of all amino acids that compose a protein.
With their method, they are able to render an entire residue with a single initial per-vertex operation.
During the vertex shader execution, amino acid properties such as position, rotation, and type, are read from the video memory and passed on to the next shader stage, i.e., the geometry shader.
The geometry shader program then fetches the local atom positions for the corresponding residue from the video memory, which are then transformed with the residue position and rotation.
For each residue atom, new triangles are injected around atom centroids, and then processed in the final per-pixel shader stage to form 2D sphere impostors, similarly to Tarini et al.~\cite{tarini2006ambient}.
It is also possible to launch the execution of multiple vertex shader programs in a single draw operation, thus reducing the latency accumulation caused when sending multiple rendering commands to the GPU. 

Given a protein composed of $4000$ atoms and $250$ amino acids, the memory footprint of a protein would thus be reduced from 16000 32 bits numbers ($4000 \times 3$ floating-point numbers per atom position, and one integer for the type) to 2000 numbers ($250 \times 7$ floating-point numbers per amino acid position and rotation and one integer for the type and excluding residue atoms, which may be are reused by other molecules).
Although there is a finite number of amino-acid structures, there also exists an infinite number of possible rotational conformations, that may often change along a single protein chain.
Therefore, it is rather challenging to accurately depict protein structures with this approach.
To address this limitation, it would be preferable to apply the same concept of instancing to entire proteins instead of single residues.
It would also help reducing the memory footprint of the scene as only the type, position, and rotation would suffice to describe a single protein (only 8 numbers needed per protein).
However, the geometry shader causes a considerable latency as the number of injected geometries increases, and therefore it is only possible to instantiate a few dozen triangles efficiently with this method.
Indeed, a single geometry shader program is responsible for transforming and injecting multiple geometries, but this operation is performed in serial, while it would be more efficient to distribute the workload across multiple threads instead.

The tessellation shader is a feature that is now available on most recent graphics hardware, and is also available on high-end mobile devices.
Similarly to the geometry shader, this shader stage is designed to dynamically inject geometries in the rendering pipeline from a GPU program.
While the geometry shader is only designed to inject a few dozens of triangles maximum, the tessellation shader is able to efficiently inject thousands of triangles by dispatching the work on multiple threads.
Initially, the feature was developed to selectively increase the level of detail of sub-regions of meshes, based on the camera distance and surface curvature.
The tessellation engine is also able to inject other types of primitives than polygons, such as lines or vertices.
We use this feature to dynamically control the number of spheres to be rendered from the GPU program executed in the rendering pipeline.
Similarly to Lampe et al.~\cite{lampe2007two}, only one initial vertex shader thread is required to draw an entire molecule, but the maximum number of vertices that can be injected per thread is up to two orders of magnitude greater with our approach.

The vertex shader, which is the first stage of the pipeline, is used to fetch protein information such as position, rotation, and type, from the video memory, and these will then be passed on to the next stages of the pipeline.
Based on the protein type and distance to the camera, we select the corresponding LOD proxy, which informs us about the number of spheres required to draw the given protein.
We then notify the tessellation engine about the number of vertices that we want to inject, i.e., one vertex per atom or meta-atom.
The tessellation engine will then dispatch the execution of a thread batch comprising one thread per vertex.
Each thread is initially given a unique ID, which is used to fetch the corresponding sphere information from the main video memory, such as the local position and radius of atoms or meta-atoms.
After transforming the local sphere position with the protein position and rotation, the sphere centroid is passed on to the next shader stage, i.e., the geometry shader.
Subsequently, the geometry shader is used to inject the remaining vertices around the sphere centroids to form the triangles of the 2D sphere billboards.
The tessellation shader supports injecting of up to 4096 vertices maximum at once, and when combined with the geometry shader the pipeline is able to generate up to 8192 triangles from a single initial per-vertex program.
An advantage of this technique is that it allows to render an entire scene comprised of several hundreds of thousands of proteins in a single draw command regardless of the protein type or LOD proxy, thus approaching zero driver overhead.
%Additionally, the memory footprint is considerably smaller than previous techniques, and the pipeline is also able to process large macromolecules with up to several thousands of atoms.

%TODO Figure Methodology ??
%TODO Figure Benchmark

\section{Occlusion Culling}

%is the rendering stage which the most demanding and causes a bottleneck in the pipeline.
%Mesoscale datasets such as the one generated with cellPACK often result in a very dense arrangement of macro-molecules.

Upon rendering macromolecular landscapes, a large number of geometries are processed through the rendering pipeline, which may overload the graphics hardware and cause bottlenecks.
Per-fragment processing is, by far, the most computationally demanding stage of our pipeline.
Because the scenes are very dense, a large number of spheres overlap each other at a single pixel location, which in turn results in an overly large number of concurrent depth-tests thus slowing down the rendering.
An efficient way to reduce this computational load is occlusion culling, and is also widely used in computer games.
The principle of occlusion culling is to exclude objects that are partially or completely hidden by other visible objects from the rendering pipeline, in order to spare computation.
There exists on modern graphics hardware, a fixed functionality called hardware occlusion queries (HOQ), which allows computing the number of visible pixels of rasterized geometries based on an input depth texture.
The depth texture may either be generated beforehand (depth-only pre-pass), or simply recycled from the previous frame.

Grottel et al.~\cite{grottel2010coherent} presented an efficient rendering pipeline for large particle-based datasets, using HOQ to discard large chunks of the data that is hidden in the current viewport.
They use a uniform grid to spatially partition the dataset and perform occlusion queries for each grid cell using cubes as bounding geometries.
Each query result is individually read back to the application after a certain latency, which is due to the processing in the graphics pipeline.
Additionally, since one draw command must be issued for each single query, the GPU driver overhead causes a small latency that accumulates with the number of queries.
For the specific use-case demonstrated by Grottel et al.~\cite{grottel2010coherent}, this approach seems reasonable because of the relatively low number of queries needed (one query per grid cell with a $15\times15\times15$ resolution).
In our use-case, we would need to perform one occlusion query for each individual macromolecule present in the scene.
Averagely complex scenes generated with cellPACK may easily comprise up to millions of individual instances when small lipid molecules are present, and therefore the presented approach is guaranteed to perform poorly with such scenes.

A more suitable approach would have to be specifically designed to reduce execution times of individual queries and also to limit GPU-driver overhead.
Hierarchical Z-Buffer (HiZ buffer) is a method introduced by Green et al.~\cite{greene1993hierarchical} that is commonly used to perform visibility look-up's efficiently.
Based on an input depth map, they compute mip-maps with a custom algorithm in order to only retain the deepest values for each texture down-sampling operation.
Hence, a single pixel from a lower resolution mip-map will indicate the deepest value from the entire region covered by this pixel in higher resolution mip-maps.
Subsequently, for each object to be queried, they compute a camera-facing bounding square.
Based on the pixel coverage and position of the squares, they perform a series of look-up's at a specific resolution in order to minimize the number of texture accesses, which tend to slow down the computation.
The use of camera-facing squares as bounding regions allows to cover the entire square with only 4 texture look-up's at the right mip resolution, thus guaranteeing a constant execution time, regardless of object sizes.
Finally, in case the depth of a given square is deeper than all of the four values queried, the object is then guaranteed to be hidden and may safely be discarded from the rendering.

Without a proper query batching mechanism, however, the overhead caused by individual queries would simply be too high to allow real-time computation with our datasets.
Therefore, we propose to adapt this method to run in parallel on graphics processing units, in order to perform multiple queries simultaneously, and thus speeding-up the operation.
Because only one thread execution is needed to compute a single query, we are also able to dispatch the GPU computation of multiple queries in a single command, thus approaching zero GPU-driver overhead.
With our most complex scene, we were able to save up to 50\% of the rendering computation thanks to batched occlusion queries.

\section{Fibres Structures}

%It is also worth mentioning that the proposed modelling approach does not aspire to provide a strictly accurate depiction of DNA structure.
%Instead, we aim a showcasing an resembling representation of the structure which is fast to compute and designed for illustration purposes only.

%and static representations fail to depict the true
%displaying a static structure would only show a single configuration 
%Displaying static structures may also 
%Molecular datasets and illustrations also often depict dynamics data
%In most cases 
%Rendering
%However, we foresee two issues with that approach.
%Firstly, the memory footprint of DNA strands might simply be too large to store the genome data of entire cells on the video memory.
% 
% Secondly, simulation tools that are able to reproduce the dynamics of DNA fibre structures are all CPU-based algorithms.
% Assuming that storing large static structures is already a challenging enterprise, the rendering of animated structures would very likely have to be streamed to the GPU memory instead.
% \textbf{Ivan: This argument comes out of the blue. 
% 	Is it your goal to have support for animation ? Why ? Refer to earlier text.}
% Interactive rendering of animated DNA structures would therefore be limited with this approach, because the cost of memory transfer between CPU and GPU would slow down the rendering considerably.

A crucial component of living organisms are fiber and polymer structures, such as DNA strands.
These structures store and convey the genetic code of living organisms, and therefore, it is important to convey how they work, which is tightly tangled with how they look.
These structures are composed of small monomers, called nucleotides, made out of a few dozen of atoms and assembled in long chains.
One approach to render this type of data efficiently would be to use instancing, similarly to protein data, and to store the position, rotation, and type, of each nucleotide on the video memory.
However, the large number of monomers contained in the entire genome, and the relatively small atomic size of individual monomers, will likely result in a considerable memory usage overhead that would limit the quantity of displayed information.
%By way of comparison, individual protein structures are on average composed of several thousands of individual atoms, which is two order of magnitude larger than a single nucleic acid.
%, thus genome data is much more challenging to render efficiently in its entirety. 
%For example, the entire genome of E.Coli, a relatively small cell, is composed of $4,641,652$ individual base pairs, which represents \textbf{xxx} GB of data.
%
Moreover, these macromolecules are dynamic entities, and therefore it is important to visualize structural changes over time to illustrate them faithfully.
In order to display the trajectory of a molecular dataset, the data must transit from CPU to GPU, either beforehand, or streamed during the visualization.
In the case of visualization of large genome datasets, the trajectory information is likely to be streamed on-the-fly to the GPU, because the data contained in all the simulation snapshots might not fit into the video memory.
However, even on most recent graphics hardware, transferring data from CPU to GPU is still relatively slow.
Although the video memory might be large enough to store millions of instances contained in a single frame, it does not guarantee that sequential simulation snapshots could be loaded and displayed in real-time.

Our solution to limiting the memory footprint and bandwidth of fibre structures is to only use the control points of a spline as input, and to position each individual nucleotide along the spline upon rendering, using GPU computing.
There exist tools that can model the structure of DNA strands simply from control points~\cite{lu20083dna}~\cite{hornus2013easy}.
However, none of these methods are implemented on the GPU; the entire genome data is first computed on the CPU and must be transferred to the GPU in order to be rendered, which may stress the memory bandwidth and usage for very large datasets.
The rendering pipeline which we developed for the fibres relies on the tessellation shader to dynamically inject atoms of the nucleotides along the fibre curve segments.
An advantage of our fibre rendering pipeline is that the previous technologies that we have developed to optimize the rendering of protein data, such as LOD or occlusion culling, may also be seamlessly used with the fibre data.
Additionally, since only the curve data is needed as input, the animation of the fibre structures becomes much more trivial to compute.
Indeed, mainstream physics animation tools that are GPU-based may be used instead of CPU-based scientific tools, which guarantees a considerable performance boost.
\begin{figure}
	\centering
	\includegraphics[width=0.90\linewidth]{graphics/Picture11}
	\caption{Screen capture of the genome of mycoplasma rendered in real-time with cellVIEW at more than 60 frames per second, and comprising over 1 million pairs of nucleotides.
		The control points of the DNA curve are uploaded to the GPU and the monomers are dynamically instantiated along the spline via a GPU program upon rendering. }
	\label{fig:picture11}
\end{figure}
In order to support different types of fibres, it is important to first understand how their structures are organized.
In the case of DNA fibres, a well-known structure is the B-DNA, which exhibits the recognizable double-helix pattern.
The structure is also very simple to model: a spacing of 3.4Å and a rotation of 34.3 degrees between adjacent nucleotides. 
We also define an average number of 12 nucleotides per segments.
Based on this knowledge, we are able to generate long and continuous B-DNA strand, using only control points and building rules as input information.

The pipeline is designed to render one segment of the curve (12 nucleotides) for each invocation of the per-vertex program.
The vertex program accesses the main video memory to read the control points information for the current segment as well as the building rule of the current fibre structure.
The building rules and the segment curvature are used to compute the position and rotation of the nucleic acids along the curve segment, and this information is then passed on to the next shader stages.
Based on the building rules of the segment and its distance to the camera, we estimate the number of spheres required to draw the fibre segment.
We then notify the tessellation engine about the number of vertices that we want to inject, i.e., one vertex per sphere.
The tessellation engine will dispatch the execution of a thread batch comprising one thread per sphere.
Upon processing individual spheres in the tessellation stage, the corresponding sphere information is fetched from the main video memory, such as the local position and radius of atoms or meta-atoms.
After transforming the local sphere position with the position and rotation of the corresponding nucleic acid, the sphere centroid is passed on to the next shader stage, i.e., the geometry shader.
Subsequently, the geometry shader is used to inject the remaining vertices around the sphere centroids, in order to form the triangles of the 2D sphere billboards.
A final render of our fast genome rendering pipeline is shown in Figure \ref{fig:picture11}.

\section{Occlusion Management}

%\begin{figure}
%	\centering
%	\includegraphics[width=0.5\linewidth]{graphics/download}
%	\caption{A realistic depiction of molecular crowding inside the E. coli bacterial cytoplasm by McGuffee and Elcock~\cite{mcguffee2010diffusion}}
%	\label{fig:download}
%\end{figure}

The phenomenon of molecular crowding is used to describe a solution when macromolecules are present in high concentrations.
Such conditions occur routinely in living cells; cellular interiors are 20–30\% volume-occupied by macromolecules, which corresponds to an approximate range of 200–300 mg/mL.
The cytosol of E. Coli, for instance, contains about 300–400 mg of macromolecules per millilitre.
Therefore, an accurate depiction of the internal structure of cells often results in a very dense arrangement of macromolecules. % as shown in Figure \ref{fig:download}.
Without efficient means to selectively remove occluding objects, it would therefore be impossible to observe hidden internal structures that play essential roles in the functioning of biological systems.
In scientific illustrations and visualizations, cutaway views are often employed as an effective technique for occlusion management in densely packed scenes.
However, a limitation of strict cutaway views is that important contextual information is removed.
Illustrators must make sure that the essential information, such as the proportions of molecular agents present in the system, are rightfully represented and not simply clipped away.
While mainstream visualization and illustration software feature cutaway tools, they do not provide the means to easily perform advanced scene composition for such scenes.
%Transparency is another approach to solve occlusion problems, however the computational cost of this technique will be too significant for large molecular scenes featuring several thousands of overlapping elements per pixel.
%Furthermore, due to the presence of a large number of potential occluders the use of transparency will likely result in overly complex images which would affect the quality of the visualization.

\begin{figure}
	\centering
	\begin{subfigure}{.6\textwidth}
		\centering
		\includegraphics[width=.99\linewidth]{"graphics/Picture4"}
		\caption{}
		\label{fig:sub1}
	\end{subfigure}%
	\begin{subfigure}{.4\textwidth}
		\centering
		\includegraphics[width=.99\linewidth]{"graphics/Picture8"}
		\caption{}
		\label{fig:sub2}
	\end{subfigure}
	\caption{Cut-awy view of the HIV + blood plasma dataset authored with the Visibility Equalizer. (a) The inside of the HIV capsid (clear blue proteins surrounded by yellow capsid proteins) is set as object of interest, which ensures occluders to be removed dynamically from any point of view. (b) Demonstration of the "fuzzy visibility" concept. Above the manually positioned clipping plane the user can decide to override the concentration of displayed elements. Additionally, we also propose to render the ghost of the removed elements, to reduce visual clutter only the first layer of removed molecules is drawn.}
	\label{fig:test2}
\end{figure}

We propose a novel method for solving occlusion problems due to molecular crowding.
In contrast with existing techniques, we take advantage of the characteristics of complex molecular landscapes such as the scenes modeled with cellPACK.
These models usually contain hundreds of thousands of individual macromolecules, however, many of them share the same structure.
Therefore, by reducing the concentration of elements sharing a similar structure, we may reveal hidden features and also preserve important information such as the type of contextual elements and their location.
This concept of a "fuzzy visibility" resembles the “screen-door transparency” technique popular in the early days of computer graphics, and where transparency is achieved by placing small holes in a polygon to reveal what is present behind.
When reducing the concentration of macromolecules to reveal structures that are located behind, we also ensure that elements are removed uniformly across the scene to preserve the proportions of the correct spatial distribution.
A potential limitation of this approach is that it will communicate erroneous quantitative information to the viewer.
In order to avoid misconceptions, we optionally propose to display the removed elements with a ghosting effect such as transparency or contours only.
To achieve this effect, discarded elements are first drawn as opaque geometries with the depth-test enabled in a separate texture before being composed on top of visible elements with alpha blending.
Therefore, only the first layer of removed elements are shown, which helps minimizing visual clutter compared with full scene traversal transparency.
Screen captures of scenes composed with the Visibility Equalizer are shown in Figure \ref{fig:test2}.

\begin{figure}
	\centering
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=.99\linewidth]{"graphics/visibility equalizer3"}
		\caption{}
		\label{fig:sub1}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=.99\linewidth]{"graphics/visibility equalizer2"}
		\caption{}
		\label{fig:sub2}
	\end{subfigure}
	\caption{The concept of Visibility Equalizer. For each group of similar macromolecules a stack of bar is displayed,  which informs us about their visibility. 
		The gray bar tells us the percentage of instances that have been clipped, either by a clipping plane or the "fuzzy visibility". The dark green bar informs us about the number of instances currently visible on the screen, and the light green bar about the number of instances rendered but occluded by other molecules. By dragging the bar the user can thus easily modify the visibility of entire groups of proteins and intuitively compose and inspect a given dataset only with a limited set of user interactions.}
	\label{fig:test1}
\end{figure}

To assist the scene composition, we additionally display a bar chart over the view that provides real-time quantitative feedback about the visibility of each macromolecular type.
A single bar comprises three distinct regions that indicate the visibility properties for a given species, as shown in Figure \ref{fig:test1}.
The grey region bar indicates the number of macromolecules that are currently discarded from the rendering.
The dark green region indicates the proportion of rendered molecules that are currently visible, while the light green region indicates the proportion of them that are currently occluded by other macromolecules.
The bar chart thus provides an overview of the visibility properties that are helpful for scene composition.
For instance, if the user wishes to observe a particular species, he will immediately be informed about the quantity of this species that are occluded by other structures, and will adapt the visualization accordingly.
The modulation of the proportion of visible elements is also done via the quantitative view by simply dragging the edge between the grey and green regions.

Another useful functionality for scene composition is the selection of species of interest.
When selecting a particular macromolecular type as species of interest, occluding elements of a different types are then guaranteed to be discarded in order to provide full-visibility for the species of interest.
Since this method is view-dependent, it is necessary to compute occluding elements routinely when the camera position is updated.
Therefore, we developed a custom culling pipeline using GPU computing to guarantee a smooth user experience even with scenes featuring a large number of objects.
Macromolecules that are set as species of interest are first rendered in a separate off-line depth texture and used as input to perform image-based occlusion queries for each of the remaining elements and detect occluders.
Additionally to our quantitative approach for occlusion management, we also provide support for traditional clipping objects such as planes, cubes, or spheres that can be placed in the 3D scene manually.

%\begin{figure}
%\centering
%\includegraphics[width=0.7\linewidth]{"graphics/visibility equalizer2"}
%\caption{}
%\label{fig:equalizer-results}
%\end{figure}
%Reference Figure \ref{fig:equalizer-results} here.

\chapter{Emulating the Machinery of Life}
\label{sec:section4}
%In structural biology, dynamics simulations, such as molecular dynamics (MD), or brownian dynamics (BD) are used to reproduce the physical behaviour of groups of atoms in three dimensions, in order to understand their function.
%The initial purpose of cellPACK was to generate models that could serve as initial state in order to run large scale dynamics simulations on super computers.
%\textbf{In computational biology, it exists a gap between structural and systems biology caused by the limitation of today's computers. Ivan: Incomplete statement out of context}
%Indeed, MD and BD simulations are simply too computationally expensive to reproduce the dynamics of biological systems for entire cells, and are restricted to smaller use cases in terms of size and duration.
%Systems biologists, on the other hand, are interested in studying the dynamics of entire biological systems, but they also value quantitative information more than spatial information.
%Therefore, they often employ strictly quantitative modeling approaches, which are computationally inexpensive.
%More recently, spatial particle-based methods for modeling reaction networks have emerged.
%Their principle consists or reproducing the behaviour or spatial diffusion and reaction for a large number of molecules in order to study the changes in species quantities over time.
%In order to reduce computational costs, the reaction diffusion processes is extremely simplified.
%Particles are subject to random walk to simulate molecular crowding without having to take into account surrounding macromolecules that do not play a role in the simulated process.
%Individual reactions are triggered upon collision events between two potential reaction partners.
%Particle-based modeling is thus a very singular approach as it provides spatial information for individual proteins and allows computing complex reaction networks in a reasonable amount of time for entire cells.
%In this section, we will investigate the use of particle-based systems to showcase the dynamics of processes in three dimension in order to complement the large but static scenes modeled with cellPACK. 



%Their principle consists or reproducing the behaviour or spatial diffusion and reaction for a large number of molecules in order to study the changes in species quantities over time.
%In order to reduce computational costs, the reaction diffusion processes is extremely simplified.
%Particles are subject to random walk to simulate molecular crowding without having to take into account surrounding macromolecules that do not play a role in the simulated process.
%Individual reactions are triggered upon collision events between two potential reaction partners.
%Particle-based modeling is thus a very singular approach as it provides spatial information for individual proteins and allows computing complex reaction networks in a reasonable amount of time for entire cells.

%Therefore, they often 
%More recently, spatial particle-based methods for modeling reaction networks have emerged.
%Their principle consists or reproducing the behaviour or spatial diffusion and reaction for a large number of molecules in order to study the changes in species quantities over time.
%In order to reduce computational costs, the reaction diffusion processes is extremely simplified.
%Particles are subject to random walk to simulate molecular crowding without having to take into account surrounding macromolecules that do not play a role in the simulated process.
%Individual reactions are triggered upon collision events between two potential reaction partners.
%Particle-based modeling is thus a very singular approach as it provides spatial information for individual proteins and allows computing complex reaction networks in a reasonable amount of time for entire cells.


%These models are valuable for scientific and educational exploration because they contain structural information that can be observed at multiple scales, such as compartments shapes, spatial arrangement of macromolecules, and atomic structure of each macro-molecule.
%
%
%
%Rendering such scenes interactively without compromising on the amount of displayed information is challenging.


%To speed up the scene rendering and composition on commodity hardware, we adapted several concepts from computer games such as instancing, and level-of-detail.
%However, the representations of these microscopic organisms are static and do not provide any information about the future states or important biochemical processes that are ongoing internally.
%The HIV virus, for instance, goes through multiple transformations during his life cycle [show figure], and showcasing only one single state would not suffice to revealing its functioning.
%Scientists have attempted to produced several states of the HIV virus life cycle, which now comprise representation of the virus in its mature and immature state[show figure].
%However, with this technology, the modeling process is stochastic and does not provide logical correspondence between the models, which simply forbids key-frame animation between two states at this stage.
%Additionally, producing enough models to generate an animation would be challenging and also time consuming because the modeling process is not streamlined and require complex manual setup for each model.

%\textbf{Ivan: Red thread is broken}
%\textbf{Ivan: until now I do not know why I read Section 3.4, it seems unrelated to the previous thesis description.}

Our grand vision is to digitally reproduce the complex mechanisms ongoing inside living organisms and expose them to a larger audience.
An essential part of this vision is to provide an interactive visualization technology that would allow the viewers to directly interact with the showcased content.
Another important aspect is the use of computational biology data to minimize manual creation of digital assets and show what is currently known and where are the borders of our knowledge. 
So far, we collected static models of entire viruses and cells and developed new methods to efficiently visualize them in real-time with a multi-scale approach.
However, to fully accomplish our vision, the next challenge is to provide the means to animate macromolecules, in order to depict the story which is associated with their function.
In animated movies conceived for public dissemination of cell biology, the actors of the machinery of life are traditionally animated with standard animation methods such as key frame animation.
Because these animations must be conceived and authored manually, the creation process in usually very expensive and time consuming.
Furthermore, the work has to be updated frequently if one wishes to keep the material up to date with state-of-the-art research in cell biology.
Experts in computational biology constantly produce large amounts of data for their research, which contain valuable information that could also be used to automatize the creation process of animated content.

Structural biology already provides us important information about how things look, e.g., the atomic structures of key macromolecules.
They also utilize this information to set-up dynamics simulations (molecular dynamics or MD) and reproduce the physical behaviour of atom in three dimensions, in order to understand the role they play in the machinery of life.
Unfortunately, this type of modeling is computationally demanding and is greatly limited in terms of size and length by the power of modern computers.
Systems biology, on the other hand, is the branch of biology concerned by the study of complex biological systems using computational or mathematical modeling.
A typical systems biology model consists of a reaction network between molecular agents that characterizes its functioning on the molecular level, also known as a pathway.
The reaction network describes the complex cascade of reactions that enables a given process, such as gene expression, energy production from nourishment, reproduction, or destruction of a cell.
A model also contains quantitative information such as initial concentrations of species and reaction rates that are observed \textit{in vitro}.
This information is then used to initialize simulation programs that aim at modeling the variation in species concentration over time.
Based on simulation results, experts are able to formulate hypotheses, which they can verify in wet laboratory experiments.
Systems biology experts are generally interested in analysing quantitative information and merely concerned about viewing the actual structure of the individual molecular agents that are simulated.
However, data produced by these experts contains valuable details about complex processes that could be used to efficiently produce dynamic 3D illustrations and would significantly improve the way visual communication of cell biology is traditionally done. 
Indeed, these models already contain enough details to procedurally lay out the scenario of an animated sequence, such as which elements should react together, in which region, in which order, and at which rate.

Standard computational models usually employ a strictly quantitative approach, which are computationally inexpensive but do not provide spatial information other than the sub-region of the cell or organism in which elements are located.
Particle-based modeling is a computational modeling approach which has recently emerged.
The principle behind this technique is very singular as it aims at reproducing the reaction-diffusion behaviour of each individual molecule contained in the system.
In order to reduce computational costs, the reaction-diffusion process is extremely simplified compared to traditional molecular dynamics.
To summarize this principle, each molecule is represented as a 3D point and subject to a simple random walk motion to simulate the diffusion in a crowded environment, and reactions between agents are triggered upon collision between the bounding spheres of two particles.
Particle-based modeling is thus more appealing to us compared to other modeling approaches, as it provides additional particle trajectories that can simply be played back in a virtual scene to automatically generate animated sequences.
In this chapter, we investigate the use of systems biology data combined with structural information to generate comprehensive visualization of simulated processes.
In the first part, we will show how to overcome a major drawback of playing-back particle trajectory data, which is how to deal with the prevailing chaos that is naturally present in living cells when observing the reaction-diffusion process for a large number of particles.
Another limitation of particle-based modeling is that the simulation must be precomputed beforehand due to its high computational cost, which prohibits interactive changes of the simulation parameters during the visualization.
Compared to particle-based modeling, quantitative modeling, such as kinetic modeling, has a very light computational footprint.
In the second part, we will explore the use of an hybrid approach using particle systems and kinetic modeling to produce a similar visual output as particle-based modeling with a much lighter computational cost, thus enabling interactive changes of the simulation parameters in real-time. 

\section{Observing Multiple Time Scales Simultaneously}

In particle-based modeling, also known as agent-based modeling, the reaction-diffusion is explicitly simulated for each individual particle present in the system.
In a single simulation step, the simulation routine performs a random displacement for each particle to mimic the diffusion phenomenon in a crowded environment, and also trigger reaction events upon collision of two neighbouring particles.
Although the primary use case for this type of modeling is to provide experts quantitative information, the modeling approach is also capable of producing 3D trajectory data and reaction log as output, which can be used to visualize the underlying spatial information~\cite{falk2009visualization}.
Particle-based simulations, on one hand, traditionally operate at a very small frequency, typically of the order of nanoseconds for a single simulation step.  
The duration of the simulation, on the other hand, is usually several orders of magnitude larger, up to several seconds, thus resulting in a very large number of simulation steps.
Hence, there exists a large time-scale discrepancy between the life of individual particles and the life of a given simulated process.

\begin{figure}
	\centering
	\includegraphics[width=0.9\linewidth]{graphics/Apoptosis}
	\caption{Screenshot of frame t and t+3 of a molecular animation \textit{Apoptosis} made by Drew Berry~\cite{Apoptosis}. Mind how the main reaction is forced to remain fairly stationary
		(upper arrow), while molecules in the background move very quickly (lower arrow). Illustrative timelapse is inspired by this effect, and aims at reproducing it based on real scientific data only.}
	\label{fig:apoptosis}
\end{figure}

Because of the overwhelmingly large number of simulation steps, it is sensible to only show simulation frames spaced at a constant given interval to reduce the viewing length. This is also referred to as fast-forward or timelapse.
A major challenge with the visualization of particle trajectories in fast-forward, is the strong visual clutter caused by the fast erratic motion of individual particles diffusing in every direction.
When viewing such results, one can observe the overall motion of swarming particles, but it is impossible to notice individual particles reacting.
However, to provide a clear visual explanation of a given process, it is crucial to showcase individual reaction events to illustrate the role of each key macromolecule present in the system.
But in order to visualize individual reactions, it is necessary to have a close-up on reacting elements and also to reduce the playback speed to the minimum to allow visual tracking of individual particles.
We are therefore facing the following conundrum: we can either observe the entire process in a reasonable amount of time without seeing individual reactions, or we can slow down the simulation and observe individual particles thus increasing greatly the viewing length.
A complete overview of the process would therefore require to constantly change the temporal resolution back and forth, as there is currently no technique that would enable us to see two phenomena occurring at two different time scales in a single view.
To address this issue, we propose a new type of visualization for particle-data inspired from animated illustrations of cell biology and that respects the following guidelines:
\begin{enumerate}
	\item The viewing length should be minimum and therefore the trajectory data should be played back in fast-forward.	
	\item Individual particles should move slow enough to be traceable with human eyes, especially those with a high degree of interest.	
	\item Events of interest, such as reactions, should be salient and last enough time in order to be noticeable by a human eye.	
	\item Despite eventual alterations to respect the previous guidelines, the visualization should be as realistic as possible to avoid misconceptions.
\end{enumerate}
The last requirement was formulated in accordance with the results of a study by Jenkinson et al.~\cite{jenkinson2012visualizing}, which demonstrated that a more realistic depiction of a process can enhance the viewer’s understanding compared to a highly abstracted one, we also observed the same principle in animated movies as shown in Figure \ref{fig:apoptosis}.


\subsection{Speed Reduction}

To reduce the viewing length of a simulation, the trajectories of the particles are played-back in fast-forward.
To achieve this, we only display snapshots of the trajectory at a fixed interval.
Thus, the spatial information between two displayed snapshots is simply omitted, resulting in larger position leaps between consecutive frames and faster particle motion.
Because of the increased speed of the particles, it might be difficult to keep track of individual particles between two consecutive frames, which prevents the viewer from observing key reaction events.
In accordance with our guideline rule Number 2, we apply a speed reduction filter to shorten the trajectories of individual particles.
The filter mechanism consists of reducing displacements between the current position of particles and their original trajectory positions sampled from the dataset at a given time.
The displacements are reduced according to the speed reduction factor as shown in Figure \ref{fig:speedreduction}.
Particles thus follow their original trajectories but are displaced over shorter distances and their speed is decreased.
As a result, the particle motion is much smoother and easier to follow by human eyes while preserving a fast playback speed.
The speed reduction filter also causes a small delay between particles and their original trajectories, however, because of the chaotic nature of the Brownian motion particle, this delay is hardly noticeable.
Indeed, diffusing molecules travel much slower than entities with a more linear motion, and therefore even if the particle lags behind its original trajectory it is almost certain to be located in its vicinity.

\begin{figure}
	\centering
	\includegraphics[width=0.5\linewidth]{graphics/SpeedReduction}
	\caption{The speed reduction principle. The black dots represent the non-filtered trajectory and the red line the reduced one. When sampling the next position from the raw trajectory, we downscale the displacement that would originally move the particle to the sampled position. This approach has the advantage of smoothing the trajectory while causing a minimum amount of latency. The displacement toward the raw-trajectory is downscale by a the speed reduction factor which controls the smoothness of the motion.}
	\label{fig:speedreduction}
\end{figure}

%As a result, particles always remain in the vicinity of their original trajectories and their motion is smoother and easier to follow by human eyes.
%because the magnitude of each displacement is proportional to the distance of a particle and its original trajectory, this delay does not accumulate and remains constant over time.
%\textbf{Ivan: two things, (1). Filtering is only possible for Brownian motion, it does not work well for linear motion. Brownian motion large overall trajectory but short particle travel.
%(2). Two phenomena, each in different time scale, therefore temporal inconsistency are very likely.}

\subsection{Reaction Highlighting}

In particle-based simulation of biochemical reaction networks, reactions are punctual events that occur upon collision of two potential reaction partners.
In the event of a reaction, the resulting products will immediately be injected in the system as the operation is performed in a single simulation step.
Therefore, it might be difficult to observe single reactions.
Additionally, when viewing the simulation in fast-forward, many important reaction events might simply be omitted due to frame dropping.
Without showing reaction events, molecular agents will appear and disappear from the visualization without providing the necessary visual clues to understand what might have caused it.
We therefore prolong the duration of the reactions to make them stand out and to inform the viewer about the nature of these events.
Shortly before each reaction event, we apply attraction forces to steer the reactants towards the reaction location. 
During this procedure the original motion of the particle is overwritten by attraction forces.
To emphasize reaction events, reacting elements are highlighted with vivid colors to contrast with the rest of the scene.
The slow attraction animation and the color highlighting thus guarantees that the viewer is informed that a reaction is about to happen.
Once a reaction is accomplished, the products are introduced and their original motion is restored.

%\begin{figure}
%\centering
%\includegraphics[width=0.5\linewidth]{graphics/movement}
%\caption{}
%\label{fig:movement}
%\end{figure}


\subsection{Lens Effect}

\begin{figure}
\centering
\includegraphics[width=0.5\linewidth]{graphics/SphericalLens}
\caption{The spherical object-space modifies the speed reduction of the particles based on their distance to the centre of a virtual sphere. Inside the sphere the trajectories of the particles are reduced so that the viewer can visually keep track of them. Outside the lens the particles are subject to their original chaotic motion thus conveying a true impression of chaos and stochastic motion.}
\label{fig:sphericallens}
\end{figure}

Because of the slow movement of the particles compared to their original trajectories, the viewer may be misguided about the true diffusion properties of molecular agents.
To preserve a realistic impression of molecular motion despite trajectory reducing, we introduce a temporal focus+context technique that applies visual abstraction of molecular trajectories solely in the foreground (focus), while showing more realistic ---yet often untraceable--- motion in the background (context).
By showing the actual particle trajectories in the background we want to create an illusion of chaos, thus informing the viewer about their real diffusion speeds while allowing him to observe important reaction events in the same view.
We propose two methods to achieve this effect.
The first method operates in object space and is described in Figure \ref{fig:sphericallens}.
We define a region anchored in front of the camera in which molecular agents will be subject to a significant speed reduction.
Outside of this region we progressively decrease the speed reduction until reaching the background zone where the original particle trajectories are shown.
The second method we propose operates in image space.
We limit the displacement of particles to a distance defined in pixel-space and based on the smooth pursuit eye-movement limit used in psychology research.
When using a perspective projection in the visualization, elements close to the camera appear to move much slower than those further away in z direction. 
In fact, their screen space velocity is approximately the same, this illusion is due to the differences in pixel coverage of particles that are located in the background.
This effect results in a smooth and continuous transition between the motion of front and background particles, which is more tedious to obtain with the world-space approach because it requires additional fine tuning.

\section{Quantity-Driven Particle Behaviour}

In systems biology, there exist various modeling approaches that are either deterministic, such as quantitative modeling, or stochastic, such as agent-based modeling.
One advantage of agent-based models, as discussed above, is the presence of spatial information that can easily be displayed in a 3D view.
Although the visualization may often be overly cluttered, we previously demonstrated that it is possible to apply filtering methods to improve the clarity and expose essential parts to the viewer.
However, the computational footprint of particle-based models is very high compared to quantitative models, because it must account for the reaction and diffusion of each single reacting entities, which are usually present in large quantities.
The simulation is therefore often precomputed prior to the visualization.
As simulations are complex to setup and to compute, the creation and exploration of multiple scenario is thus slowed by this workflow.
Additionally, with large and complex models, the resulting trajectory files may be challenging, over tens of gigabytes of data for $100000$ snapshots and only $5000$ individual particles, which is cumbersome to manipulate when dealing with a multitude of different scenario.

A more practical workflow for our use case would be to visualize the particle positions on-the-fly as the simulation runs.
With such an approach, it would also become possible to influence the course of a simulation, for instance, increase the temperature, or introduce new species, and to directly observe the impact of these changes without having to run another simulation or deal with overly large files.
Most-efficient simulation tools, such as Smoldyn~\cite{andrews2010detailed}, are able to compute single simulation steps in a matter of seconds, thanks to efficient parallel algorithms running on GPU.
A few tools additionally support \textit{in-situ} visualization of particle-based models computed in real-time with such an approach.
However, the computation is still too demanding to deliver a smooth user experience even on high-end commodity hardware, which prohibits the use of these tools for interactive and explanatory applications.

Additionally, to visualize and understand a biological system, one must be able to observe the logical cascade of reactions described in the reaction network, as depicted in scientific animations.
With particle-based modeling, it is impossible to influence where and when reactions will occur.
Therefore, the viewing of an entire cascade of reactions may be very tedious because it requires manual spatial exploration and waiting times between successive reactions.
To overcome this issue, we would have to gain control over where reactions might occur.
This can only be achieved by decoupling the quantitative information from the spatial information.
In other words, we would need to use a modeling approach that would allow us overwriting spatial information without influencing the course of the simulation.
Quantitative modeling methods, such as kinetic modeling, do not feature particle trajectory data.
They are also much faster to compute and could easily be simulated along with a complex 3D visualization with high frequency refresh rates.
To address the previous limitations of particle-based modeling, we thus chose to explore the use of quantitative information to drive 3D particle animations in real-time.

With particle-based modeling, the behaviour of individual particles directly influences the concentration of species over time, as reactions are only triggered by collision events.
Given a reaction $A + B \Rightarrow C$, for example, when two reactants $A$ and $B$ are closely located next to each other, then a reaction is likely to occur.
In the event of a reaction, elements $A$ and $B$ will then fuse to produce element $C$, thus increasing the concentration of element $C$ and decreasing the concentration of elements \textit{A} and \textit{B}.
In particle-based simulations, individual agents are thus actors of their own fates.
We propose a new type of particle system where the behaviour of particles is purely driven by quantitative information.
We introduce a new type of agent, whose behaviour mostly depends on external parameters, and which we dub \emph{passive agents}.
As the quantitative simulation advances, the system will query the number of reactions to occur at a given time, and subsequently force passive agents to react.
Hence, passive agents do not play any role in the simulation process, their only function is to act as a visualization proxy to show quantitative information in three dimensions.
While the visual output is hardly distinguishable from an actual particle-based simulation, the benefits of this approach is two-fold.
Firstly, the simulation of the reaction network is very lightweight to compute, which makes this approach much more usable for interactive applications.
Secondly, the spatial information is completely decoupled from the simulation, which means that we can have control over where reactions will occur.
This approach grants us the freedom to dynamically trigger a cascade of reactions directly in the viewport in order to facilitate storytelling of complex reaction networks.
This would not be possible with an agent-based simulation.

\begin{figure}
	\centering
	\includegraphics[width=0.8\linewidth]{graphics/figure2_1}
	\caption{Workflow of the passive agent-base system. A biological process is represented as a metabolic network model, which is numerically simulated. The controlling module, called Omniscient Intelligence, (OI) routinely reads the results from the quantitative simulation, and dispatches reaction events to the agents. Agents are diffusing in the 3D scene, passively waiting for reaction order from the OI. Upon reaction events the target agents are steered towards each others and forced to react. Finally the position and rotation of the molecules are passed to the rendered which renders the set of particles according to their structural information. Via user input it is also possible to interact with the simulation parameters and affect the outcome of the visualization.}
	\label{fig:figure21}
\end{figure}

The workflow of our passive-agents system is shown in Figure \ref{fig:figure21} and is laid out as follows:
On the one hand, we have a biological system described as a reaction network and modeled with a scientific simulation software using a kinetic modeling approach.
On the other hand, we have a 3D scene filled with potential reactants, and which is set according to the initial conditions of the model, such as the initial quantity and location of each species.
Each passive-agent additionally undergoes a 3D random walk motion to simulate the diffusion process, similarly to traditional particle-based simulation.
We then routinely query the quantitative simulation to fetch information about the number of reactions that are meant to occur at time $t$.
Assuming a reaction of type $A + B \Rightarrow C$, for example, the particle system will first randomly select two elements \textit{A} and \textit{B} based on their spatial proximity.
The two particles will then be subject to attraction forces until they enter in collision.
Upon collision of the reactants, the reaction is performed, products are injected and the reactants are removed from the system.
Finally, the reaction products return to a normal diffusion behaviour until they get chosen for a subsequent reaction defined in the network.
By default, the reaction participants are randomly chosen in the 3D scene, but it is also possible to set a reaction priority to a few elements present in the viewport, in order to show cascades of reactions without having to browse through the entire scene.



\chapter{Conclusion}
\label{sec:section5}

The initial aim of this PhD project was to develop a visualization platform that would keep the audience and multidisciplinary experts informed about state-of-the-art knowledge in cell biology, using interactive 3D visualization as means of communication.
We intended to use as much available scientific data as possible to reduce the amount of manual work traditionally needed for creating digital assets for such content, and make this enterprise possible only with a small team of researchers.
We also aimed at providing interaction, which is not usually present in traditional visual communication of cell biology, although it is clear that it would have a strong and beneficial impact on the learning experience.
Therefore, the presented techniques have a strong emphasis on GPU-computing in order to deliver a responsive user experience with high-frequency refresh rates. 
To achieve this ambitious goal, we first identified and isolated the most challenging parts of the pipeline and focused on addressing them individually.
In some cases, these challenges where addressed by improving current state-of-the-art techniques, such as large scale molecular rendering, but we also presented novel concepts to solve visualization problems that have not yet been investigated in the context of cell biology, such as the concepts of Omniscient Intelligence, Illustrative Timelapse or Visibility Equalizer.  

\section{Summary}

We first proposed a novel technique to render large sets of proteins, while preserving atomic-scale details when zooming in.
Previous methods utilized the concept of instancing, but also relied on ray casting, which fails to deliver high frequency refresh rates. 
We proposed to utilize the rasterization pipeline instead because it features a powerful tessellation engine that allows us to dynamically control the level of detail and reduce GPU-driver overhead. 
Fiber structures, such as DNA for example, are present in large quantity inside living cells, but tend to have a more complex structure than proteins. 
Therefore, as a follow-up, we extended this technique to support these structures.
We also increase the rendering speed by developing an occlusion culling scheme inspired by techniques traditionally used in graphically advanced computer games. 

Up to this point, the scenes used to showcase rendering techniques were made up for benchmarking purposes and not realistically modelled to reflect the correct density of proteins present inside living cells.
Our technique has proven to deliver fast frame rates, even when using realistic whole-cell atomic structures provided by the researchers of the Scripps Research Institute, and modelled with cellPACK.
These realistic models aim at reproducing a natural arrangements of proteins inside living cells and viruses, which are subject to molecular crowding and therefore densely packed next to each others.
As a result, when displaying the entire datasets, only the most frontal atoms are visible, thus hiding important structural information buried internally.
We also proposed a novel solution to solve occlusion problems when visualizing whole-cell structural models.
We utilize the fact that many macromolecules present inside the cells actually share a similar atomic structure to reduce the concentration of displayed elements of a given type, and clear the view to reveal hidden structures.
While the exact number of displayed elements is overridden, important information about the location of these structures and the proportions are preserved.

So far, we have only addressed challenges that are concerned with the visualization of static datasets.
However, to truly express the function of these structures, we must animate them to communicate where they travel and how they interact with their surroundings.
In computational biology, there is a modeling technique called particle-based modeling that can reproduce biological systems on the scale of entire cells while providing spatial information such as 3D trajectory and reaction history.
When directly viewing the results of such simulation, it is very difficult to observe the behaviour of individual particles because of the large number of elements present in the system, and because of their fast and erratic diffusion motion.
A solution to this problem would be to increase the temporal resolution of the visualization to observe individual particles, and to constantly go back and forth between two temporal scales to observe either the simulated process or single particles.
To improve the visualization of multi-scale processes, we propose a novel time-lapse method that is designed to visualize the results of a simulation at the temporal resolution of the system, while applying a speed reduction filter to particles so that their motion can comfortably be perceived by human eyes.
The filtering is only applied in the focus region while the original motion is preserved in the context area.
This partition helps to preserve the original impression of chaos and prevent us from misguiding the viewer.
This illusion was first introduced by the scientific illustrator Drew Berry in one of his animated films~\cite{Apoptosis}, we then formalized this concept and transposed it to scientific visualization.

A major downside of particle-based modeling is the large computational footprint, which obliges us to precompute the simulation before visualizing it.
This constraint prohibits us from providing a truly interactive experience where the viewer would be able to interact with the simulation parameters and instantly observe the changes.
Kinetic modeling is a strictly quantitative modeling approach, which is much faster to compute, but does not provide spatial information.
In order to utilize kinetic modeling to generate 3D spatial animations, we invented a new type of particle system that is able to convert the quantitative information into a spatial representation enhanced with structural information. 
We introduced the notion of passive agents, which unlike traditional agents, have their behaviour dictated by the omniscient intelligence, a controlling mechanism that utilizes the quantitative information to dispatch reaction order to the agents.
The particle system relies on GPU computing to routinely perform diffusion motion displacement and neighbouring search, allowing us to emulate the behaviour of several million molecules in real-time, while providing a similar visual output as genuine particle-based modeling.
Since spatial data and simulation are decoupled, we thus gain control over what particular elements are going to react and when.
With particle-based modeling, this behaviour is stochastic, which means that when focusing on a given particle, we have no guarantee that the particle will undergo a complete reaction cycle described in the pathway in a reasonable amount of time, which is impractical for story-telling.
With passive agents, we gain the ability to focus on a single element and to force it to undergo an entire reaction cycle, thus facilitating the viewing of relevant physiological events.

\section{Lessons Learned}

In the course of this thesis, we have learned that biological entities tend to feature a large number of elements sharing a similar structure, and that we can often use this specificity to our advantage when addressing visualization challenges.
For example, cells are composed of multiple sub-cellar entities, such as Mitochondria or Golgi apparatus, which are present in multiple instances.
The building blocks of cells are organelles are the proteins, in the case of Erythrotytes (red blood cells) for instance, about \sfrac{2}{3} of their structure is made out of haemoglobin proteins. 
The internal structures of proteins reveal long chains of smaller links, called amino acids, and it only exists a few tens of different types, which are mostly composed of 6 distinct atomic elements (C, N, O, H, S, Se).
The presence of so called "building block", or structures (n) repeated several times (N), is thus a phenomenon that is occurring on several magnification levels, from atoms up to entire cells.
In our contributions, we utilize this "n << N" specificity in the following contexts:

\textbf{Rendering}: we can render one object many times, we can use instancing and reduce the GPU driver overhead resulting into an immense speed-up if n << N.

\textbf{Memory Management}: instancing naturally reduces the memory footprint for storing the same object only once instead of multiple times, which is crucial for rendering a scene using consumer level graphics hardware.

\textbf{Occlusion Management}: we can utilize this characteristic when we want to cut away some part of the data. 
We can remove some instances of a particular building block as long as we keep few of them in the scene so that the viewer understands their spatial distribution in the cut-away area from the visible ones.

\newpage

\textbf{Visual Story-telling}: we can tell a story about a process using any structural building block, we do not need to use a particular one and and follow the course of a realistic particle behaviour that may not always reveal important information.
This gives us freedom for designing explanatory storytelling visualization techniques.

\section{Future Work}

The models employed to showcase the dynamics of biological systems are only simple yet realistic demonstration models, which do not truly compare in complexity with the large systems that we initially wanted to showcase such as en entire and functioning E.Coli bacterium.
Although we showed that the techniques we designed are able to support real scientific data, we yet have to combine structural and physiological composition for entire cells instead.
This ambitious goal could only be fully achieved with a tight collaboration with biology experts.
To facilitate the collection and validation of information from different online sources, it would also be interesting to develop automated means to collect this information and compile it into a meta-model comprising both structural and procedural information.
Hence, once new information is available, the system would be able to scan all the sources and update the model accordingly, thus saving us from tedious and cumbersome manual operations.

We also need to figure out how to scale upward with the models that we are able to render.
So far, we are able to render viruses or prokaryotic cells such as HIV, Mycoplasma mycoides or Escherichia Coli. 
Eukaryotes are the cells that compose living organisms such as plants and animals, therefore it is important to explain their mechanism to understand how we work.
These cells, however, are much bigger than prokaryotes as they contains membrane bound organelles such as Mitochondria or Golgi apparatus, which may be as large as a bacterium and are therefore much more challenging to render.
To render these larger structures we would need new ways of representing shapes other than spheres while preserving the massive zooming continuum.
We would also need to work even closer with the scientists to help them gather and extract important information from the raw data they collect.

An important aspect which we have not yet explored and would be worth investigating in future work is accurate real-time collision detection for large molecular datasets.
Although particle-based modeling or passive agents provides us the 3D trajectory of macromolecules, these approaches only consider molecules as a simple 3D point and the notion of shape or collision with surrounding bodies is simply ignored.
Thanks to the democratization of graphics hardware it is now possible to perform large-scale rigid-body collision-detection for up to several million of individual bodies, in real-time and on commodity hardware.
It would be worthwhile to integrate such system with the visualization of molecular landscapes, not only it would correct overlapping bodies when viewing dynamic data, but it would also allow us to generate large structures similarly to the ones modelled with cellPACK but in real-time.
Indeed, the computational footprint of cellPACK does not allow the creation of models in real-time, however, with such technology we would be able to overcome this limitation and to progressively model a sub-region of a cell that is visible in the viewport as we navigate though the scene.
This would also allow us to discard structural information which is not currently visible and address the memory space limitation of graphics hardware, thus enabling the visualization of even larger structures than the ones currently supported with cellVIEW.

Another aspect which is worth investigating is dynamic story-telling to visually explain biological networks that are usually described as cryptic diagrams and only comprehensible by domain experts.
While we have successfully accomplished the task of showing how elements of a system move and interact with other elements, it is now clear to us that showcasing only a realistic view of a system might not suffice to efficiently and visually communicate this type of information.
With the concept of Omniscient Intelligence, we have shown how to distort the normally stochastic behaviour to present a complete reaction cycle in a reasonable amount of time.
We now have to start thinking about new methods to morph this approach with more traditional 2D information and possibly provide intermediate views and transitions between realistic 3D representation and static 2D reaction diagrams.


\part{Selected Publications}

\renewcommand\appendixname{Paper}
\appendix



\chapter{Illustrative Visualization of Molecular Reactions using Omniscient Intelligence and Passive Agents}

\begin{flushright}
	Published in \cite{le2014illustrative} by the Eurographics Association.
\end{flushright}

\includepdf[pages=-]{papers/published/eurovis2014.pdf}


\chapter{Illustrative Timelapse: \\
	A Technique for Illustrative Visualization 	of Particle-Based Simulations}

\begin{flushright}
	Published in \cite{le2015illustrative} by IEEE.
\end{flushright}

\includepdf[pages=-]{papers/published/pacificvis2015.pdf}


\chapter{cellVIEW: \\
	A Tool for Illustrative and Multi-Scale Rendering \\
	of Large Biomolecular Datasets}

\begin{flushright}
	Published in \cite{le2015cellview} by the Eurographics Association.
\end{flushright}

\includepdf[pages=-]{papers/published/vcbm2015.pdf}


\chapter{Visibility Equalizer: \\
	Cutaway Visualization of Mesoscopic Biological Models}

\begin{flushright}
	Published in \cite{le2016visibility} by the Eurographics Association.
\end{flushright}

\includepdf[pages=-]{papers/published/eurovis2016.pdf}
