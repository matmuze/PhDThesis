\newacronym{ctan}{CTAN}{Comprehensive TeX Archive Network}
\newacronym{faq}{FAQ}{Frequently Asked Questions}
\newacronym{pdf}{PDF}{Portable Document Format}
\newacronym{svn}{SVN}{Subversion}
\newacronym{wysiwyg}{WYSIWYG}{What You See Is What You Get}

\newglossaryentry{texteditor}
{
	name={editor},
	description={A text editor is a type of program used for editing plain text files.}
}

\chapter{Abstract}


This is where the first chapter begins...


\chapter{Related Publications}

This is where the first chapter begins...

\chapter{Introduction}

\section{Introduction}

%\textbf{Importance of visualisation in the physiology}
%
%Physiology describes how livings organism work, and it spans across multiple scientific fields.
%
%It is therefore important to be able to communicate scientific advances efficiently between experts with various scientific background.
%
%Additionally, there is also a growing interest from the general audience to understand how their own body function.
%
%Explaining such complex processes is usually very challenging without a supporting illustration.
%
%Although some illustrators are still doing hand-made illustrations, many of them are using tools inspired from movie production to accelerate their work.
%
%Despite that, modeling a single scenario is still very time consuming (gathering data and realization).
%
%Moreover, illustrations are usually static, (still images and movies) and do not offer much interactivity.


\textbf{Importance of Visualisation in Biology}


%To being with this ambitious enterprise it is necessary to starting studying how processes work on the atomic level, the smallest level which we  understand to this day.
Biochemistry is the root of complex physiological processes that allow living organisms to function.
In order to understanding how we work we must understand the complete cascade of events from the macroscopic down to microscopic and atomic levels.
Because physiology truly spans across multiple scientific fields, it is crucial to communicate advances in biochemistry efficiently between experts with various scientific background.
Moreover, there is also a growing interest from the general audience to understand how their own biological machinery works.
Visual communication is without a doubt a very efficient way to educate a non-expert audience about the functioning of physiological processes.
A quick glance at physiological textbooks is enough to realize that they would be close to useless without any expression illustrations.
Printed illustrations have the clear advantage to be easily accessible by the viewer, via a school textooks, scientific magazines, encyclopedia, or simply via the web. 
Illustrations, such as the ones made by David Goodsell, for example, are often used to illustrate cellular biology textbooks.
The illustrator like to depict entire sceneries on the mesoscale levels that would be impossible to observe otherwise in such detail  using current optical instruments.
His illustrations have acquired a certain notoriety due their quality which can be assessed by two factors:
\\
\\ \textbf{How close is the depiction to the reality ?}
\\ \textbf{How clear is the depiction to the viewer ?}
\textbf{Show an illustration here and describe what is biochemical process shown.}
The realization of such illustrations, however, is very laborious and demand a lot of skills and time.
The first step is the conception, which consists of gathering knowledge from the literature in order to thoroughly understand the process that should be depicted.
%This step is crucial as it will determine knowledge to picture translation process.
Based on this knowledge the illustrator will decide which elements should be present, their location, quantities, and which behaviour should they exhibit.
This task demands a strong understanding of biology as scientific articles are intended to be read by experts and peers.
The second step is the realization, which consists of drawing a systematization of the process (give example of a process here).
It is important not to confuse the work of a scientific illustrator and the work of an artist.
Although they both aim at conveying a message or an idea, the artist has the freedom to hide him message behind an abstraction curtain in order to challenge the viewer.
On the contrary, the job of an illustrator is to make the communication process as clear as possible in order to expose scientific concepts to a laymen audience.
There are several key aspects to seriously take into account in order to achieve a high quality illustration:
\\
\\ \textbf{Composition} (Camera, Arrangements of objects, occlusion)
\\ \textbf{Lighting} (Depth perception, shading).
\\ \textbf{Color Coding} (Colors).
\\ \textbf{Storytelling}.

It also exists different types of supports other than still images such as animated movies (animated and non-interactive) and educational games (animated and interactive).
Some illustrators prefer working with pencils and brushes (D.Goodsell) while other prefer using computer-aided design software such as Photoshop (2D) or Maya/Cimema4D (3D).
The use of computer graphics software, however, is becoming increasing popular among illustrators because it greatly facilitates the drawing of 3D dimensional data, which is also becoming the standard in animated movie creation.
Perspective and lighting effects, for instance, are automatically handled by the software leaving more time to the artist to work on other aspects such as color coding, composition, animation or storytelling.
The storytelling is a key element of an illustration as it is responsible of conveying the core message.

Animation is also widely used by scientific illustrators as it is great way to engage the viewer into a story and to ensure a maximum degree of comprehension.
%Animated content is very powerful in illustration because it efficiently engages the viewer into the story (stupid comparison of people preferring movies to books).
Due to the popularity of digital effects used in the movie industry over the last decades, 3D animation packages and online training became widely available thus popularizing the use of these tools by scientific illustrators.
One notorious animated educational material is "The Inner Life of the Cell" realized in 2008 by the XVIVO medical illustration studio and appointed by Harward University.
This short animated video beautifully depicts in few sequences the basics of the complex biochemical processes of a living cell such as.....
\textbf{An immense effort was spend to only to show local reaction but also to represent the surroundings faithfully}
The local molecular interactions are embedded in their logical surrounding, which allows the viewer to understand the correlation when transitioning to a subsequent interaction.
The sequences also seamlessly transit between different zoom levels to understand the impact of small reaction on a higher level.
For reaching such standard in term of animation quality it took them 14 months to design, model, animate and render this movie.

Another way to engage the subject even deeper into a scenario is interactivity.
By embedding the subject into the presented content, he becomes an actor of the virtual world and his attention naturally increases. 
Moreover, the traditional reward system present in most computer games ensures that the player stay engaged for a very long duration (Game based-learning).
The motivation of a player, however, is not always solely driven by the high-scores.
We can observe a recent trend among AAA game titles towards interactive storytelling rather than pure gameplay.
Those titles usually feature cinematic sequences, interleaved between interactive sessions where the player has to successfully achieve a task.
The player motivation is then also driven by his curiosity to unravel the storyline.
This type of game usually keeps the player engaged for a duration of 50 to 100 hours.
Over the last years, the net revenues of the computer game industry have largely outperformed those of the movie industry, which was not the case a few years ago.
This trend clearly indicates the growing interest of the public in interactive entertainment and  storytelling. 
Moreover, the recent emergence of a new generation of virtual and augmented reality hardware such as the Occulus, HTC Vive, Microsoft Hololens, are now paving the way towards the next generation of computer games which promise to be even more immersive, realistic and engaging.

Given the raising interest of the general audience in this type of interactive experience we can imagine that this could be hugely effective for edutainment and scientific dissemination of cell biology.
Immune Attack or Sim Cell are two famous examples of edutainment titles whose goal is to reveal the functioning of living cell through accomplishment of actions that are part of the physiological processes.
\textbf{Those a nice examples but still quite low buget, too few titles out there}
The realization of interactive content, however, is more constrained compared to production of movies which can be a real challenge. Indeed due to fast framerates needed in computer games (20 to 60 fps even more for VR) it difficult to reproduce entire complex environment we may see in movies, which may affect the overall communication impact. 
Additionally the creation team also requires more people e.g. computer programmers, which may increase the production costs and times. 
Given that the creation of a high quality explanatory video about cell biology may already take up to several months or even years it is clear that the next generation of interactive scientific illustrations is going to need efficient tools to facilitate the content creation.

What we need is:
a. Faster tools to render
	Rendering several hundreds of thousands of molecular agents can take up to dozens of minutes to render a single image depending of the image quality presets such as resolution or lighting effects.

b. Faster tools to compose
	Creating large structures requires assembling thousands of proteins or lipids together based on available knowledge present in scientific literature.

c. Faster tools to generate story telling
	A great part to the creation process is spent understanding a given process in order to depict the clearest visual explanation as possible.
	Creating an animated scenario for an entire living organisms, such as the E.Coli bacteria would require spending a lot of time collecting, reading and understanding all the physiological processes that allow the cell to function.

%c. Faster tools to animate
%	Animation of a large amount of proteins, requires using physics simulation tools that similarly to rendering software are very time consuming for large scenes.

\section{Background}

\textbf{Computational Biology to Assist Scientific Illustration}

%In biology the cost of wet lab experiments is significantly higher than the cost of computer-based simulations.
%
%Such simulation have a great value because they provide important insight about the functioning of a system. 
%
%Over the last years the use of computer-based simulation have significantly increase in biology.

In biochemistry, it exists two methods to conduct analysis and understand how processes work, called dry and wet laboratories.
Wet laboratories are laboratories where chemical agents are phyiscally manipulated and then observed.
Dry laboratories are laboratories where computational or mathematical methods are employed for the modelling and analysis of biochemical processes. 
Over the last decades, the use of in-silico (dry lab) modeling and analysis has significantly increased due to development new software and the decreasing costs of super computers.
\textit{The goal of a model is to approximate a given process, a complete and accurate model might simply too complex to describe of to simulate, also some aspects may sometimes be unknown}
\textit{Isolate a single part of the entire machinery to easily study its mechanism.}
Because it may often impractical to accurately model a process in its entirety, dry laboratory experiments are often criticized for their lack of precision.
The analysis of such models, although often inaccurate, is nonetheless still quite valuable for researchers.
In 2013 Martin Karplus, Michael Levitt and Arieh Warshel were awarded the Nobel prize in Chemistry for their work of theoretical modelling for complex chemical system.
Their work showcase the importance of theoretical modelling as a tool to complement experimental techniques.
Indeed the collected information is served to formulate new hypothesis which can be later on verified in wet laboratories.
Wet-lab experiments are usually expensive to conduct, and therefore the analysis of theoretical modeling may bring guidance to researchers and save the time and money needed to run too many wet lab experiments.
As a result of the increase in popularity of dry lab experiments, a significant amount of simulation data has been gathered (input) and produced (output).
The data most frequently stored in digital format and can be easily shared to other peers via online databases.
Available data may comprise structural information (how things look) or procedural information (what to they do) for a large number of biochemical agents.
\textbf{What is structure information ?? give concrete example}
\textbf{New mesocale models are being developed to better understand the pysiology of viruses and cells.}
%Right now only static models exists, but thanks to our technology we are paving the way toward interactive visualization of larger dynamic structures.
\textbf{What is procedural information ?? give concrete example}
At this stage they have very limited ways to see how these models of physiology behave. 
Such a visual form is an abstraction that is hard to relate to what is visually observed in experiments. 
In interdisciplinary physiological sciences this might hamper communication of results. 
Based on these two information, however, it would be theoretically possible to digitally produce a visualization that would illustrate a given biochemical process.
This technology could save countless hours of work to illustrators because interesting scenario could be simply queried and fetched from a database and automatically scripted into digital storyline.
Furthermore assuming that the rendering and simulation to be real-time capable, the manipulation of the simulation parameters would provide the means to alter the course of the scenario on-the-fly, thus offering a interactive and engaging experience for the viewer.
\textbf{Add a concrete example to physiological process which could be interesting to interact with}.
This might be the technology that would revolutionize how to effectively communicate how these processes work.
\textit{Biology, medicine, and other sciences can strongly profit from a visualization of physiology in order to gain, verify, and communicate the knowledge and the hypotheses in this field.}
\textbf{A common framework to bridge real-time simulation and meaningful 3D visualization together}

%\textbf{Microscope analogy ??}
%
%\textbf{Mention analogy between the microscope as a tool for viewing experimental data, but the lack of unified tool for viewing theoretical data.}
%
%\textbf{Ant-farm analogy ??}

\subsection{Biological Systems}

\textbf{What is a process ? how to model it ?}

One important aspect to consider if we wish to improve how visual communication is made is the storytelling creation process.
The creation of animated illustration is a very time-consuming enterprise, it demands a lot of knowledge acquisition about physiology to come up with an accurate and satisfying storyboard.
Moreover, this process is not streamlined and it could greatly benefit from better and more automated tools.
For example there are several procedural descriptions of life function and some of these descriptions are also available in computerized form as simulations.
The procedural descriptions provide information about how a given system works and this information is typically used by illustrator to manually animate the cascade of reactions that describe of process (see apoptosis).
However the simulated data contains additionally information about how would the species actually behave throughout time, given a set of initial parameters such as which elements are present, in which quantities and also the reaction rate which may depend on the location or temperature.
We are therefore convinced that efficiently using available data from computational biology could be the key to automatize the challenging task of storyboarding.
The branch of biology which focuses of the study of procedural models is system biology.
System biology models describe a complete or partial \textit{biochemical} biological process in order to simulate and analyse its functioning in more details.
The scientist who simulate those models are interested in studying the evolution in concentration of certain species, and also sometimes their spatial distribution.
Most commonly they use computer-based simulation programs to gauge how those properties change over time.
Based on this information and their own knowledge they are able to formulate hypothesis which helps them to broaden their understanding of the machinery of life. 
The models, as we understand them, describe the different interaction between species, which elements react with each other, what is the product of the reaction, and how fast do they react.
Additionally the concentration of each individual species defines the initial state of the simulation.
%It is common to describe these interactions as a reaction network, called pathway, to visually communicate given processes in biology books for instance.
Complex reaction networks can also be described with a custom markup language (such as SBML) and then stored in a digital file and easily shared with other researchers.
Digital models descriptions are also widely supported by simulation programs to facilitate the initialization procedure.

It exists several simulation methods to compute the number of species present in a system at a given time, they can be classified into two groups according to their modus operanti which can be either deterministic or stochastic.
The deterministic methods relies on the use of differential equation solvers to deduce the concentration at a given time. 
The result depends mostly on initial species concentration and the reaction rates.
This type of simulation was first introduced and still remains very popular among system biologists because it is easy to compute and reliable also.
Another type of simulation method is agent-based modeling.
This computational method quite differs from the strictly mathematical approach, as it aims at reproducing the original reaction-diffusion behaviour of biochemical agents in three dimensional space.
This technology was primarily developed to study and understand complex migration pattern among animal of human population.
This concept was naturally transposed study the behaviour of chemical species as more capable computer hardware became available and affordable.

With agent-based modeling, actors of the studied physiochemical phenomenon are virtually represented as a 3D point in space and subject to constant diffusion motion inspired from the diffusion speeds which was observed in vitro.
New elements are introduced or removed following reaction events, and the scientific analysis are based on the current number of species at a given time.
The reaction between two potential reactant is based on their local proximity and the reaction rate.
When an elements is located closely enough, the system would evaluate the reaction probability based on the reaction rate which was observed in-vitro.
Should a reaction event happen between one, two or more potential actors of a reaction the resulting species of the reaction will be introduced and the reactants removed.
\textbf{(show figure)}
In few cases also, such as modeling of signalling processes, deterministic methods do not produce reliable results because of the very low concentration of reactive agents, and agents-based modeling would be preferred.
However a disadvantage of agent-based modeling, is the complexity of the computation.
While solving a ordinary differential equation system if very straightforward, agent-based models take into account and simulate each single reacting entity.
The results of such simulation can take up to days or week for larger systems, even of super computer and for this reason this methods still remain less popular than the simpler quantitative models.
An advantage of agent-based modelling for automated storytelling however, is the availability of spatial information, which quantitative does not provide.
In the following sections we will introduce two different techniques especially developed for automated storytelling and which make use of simulated data from deterministic and stochastic models, respectively.

\subsection{Biological Structures}

Structural biology is the branch of molecular biology which mainly focuses on studying the structure of proteins and nucleic acids (the form, how they look), and how they acquire their form.
Data acquisition methods such as x-ray crystallography are widely used to read in details the atomic structure of proteins, i.e, the positions or atoms, their type and the type of bonds between them.
Based on this information it is possible to decrypt the sequence of amino acids that forms a protein.
The PDB was created to facilitate the sharing crystallography results of proteins, and now features an impressive collection of protein structures which is easily available to anyone for free.
This information is vital to biologist as it allows them to understand the functioning of proteins, the structure is also to run molecular dynamics, which aims are reproducing in a computer simulation the interactions and forces that drive atom behaviour in reality.
The shapes of proteins are tightly related to their function and therefore accurate atomic structure is also important for scientific illustrators to accurately depict the shapes of proteins.
A few set of tools, (in the form of plugins to amination package) were developed to streamline the illustration process and generate a mesh-based representation simply from a protein description file.
X-ray crystallography is limited in a sense that it is not capable of capturing large and complex structures such as organelles, viruses of cells in their entirety.
Electron microscopy imaging on the other hand still does not offer enough resolution to capture the type of individual atoms which make the segmentation task between proteins extremely challenging.
So far the spatial arrangement of protein that form greater structures such as organelles, viruses and cells was a manual a cumbersome task. 
To fill the mesoscale gap between atomic level data acquisition, and cell level data acquisition, scientists from the Scripps Institute in San Diego have developed new sets of tools that able to procedurally construct large mesoscale structures based on input parameters, such as the shape and location of the compartment, and important information about the proteins that are present such as the shape, the number, the location and the spatial distribution.
Their goal is to generate multi-scale models that could be used an input to run large simulation (MD or BD) for a complete or partial organism.
Additionally the generated structure can also be used to generate accurate 3D models in animation packages, thus preventing illustrator to manually place and arrange proteins when depicting a larger protein ensemble.
This project was a step in the direction of our vision which is to improve how visual communication of molecular biology is done.
However the overwhelmingly large number of elements that compose large mesoscale structures begin to truly challenge animation packages, that were not design with such constraint in mind.
While it is still possible to render still images in very high quality, real-time visualization of these models is challenging, even with simplified protein structures meshes. 
This affects the productivity or those create the models, as well as those who are using it for illustration purposes.
Also it this limitation somehow compromise the transition to the next generation of interactive scientific illustrations for molecular biology as current visualization fail at delivery the minimum requirement in terms of frame rates for showcasing interactive content.
In the following sections we will present two work that aim at enabling real-time rendering and compositing of large mesoscale models using state of the art computer graphics and GPGPU techniques.

\section{Related Work}

Our vision is to improve visual communication for molecular biology and ultimately to implement interactive showcasing as a new media for educating the masses

In the visualization literature it a exists several works that are going in a similar direction.

Firstly in terms of rendering molecular structures.

To begin with molecular visualization is it important to mention the different types of representations that are commonly employed.

The simplest representation is the sticks model, where each bonds are represented with as line and color coding is used to indicate the atom type at the line extremities or joints.

A representation which is commonly understood by a larger audience is the Van der Waals representation also known as space filling. 
Atom are simply represented as spheres where the spherical radius correspond to the atomic radius.

Molecular surface representation were introduced to reveal information that were usually not very salient in other representation, such as the presence of pockets and cavities.
The output is usually a mesh that enclose small holes between atoms which are not accessible by solvent molecules.
This way it is easier to identify important reaction locations that are buried inside the proteins and accessible by small reactants.
Another popular representation is the secondary structure or cartoon representation which is highlights the properties of the proteins chains such as sheets or helices.

When representing chemical elements on the mesoscale level it might not always be necessary to show too much information about the proteins such as the bonds of the secondary structure as it would introduce too much visual clutter, the shape information however is important.

As polygon mesh-based data is the standard in computer graphics scientific illustrator like to use the molecular surface representation as it is compatible with their tools.
It is also possible to set the degree of smoothness of the surface and thus reduce the high frequency details while preserving overall shape and reducing the polygon count.

\textbf{(Bioblender, molecular maya, PMV, tools for illustration traditionally use meshes)}

While it would be theoretically possible to use a mesh to represent all the spheres using the Van der Waals representation, the polygon count would be overly high compared to a simplified molecular surface.

In most standard animation package that are supporting molecular biology mesh-based representations are used.

However, in standalone and more bleeding edge molecular visualization packages, such as QuteMol is it possible to show the highly detailed Van der Waals representation with much better performance than with mesh data.

Tariny et al where the first to introduce the technique of impostors for rendering molecular datasets. 

The principle inspires from the well-known billboarding technique borrowed from computer graphics and games. 

This method only requires two triangles to draw a perfectly smooth sphere while with the mesh-based option a few hundreds of polygons would be needed for drawing a sphere with a decent level of smoothness.










As the size of the molecular datasets increased, new visualization methods were specifically developed to keep up with the progress. 

Traditionally molecules 


Lampe et al. [8] 

The development of new modelling techniques for three dimensional molecular structures led to a rapid increase in size of the studied datasets. 

To keep up with the progress, 
pioneered real-time rendering of large-scale atomic data on consumer
level hardware. They extended the fast billboard-based approach
introduced by QuteMol [19], by rendering the structures per residues
rather than per atom, thus reducing the memory bandwidth usage
and the GPU driver overhead. Lindow et al. [11] followed-up by
using instanced rendering to draw entire structures instead. For each
molecule type, they generate a 3D grid of the atoms which is stored on
the GPU, and then ray-casted upon rendering. Falk et al. [3] further
refined the method with improved depth culling and hierarchical ray
casting to achieve faster rendering performance for even larger scenes


%It exists several modeling techniques that were developed to study the dynamism of a given process.
%
%Kinetic modeling take into account the concentration and reaction rates and predict the number of element present at a given time via numerical integration (ODE).
%
%Although no spatial information is taken into account the variation rates in concentration is very valuable to the biologists.
%
%
%Another type of physiological modeling is agent-based modeling.
%
%Agent-based modeling was initially developed for studying the behaviour of simple thinking digital entities (Animal population growth/decay)
%
%In the biological each agent represent a reactant and has a defined set of possible actions which are defined by the pathway.
%
%Unlike with kinetic modeling, the simulation is based on stochastic rather than determinism.
%
%The whole system is mimicking the reaction-diffusion principle, where each elements are diffusing randomly and occasionally reacting when in contact with a potentinal reaction partner.
%
%This type of simulation are designed to closely resemble how the process actually behaves, and therefore it would be ideal to exploit this type of results and to introduce it in our virtual microscope.
%
%\textbf{Microscopes cannot zoom as deep or slow down time but we potentially could}
%
%From visualizing such system come a few challenges, like exploration or across multiple spatial or temporal scales or occlusion management.
%
%
%
%The computation of agent-based simulation is very slow, the entire simulation must be precomputed prior to the visualization which can take hours or days.
%
%Unfortunately, no interactive change of scenario is possible with this type of simulation methods.
%
%On the other hands kinetic modeling is very fast to compute but does no provide spatial information.



%In biology an increasing number of experiments are conducted in silico because of the gain in time and running costs of in vivo experiements.
%
%Simulation are used to formulate hypothesis.
%Explain the different simulation methods particle based and kinetic models and how do they differ.
%
%The microscope was invented to observe the results of in vivo experiments
%
%\textbf{A microscope can observe cross sections...}
%
%It both shows what how things look and how things work.
%
%The results of computation often need to be visualized in order to be better understood.
%
%Visualization needs a good framework that would serve the same purpose as the microscope.
%
%Computational biology offers currently offer structural and functional information about organisms such as E.Coli.
%
%By integrating the two aspects together it would be possible to emulate processes that are even not observable as in vivo.
%
%With the certain advantage that the viewer would no longer be bound in either space or time. 

%
%\textbf{Procedural information available in public databases (pathway) could be used in conjunction with structural information to automatically generate animated scenario that would save a scientific illustrators a lot of time}.
%
%Rather than taking few months to create a single scenario, illustrators could potentially generate a plethora of scenario simply based on simulated models.
%
%Additionally, if the simulation is fast enough it would also be possible to generate a scenario that would not be predefined and be interactively changed by the viewer.
%
%The key in building the tools for the next generation of biological illustrations is interactivity.
%
%On the one hand, interactivity engages the viewer into a more immersive story, not only by moving around but by also by modifying the course of the story by introducing or removing key elements in the system and observe the consequences.
%
%On the one hand introducing interactive illustrations and thus, developing rendering and simulation technique that could run in real-time also means facilitating the work of scientific illustrators 
%which are currently using tools designed for offline movie creation and are therefore very slow to compute (ray tracing, large physics simulation).


%***************************************************************

%Say that the next step-up in scientific illustration is interaction/real-time graphics.
%
%Talk about the currently existing interactive things (Games, tools, installation, VR/AR).
%
%Talk about the importance of interactivity (Faster content creation, non deterministic story telling).
%
%Talk about the limitation is terms of time needed for asset creation.
%
%Talk about the vision, which is an interactive platform that would utilize the data available to  

%***************************************************************


%-----------------------------------------------

%\textbf{Towards a dynamic and interactive model of a cell (E.Coli)}
%
%\textbf{Requirement analysis}
%
%The heart of this thesis is joining together structural and functional data, and the introduction of interactive techniques for the production of biological illustrations.
%
%We see the introduction of interactive illustration as two fold:
%
%.Gain time in producing illustration (render/animation times)
%.Offer a more engaging user experience (multiple scenario exploration VR)
%
%%Dealing with mutliple local scales
%The challenge in rendering lies in rendering multiple scales when dealing with atomic level data.
%
%%Dealing with mutliple temporal scales
%The challenge in animation lies in how to simultaneously show processes operation at different time scales.
%
%%Dealing with interaction and scenario changes
%The challenge in interaction lies in the fact that simulation of spatial data is very slow
%
%%Dealing with occlusion
%Large amount of data which need to be rendering when observing data o

%-----------------------------------------------

%Illustration help to understand but they are time consuming (gathering data, and realization).
%
%Computer graphics programs already help illustrators to accelerate their work (Maya, Cinema4D)
%
%But there is still a lot of work to do.
%
%Constant improvement in game technologies in often applied across field to accelerate the artists job in the movie industry (real-time simulation, real-time rendering, interaction)
%
%This help scientific illustrators too, but often they are using dedicated tools which are not mainstream and therefore only very few process is made for them.
% 
%Structure:
%
%Dealing with Large Amount of Data 
%
%Dealing with Multiple Scales.
%
%Dealing with Stochasticity / Introducing interaction
%
%Dealing with Occlusion.

%Part C the visualization (occlusion)
%Part E spatial navigation 
%Part F temporal navigation 
%Part D scenario interaction 

\section{Using Computation Data for Storytelling}

\section{Efficient Rendering of Large Structures}

\section{Efficient Spatial Composition and Occlusion Management}




\section{Conclusion}

\chapter{Research Publications}


%
%
%
%
%
%Since \LaTeX\ is widely used in academia and industry, there exists a plethora of freely accessible introductions to the language.
%Reading through the guide at \url{https://en.wikibooks.org/wiki/LaTeX} serves as a comprehensive overview for most of the functionality and is highly recommended before starting with a thesis in \LaTeX.
%
%\section{Installation}
%
%A full \LaTeX\ distribution\index{distribution} consists of not only of the binaries that convert the source files to the typeset documents, but also of a wide range of packages and their documentation.
%Depending on the operating system, different implementations are available as shown in Table~\ref{tab:distrib}.
%\textbf{Due to the large amount of packages that are in everyday use and due to their high interdependence, it is paramount to keep the installed distribution\index{distribution} up to date.}
%Otherwise, obscure errors and tedious debugging ensue.
%
%\section{Editors}
%
%A multitude of \TeX\ \glspl{texteditor} are available differing in their editing models, their supported operating systems and their feature sets.
%A comprehensive overview of \glspl{texteditor} can be found at the Wikipedia page  \url{https://en.wikipedia.org/wiki/Comparison_of_TeX_editors}.
%TeXstudio (\url{http://texstudio.sourceforge.net/}) is recommended.
%
%\section{Compilation}
%
%Modern editors usually provide the compilation programs to generate \gls{pdf} documents and for most \LaTeX\ source files, this is sufficient.
%More advanced \LaTeX\ functionality, such as glossaries and bibliographies, needs additional compilation steps, however.
%It is also possible that errors in the compilation process invalidate intermediate files and force subsequent compilation runs to fail.
%It is advisable to delete intermediate files (\verb|.aux|, \verb|.bbl|, etc.), if errors occur and persist.
%All files that are not generated by the user are automatically regenerated.
%To compile the current document, the steps as shown in Table~\ref{tab:compile} have to be taken.