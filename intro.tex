\newacronym{ctan}{CTAN}{Comprehensive TeX Archive Network}
\newacronym{faq}{FAQ}{Frequently Asked Questions}
\newacronym{pdf}{PDF}{Portable Document Format}
\newacronym{svn}{SVN}{Subversion}
\newacronym{wysiwyg}{WYSIWYG}{What You See Is What You Get}

\newglossaryentry{texteditor}
{
	name={editor},
	description={A text editor is a type of program used for editing plain text files.}
}

\chapter{Abstract}

This is where the first chapter begins...

\chapter{Related Publications}

This is where the first chapter begins...

\chapter{Introduction}

%\textbf{Importance of visualisation in the physiology}
%Physiology describes how livings organism work, and it spans across multiple scientific fields.
%It is therefore important to be able to communicate scientific advances efficiently between experts with various scientific background.
%Additionally, there is also a growing interest from the general audience to understand how their own body function.
%Explaining such complex processes is usually very challenging without a supporting illustration.
%Although some illustrators are still doing hand-made illustrations, many of them are using tools inspired from movie production to accelerate their work.
%Despite that, modeling a single scenario is still very time consuming (gathering data and realization).
%Moreover, illustrations are usually static, (still images and movies) and do not offer much interactivity.

%Printed illustrations have the clear advantage to be easily accessible by the viewer, via a school textooks, scientific magazines, encyclopedia, or simply via the web. 
%To being with this ambitious enterprise it is necessary to starting studying how processes work on the atomic level, the smallest level which we  understand to this day.
%It also exists different types of supports for visual communication other than still images, such as animated movies (animated and non-interactive) and educational games (animated and interactive).
%Due to the popularity of digital effects used in the movie industry over the last decades, 3D animation packages and online training became widely available thus popularizing the use of these tools by scientific illustrators.
%The storytelling is a key element of an illustration as it is responsible of conveying the core message.
%Animated content is very powerful in illustration because it efficiently engages the viewer into the story (stupid comparison of people preferring movies to books).
%Due to the , 3D animation packages and online training became widely available thus popularizing the use of these tools by scientific illustrators.
%which became widely available thanks to the popularity of digital effects used in the movie industry over the last decades
%In order to understand how we work we must first understand the complete cascade of events down from atomic level and up to microscopic and macroscopic levels.

\textbf{Importance of Visualisation in Biology} \\
Biochemistry lies at the root of complex biological systems that describe the machinery of life, and in order to understand how we work we must first understand the complete cascade of events that is taking place at the atomic level.
Because biological systems spans several scales and scientific fields, such as biology, chemistry, mathematics, or medicine, it is crucial to communicate advances in biochemistry efficiently between experts with various scientific background.
Moreover, there is also a growing interest from the general audience to understand how their living organisms work.
Visual communication is undeniably an efficient way to educate a non-expert audience about the functioning of physiological processes.
A quick glance at physiological textbooks is enough to realize that they would be close to useless without any illustrations.
Illustrations, such as the ones made by David Goodsell, are often used to illustrate such textbooks.
The illustrator likes to depict entire sceneries on the mesoscale levels, such as ...(figure X) that would be impossible to observe otherwise in such detail using current optical instruments.
His paintings rightfully balance scientific accuracy and clarity, which makes them very popular because they are accessible to a large audience. 
The realization of such illustrations, however, is very laborious and also demands highly skilled individuals.
The first step of the creation process consists of gathering knowledge from the scientific literature in order to thoroughly understand the process that is to be depicted.
This task demands a strong understanding of biology as scientific articles are intended to be read by experts and peers.
Based on this knowledge, the illustrator will decide how to compose the scene, i.e., which macromolecular structures should be present, where should they be located, in which quantities, and what behaviour should they exhibit.
The second step of the creation process is the drawing. 
It is important not to confuse the work of a scientific illustrator with the work of an artist.
Although they both aim at conveying a message or an idea, the artist has the freedom to hide his message behind an abstraction curtain in order to challenge the viewer.
On the contrary, an illustrator has to convey a message as clearly as possible in order to expose scientific concepts to an uninformed audience.
This concretely means that the illustrator is bound to a set of logical guidelines in terms of composition, lighting, color-coding or storytelling.

While some illustrators prefer working with paper and pencil or 2D composition software such as Photoshop, the new generation of illustrators grew with 3D rendering and animation packages such as Maya, Cinema4D or Blender, made mainstream by the popularization of digital effects in the movie industry.
The use of such computer aided design tools greatly facilitates the drawing operation of three dimensional data.
Perspective and lighting effects, for instance, are automatically handled by the software, thus leaving artists more time to work on other aspects such as material design, composition, or post-processing.
Since less time is spent working on single images it is also less cumbersome to produce animated stories.
Consequently, 3D animation  became an increasingly popular means of visual communication to depict the machinery of life, as it is a great way to engage the viewer into a storyline.
One notorious animated educational material is "The Inner Life of the Cell" realized in 2008 by the XVIVO medical illustration studio and appointed by Harvard University.
This short animated video beautifully depicts in few sequences the logical cascade of events that describe complex biochemical processes of a living cell.
The structures of the different actors are based on real structural information available from public databases and their behaviour based on the most recent knowledge of cell biology.
Furthermore, environments surrounding each event is also accurately depicted to provide important information about location and scale.
This masterpiece of scientific animation took the whole team of experts 14 month to produce, which is a good average production time for a 5 to 10 minutes educational material of this quality.
Unfortunately, on top of being time consuming, the production of such films is also very costly, which somehow limits the accessibility and availability of such visualization.

%Over the last years, the net revenues of the computer game industry have largely outperformed those of the movie industry, which was not the case a few years ago.
%This trend clearly indicates the growing interest of the public in interactive entertainment and  storytelling. 
%Moreover, the recent emergence of a new generation of virtual and augmented reality hardware such as the Occulus Rift, HTC Vive, Microsoft Hololens, are now paving the way towards the next generation of computer games which promise to be even more immersive, realistic and engaging.
%Given the raising interest of the general audience in this type of interactive experience we can imagine that this could be hugely effective for edutainment and scientific dissemination of cell biology.
%By embedding the subject into the presented content, he becomes an actor of the virtual world and his attention naturally increases. 

%\textbf{Here break the read thread between games, and introduce the idea of entirely virtual biological systems.}

Another type of media which has a great potential for educational purposes is interactive applications.
Compared to movies, interactive titles, such as computer games, are able to keep the player engaged with educational content using the traditional reward system present in many computer games.
Immune Attack[] or Sim Cell[] are two famous examples of edutainment titles whose goal is to reveal the functioning of living cells through accomplishment of actions that are part of physiological processes.
Promising new VR devices have also emerged over the last years, and are now paving the way for more exciting and engaging user experiences that could have a great educational outreach.
However, the production of high quality interactive content, similarly to animated films, is also a lengthy and costly enterprise, as technicians and programming experts are also needed in addition to the team.
Interactive applications may also have an educational purpose without necessarily introducing gameplay mechanisms or score-based rewards. 
A good example is interactive maps applications, such as Google Earth[].
Unlike static maps, these applications enable on-demand access to specific information.
Through a set of 2D interactions such as zoom, rotate and pan, or 3D interactions such as tilt, the user is free to navigate to whichever part of earth that he deems interesting.
It also features multiple zoom levels from planets down to the size of building, houses or even cars.
Another strong advantage of the platform is crowd-based collaboration.
Three dimensional data obtained from scans of entire cities for instance can be provided by the municipalities and added to the platform for in-depth city architecture exploration.
Finally, the platform is not only bound to static representation of earth, dynamic data such as traffic or meteorology provided by third party applications can also be plugged and included to the platform.
The final outcome is a system that enables omniscient and three dimensional observation of the planet and its dynamics based on available data.
The educational outreach of this software is undeniable as it transcend all types of media previously used and provides unconstrained access to multiple types of information at once. 

To our knowledge, the concept of reproducing an observable virtual environment as such, has not yet been transposed to the level of an entire cell.
In order to achieve this enterprise one would need access to important data such as the three dimensional structures of macromolecules and greater ensembles that form the compartments and various organelles of the cells.
As cells carry multiple functions expressed in biological systems, reaction networks between micro and macromolecules, as well as dynamic data, such as molecular reactions and migration patterns also would be needed to digitally reproduce a truly observable living cell.
Fortunately, a large amount of biological knowledge is publicly available via online databases.
The Protein Data Bank[], for instance, is a project that aims at regrouping every known protein structures in large public data base.
Ecocyc[], is another project that aim at gathering extensive procedural descriptions of the biological systems that are ongoing in the E. Coli bacterium.
Based on these descriptions, biologists have managed to simulate processes of the entire bacterium at once on a super-computer and have also made the results available.
Similarly to satellite data, or traffic data in Google Earth, the data present in these databases are steadily updated with most recent scientific knowledge by a large crowd of researchers, for as long as the projects are maintained.
Despite the large quantity of data available, there is yet no solution that could generate a comprehensible digital cell based on this data, and that would be both dynamic and multiscale.
As the production of animated and interactive educational content is often prohibitive, we envision that such interactive platform would be a great mean to keep layman audiences and multidisciplinary experts informed about the latest knowledge of cell biology.

%\textbf{Here break the read thread between games, and introduce the idea of entirely virtual biological systems.}
%
%analogy to Google Earth (Virtual Earth)
%, not static maps, on demand access to specific inforation.
%Not a game, not movies, something new different concept.
%Google maps 
%
%Crowd based participation on structures...
%Structures vs dynamics of traffic for instance.
%Increasing incorporation of dynamics phenomena.
%Unconstrained access,
%
%What is needed for applying same concept to cell level ??
%What are structures, envrionements, actors, protein energy organelles reaction networks.
%
%(A large amount) these information is already available (PDB, cellpack, computational models)
%And is also steadily updated.
%
%(http://ecocyc.org/) Try to gather multiple of information in one place (Wiki)
%\textit{EcoCyc is a scientific database for the bacterium Escherichia coli K-12 MG1655. The EcoCyc project performs literature-based curation of the entire genome, and of transcriptional regulation, transporters, and metabolic pathways. }
%
%The data ----> google earth



%Another type of media which has a great potential for educational purposes is computer games.
%Immune Attack[] or Sim Cell[] are two famous examples of edutainment titles whose goal is to reveal the functioning of living cells through accomplishment of actions that are part of physiological processes.
%These titles are based on the traditional reward system present in many computer games and that ensures to keep the player engaged with educational content.
%The motivation of a player, however, does not always have to be solely driven by the high-scores.
%We can observe a recent trend among AAA game titles towards interactive storytelling rather than pure gameplay.
%Those titles usually feature cinematic sequences, interleaved between interactive sessions.
%The resulting user experience then blends animation and interaction, and the player's motivation is thus driven by his curiosity to unravel the storyline rather than high-scores.
%We envision to translate this concept to dissemination of sciences in order to produce more engaging user experiences that would raise awareness about cell biology.
%Interactive rendering would also enable new types of action for user-driver exploration that would not be possible otherwise with movies, such as camera navigation, selective visibility of certain elements to reveal important hidden structures, or interactive change of biochemical properties to modify the course of the scenario.
%Additionally, the recent emergence of highly capable virtual reality headsets promises the next generation of interactive experiences to be even more immersive and engaging.
%This could encourage even more people to explore and interact with virtual educational content.
%
%The production of high quality interactive content, however, is often more constrained than the production of offline animations.
%Indeed, due to fast framerates needed in computer games (20 to 60 fps, and even more for VR) it is difficult to interactively render entire complex environments that we may see in movies, which could affect the overall educational impact. 
%Additionally, the creation team also requires more people, e.g. computer programmers, which could increase the production costs and times compared to movies.
%Given that the creation of a high quality explanatory videos about cell biology is already a very lengthy and costly enterprise, it is clear that new tools and technologies would have be developed to enable the creation of next generation of interactive user-experiences that would keep audiences informed about cell biology. 
%
%Firstly, it is important to improve the real-time rendering capabilities of interactive content creation tools, in order to quickly display large and complex structures on par with those seen in animated movies.
%Secondly, is it also crucial to streamline the content creation process in order to reduce production times and costs.
%Computational biology generates data which contains valuable information about the structures of macromolecular elements and ensembles which compose molecular landscapes.
%It also comprise procedural descriptions of dynamic biological systems, obtained via computerised simulations.
%Our idea is to combine these two aspects to virtually reproduce an observable dynamic biological system in three dimensions.
%Animations would then be automatically generated by scientific data, thus saving illustrators countless hours of manual composition and animation.
%Ultimately, we envision this concept to lead to the popularization of digital and educational material and thus to improve visual communication to layman audiences and across scientific disciples.


%In biology the cost of wet lab experiments is significantly higher than the cost of computer-based simulations.%
%Such simulation have a great value because they provide important insight about the functioning of a system. %
%Over the last years the use of computer-based simulation have significantly increase in biology.
%\textit{The goal of a model is to approximate a given process, a complete and accurate model might simply too complex to describe of to simulate, also some aspects may sometimes be unknown}%
%\textit{Isolate a single part of the entire machinery to easily study its mechanism.}%
%These experiments are widely criticized because often deemed too approximative due to human knowledge or computational limitations.

\textbf{Computational Biology to Assist Scientific Illustration}\\
In biochemistry, it exists two distinct experimental protocols, respectively called dry and wet laboratories.
Wet laboratories are where chemical agents are physically manipulated and then observed.
Dry laboratories are where computational or mathematical methods are employed for the modelling and analysis of biochemical processes. 
Over the last decades, \textit{in-silico} (dry laboratories) experiments have significantly increased due to the development of new software and the decreasing costs of super computers.
%Due to computational or human limitations, it may be often impractical to accurately simulated a given process in its entirety.
Despite being often criticized for being too approximative, dry laboratory experiments represent a valuable source of information for researchers nonetheless.
In 2013 Martin Karplus, Michael Levitt and Arieh Warshel were awarded the Nobel prize in Chemistry for their work of theoretical modelling for complex chemical system.
Their work highlight the importance of theoretical modelling as a tool to complement experimental techniques as wet-lab experiments are usually complex and expensive to conduct.
The analysis of theoretical modeling brings researchers the necessary guidance to formulate new hypothesis which can be later on verified in wet laboratories, this saving the time and money needed to run too many wet lab experiments.
As a result of the increasing popularity of dry lab experiments, a significant amount of data has already been gathered and produced.
Data is often stored in digital format and can shared to other peers via online databases.
Structural biology and Systems biology are branches or molecular biology that both heavily rely on computational method.
Structural biology informs us about how things look, i.e., what is the atomic structure of a protein, while systems biology informs use about how things work, i.e., how micro and macromolecular interacts and to form a given physiological process.

%\section{Related Work}
%
%Our goal is to improve visual communication of molecular biology, and ultimately to promote interactive showcasing as a standard media for educating the masses about biology.
%Valuable data from computation biology is constantly produced by experts and is often publicly available. 
%It contains thorough description about how things look (structures) and how things work (processes), and we intend to make extensive use of this data to enable the next generation of scientific illustration.
%The visualization literature already comprises a few work that are going in a similar direction and we will distinguish between the related work related to structural visualization in the first section and follow-up with related work about visualization of biochemical processes.

\textbf{The Structures}\\
Structural biology is concerned by the structure of biological macromolecules (proteins and nucleic acids), and the relationship between molecular structure and function.
Data acquisition methods such as x-ray crystallography are traditionally used to read in details the atomic structure of proteins, i.e, the positions or atoms, their type and the type of bonds between them.
Acquired atomic structures are often stored in digital files and shared via public data bases such as the Protein Data Bank[] to facilitate collaboration among biologists.
This information is then processed to decrypt underlying important structural information and also used to run molecular dynamics (MD) simulations, which aims at reproducing atomic interactions and forces to observe the actual behaviour of macromolecules over time.
Visualization is an important component of this discipline because atoms are arranged and assembled in 3 dimensional space and therefore a visual representation is often required.
Biologists developed several types of representation to illustrate molecular structures, and they are also supported by mainstream visualization packages such as VMD[] or PyMol[].
The simplest representation is the sticks model, where each bonds are represented as a line, and color coding is used to indicate the atom type at the line extremities or joints.
The Van der Waals (VdW) surface, is probably the most commonly understood representation and which simply shows atoms as spheres whose radius corresponds to the atomic radius.
A popular representation among structural biologists is the secondary structure or ribbon diagram, it is used to reveal properties of the protein backbones such as sheets or helices.
Finally, the molecular surface representation is used to show a continuous surface that closely surround atoms of a protein and that also close small holes between atoms that are not accessible by small solvent molecules.
This method was first introduced to reveal information that is not salient enough with other types of representation, such as the presence of pockets and cavities buried in the protein structure and that can potentially host important reaction sites.
In scientific illustrations the shape of a protein is an important aspect to convey as it is tightly related to its function, and therefore the surfaces or VdW representations are often preferred and are also easier to understand.
Furthermore, VdW spheres and molecular surfaces can easily be stored as polygon meshes which are supported by 3D animation packages.
BioBlender[], Molecular Maya[], ePMV[], are examples of plugins for animation packages that were specifically developed to ease the loading and rendering of molecular surface meshes.

\textbf{More details about cellpack,
what are compartments, structures, combined into a recipe ... bla, bla
collision-based packing algorithm blablabla...}



X-ray crystallography is limited in a sense that it is not capable of capturing large and complex structures such as organelles, viruses, or cells in their entirety.
Electron microscopy imaging, on the other hand, still does not offer enough resolution to capture individual atoms which make the segmentation task between proteins extremely challenging.
So far, only little is known about spatial arrangement of proteins that form greater structures and their modeling would be a cumbersome and manual task.
To fill the mesoscale gap between atoms and cells, scientists from the Scripps Institute in San Diego have developed cellPACK[], a tool to procedurally construct large mesoscale structures based on input parameters, such as the shape and location of compartments, and important information about the proteins that are present such as the shape, the number, the location and the spatial distribution.
Their goal is to generate large atomic structures that could then serve as input to run large dynamics simulation for a complete or partial organism.
Additionally, the generated structures can also be used to create accurate 3D meshes of mesoscale structures in animation packages for illustration purposes.
%, thus preventing illustrator to manually place and arrange proteins when depicting a larger protein ensemble.
These large models are highly valuable to us, as they comprise complex data that would have to be modeled manually otherwise, thus compromising the production of illustrative content.
However, the overwhelmingly large number of elements that compose these mesoscale structures begin to truly challenge animation packages that were not design with such constraints in mind.
While it is still possible to render still images in very high quality, real-time visualization of these models is simply not possible, even with simplified meshes. 
This affects the productivity of those who create the models, as well as those who are using it for illustration purposes and it also compromises the transition to the next generation of interactive scientific illustrations.

To keep up with the increasing size of atomic datasets, Tarini et al.[] introduced a novel visualization technique inspired from 2D billboards, a popular concept in computer games.
The technique consists of drawing camera-facing 2D sphere impostors rather than tessellated 3D spheres when rendering individual atoms, thus considerably reducing the polygon count.
With their approach, they were able to interactively render large datasets (up to xxxk atoms) with very high quality details at a much lower cost than meshes.
Shortly afterwards, Lampe et al. [8] extended the billboard technique by leveraging the GPU rendering pipeline to reduce memory bandwidth usage and GPU driver overhead. 
Grottel at al. proposed to improve the rendering speed of large particle-based datasets by implementing occlusion culling to discard hidden particle chunks from the rendering pipeline, based on the depth information obtained in the previous frame.
Lindow et al. [11] followed-up the work of Lampe et al.[] by using instanced drawing of proteins structures.
The protein structures are priorly stored in small unique 3D grids and ray-traced in the fragment shader upon rendering.
Falk et al. [3] further refined the method by introducing depth culling and level-of-detail for the grid structures to achieve faster rendering performance for even larger scenes.
While the presented methods only support the VdW representation, a few techniques were also developed to improve the rendering of large and highly detailed molecular surfaces using GPU ray-casting and efficient supporting structures instead of meshes[][][]. 
However, these methods implement grid-based space partitioning to accelerate the rendering, and their usage is thus limited to rather small scenes, which is impractical for illustration.
Up to this point, the rendering methods presented in the literature have reached unprecedented levels of performance, in terms of size of supported dataset and rendering speed, thus enabling real-time rendering of large mesoscale models created with cellPACK[].
However, the minimum requirement for a comfortable user experience is between 20 and 60 Hz on average for games, and higher than 60 Hz for VR content, which is still out of reach for very large datasets as the state-of-the-art method is capable of rendering 10 billions atom at 4 fps, which additionally leaves little to no room for other type of computation such as animation or physics simulations.

Moreover, the presented solutions were specifically designed for efficient rendering of proteins data only.
None of the techniques mentioned above have proved to efficiently support other types of molecular structures that exhibit much more complex organization, such as lipid membranes, nucleic acids or fibers, which ought to be taken into account for a truer depiction of molecular landscapes.
Indeed, the properties of these structures can represent a challenge, especially for dynamic data because the assembling blocks of these structures are considerably smaller and also more numerous than with protein data.
Several work focus on the modelling and visualization of such structures, such as life explorer[] or lipid wrapper[].
However, none of these tools feature a visualization method that has the ambition to support overwhelmingly large datasets such as entire cells and which we deem crucial to achieve interactive and explanatory visualization of molecular biology.
Furthermore, macro-molecular elements are usually densely arranged inside compartments (see figure), which may results in serious occlusion problems when interactively exploring a dataset.
This visualization challenge has not yet been explored in the context of molecular visualization although it is crucial to address it to improve the quality of the user experience.
It is also important to mention that none of the presented method above has been integrated into an animation or game creation package, which makes impossible for content creators to use it is a practical scenario.
This review of the literature clearly underlines the lack of an efficient and integrated solution that would:
a) Allow real-time rendering of datasets up to the size of a single cell
b) Support all type of molecular structures efficiently
c) Implement efficient occlusion management
d) Provide tools that are embedded with animation or game creation package for optimized collaboration with various actors of the illustration process.

\textbf{Systems}\\
Systems biology is the branch of biology concerned with computational or mathematical modeling of complex biological systems.
The organization of biological systems spans several scales, on the molecular level they consists of reaction network between molecular agents such as enzymes, metabolites, or proteins, also known as pathway.
Those systems describe the multitude of functions that are carried out by living cells, such as energy production, gene expression, and ability to divide or die.
Based on the pathway description, scientists reproduce the dynamics of a system \textit{in silico}, via simulation tools, and observe the changes in species concentration over time.
The results of the simulation are then further analysed to predict and understand how these systems change over time and under varying conditions, and potentially develop solutions to health issues.
The complex reaction networks are usually described with a custom markup language, such as SBML[], and used as input for the simulation tools.
Similarly to protein structures the system descriptions are often shared with peers via public online databases[].
Biologists have developed several methods to simulate the dynamics of a system.
Depending on their modus operandi the modeling approach can either be deterministic or stochastic.
Models may also feature spatial information or be purely quantitative. 

Quantitative modeling (or Kinetic modeling) relies on the use of differential equation systems to compute the species concentrations at a given time and is therefore deterministic.
Results only vary according to the initial conditions such as concentrations and reaction rates that are predefined in the model.
Additionally, the models may also feature spatial information such as compartmentalization of species.
Quantitative was the first modeling method introduce and still remains very popular among system biologists because it is reliable and computationally inexpensive.
%A typical visual output for this type of simulation is a time-concentration plot.
Another type of modeling is agent-based modeling.
This method completely differs from the strictly mathematical approach used in Kinetic modeling.
It aims at reproducing the original reaction-diffusion behaviour of biochemical agents in three dimensional space and is therefore stochastic.
This technology was primarily developed to simulate and understand complex migration pattern among animal or human populations.
The concept was then transposed to study the behaviour of chemical species as more capable computer hardware became available and affordable.
With agent-based modeling, actors of systems are virtually represented as a 3D point in space and subject to constant random motion based on diffusion speeds observed in vitro.
New elements are introduced or removed according to individual reaction events.
Reaction events are triggered based on local proximity of potential reaction partners and reaction probability based on the reaction rates observed in-vitro.

It exists a few popular tools that aim at connecting the modeling, simulation, and data analysis in a single framework to facilitate the task of biologists, such as CellDesigner[] TinkerCell[] or VCell[].
These tools usually cover non-spatial models (quantitative modelling), except VCell[] which also supports the use of external agent-based simulation modules such as Smoldyn[].
At this stage, scientists studying these models have very limited ways to see how these mathematical models of physiology behave.
They can interact with the model by specifying input parameters to the simulation and the resulting visualizations are often time-concentration plots.
Even when the simulation method produce potentially interesting 3D trajectory data, these tools will favour highly abstracted visualizations which only experts can understand.
Such a visual form is hard to relate to what is visually observed in wet-lab experiments. 
In interdisciplinary physiological sciences this might hamper communication of results. 
However, the underlying data present in the models contains thorough dynamic descriptions of how these biological systems work.
These models comprise, for instance, which elements are presents, with whom do they react, a which rates, and even sometimes there location is also provided.
When associated with corresponding structural information this data could potentially be used to automatically generate explanatory animated illustrations.
Biology, medicine, and other sciences can strongly profit from a visualization of physiology in order to gain, verify, and communicate the knowledge and the hypotheses in this field.
Additionally, dynamics simulations, when computed along with the visualization could enable online changes of simulation parameters, such as species quantities or temperature, and directly observe how it would affect the system.

Few specific tools feature three dimensional visualization of particle trajectories obtained from agent-based simulation results, supporting either playback of pre-computed data [CellBlender][Falk] or in-situ visualization of simulation computed in real-time [ZigCell].
In the case of signalling pathways modeling, for example, such visualization may be informative to scientists that are interested in observing the spatial distribution of certain species over time.
To a certain extent, this information could also have an informative value for the laymen because it depicts complex processes in the form of a 3D animation.
The viewing of actual three dimensional molecular interactions, such as the ones observed in scientific animations, could then be used as a mean to automatically generate expressive illustration of the simulated system.
Naive visualization of the raw trajectory data, however, may often results in an overly cluttered view due to the large number of elements randomly moving in every directions.
To ensure a maximum degree of clarity it is therefore crucial to provide the means for guiding the viewer in such complex scenes. 
Falk et al.[] added to the raw visualization the possibility to track single elements throughout time, by tracing the trajectory of a particle and highlight the cascade of reactions which is relevant for the study of signalling pathways.
Although previous works attempted to improve the raw visualization of particle-based visualization, there is still left to address the several grand challenges such as occlusion management for dense scenes or the issues that are due to time-scale discrepancies between entire processes and individual reactions, and which is described as follows: 
On the one hand, observing the particle-based simulation at a slow pace to be able to keep track of individual elements and see interactions would require hours to explore the entire process.
On the other hand, observing the simulation in fast-forward would shorten the viewing length but it would become impossible to observe important reaction events because of the fast pace at which elements would be moving.

While solving ordinary differential equation systems of kinetic models is very straightforward, agent-based models must represent and simulate every single reacting entities, and that are usually present in large quantities.
The simulation of particle-based systems is thus very demanding in terms of computation and is usually precomputed prior the visualization.
This limitation thus prohibits online interaction with the simulation parameters and real-time exploration of multiple simulation scenario.
Even most-optimized simulation tools that leverage the power of GPU computing [][], specifically developed for \textit{in-situ} visualization are still not able to deliver a true real-time user experience.
Quantitative modeling methods do not feature 3D dimensional data but are also much faster to compute, and could easily be simulated along with a complex 3D visualization.
Therefore it would also be interesting to explore the use of real-time quantitative information to animated 3D particle based on real simulation data but at a much lower cost than agent-based methods.


\section{Section 1}

\section{Section 2}

\section{Section 3}

\section{Section 4}

So what, our vision is still not done.

We have large structures but cannot render then quickly enough.
Crucial to be quick.



%\section{Related Work}
%
%Our goal is to improve visual communication of molecular biology, and ultimately to promote interactive showcasing as a standard media for educating the masses about biology.
%Valuable data from computation biology is constantly produced by experts and is often publicly available. 
%It contains thorough description about how things look (structures) and how things work (processes), and we intend to make extensive use of this data to enable the next generation of scientific illustration.
%The visualization literature already comprises a few work that are going in a similar direction and we will distinguish between the related work related to structural visualization in the first section and follow-up with related work about visualization of biochemical processes.

%\textbf{Structures}
%
%Structural biology is the branch of molecular biology that focuses on studying the structure of macromolecular elements in order to better understand their function.
%Visualization is an important component of this discipline because atoms are arranged and assembled in 3 dimensional space and therefore a visual representation is very often required.
%Scientist are using different types of representation to illustrate molecular structures, and we will describe the most commonly used ones.
%The simplest representation is the sticks model, where each bonds are represented as a line and color coding is used to indicate the atom type at the line extremities or joints.
%A popular representation among structural biologists is the secondary structure or ribbon diagram, it is used to reveal properties of the proteins backbone such as sheets or helices.
%The molecular surface representation is used to show a continuous surface that closely surround atoms of a protein and that also close small holes between atoms that are not accessible by small solvent molecules.
%This method was first introduced to reveal information that is not salient enough with other types of representation, such as the presence of pockets and cavities buried in the protein structure and that can potentially host important reaction sites.
%In popular molecular visualization packages such as VMD[] or PyMol[], molecular surfaces are stored as polygon meshes, for the simple reason that meshes are widely supported by graphics APIs.
%Another representation which is commonly understood by a larger audience is the Van der Waals surface, which simply represent atoms as spheres whose radius corresponds to the atomic radius.
%Compared to molecular surfaces, the Van der Waals representation provides important information about atomic composition of proteins which is not clearly visible with molecular surfaces.
%The Van-der-Waals surfaces are usually represented as meshes, but because the surface features highly detailed asperities it often requires a high polygon count to obtain a decent mesh quality, and which may overload the rendering pipeline when a large number of proteins is displayed.
%Molecular surfaces are therefore a commonly used representation among illustrators for depicting large and complex scenes because they can easily import and render meshes with 3D animation packages, and also because surfaces meshes are more lightweight than Van der Waals meshes.
%BioBlender[], Molecular Maya[], ePMV[], are examples of plugins for animation packages that were specifically developed to ease the loading and rendering of molecular surface meshes.
%To improve rendering performances of highly detailed molecular structures, Tarini et al. introduced a novel visualization technique to render Van der Waals surfaces with a much lower polygon count than with surface meshes.
%
%
%
%The rendering method is inspired from 2D billboards, a concept commonly used in computer graphics and games, and consists of drawing camera-facing 2D sphere impostors rather than tessellated 3D spheres when rendering individual atoms, thus considerably reducing the polygon count.
%With their approach, they were able to interactively render large datasets (up to xxxk atoms) with very high quality details.
%This method inspired several follow-up works that address the challenge of the constantly increasing size of available datasets in molecular biology.
%Lampe et al. [8] pioneered real-time rendering of large-scale atomic data on consumer level hardware. 
%They extended the fast billboard-based approach introduced by Tarini et al. [19] by leveraging the geometry shader to instantiate entire atom-blocks rather than single atom, thus reducing the memory bandwidth usage and GPU driver overhead by invoking fewer draw operations. 
%Grottel at al. proposed to improve the rendering speed of large particle-based datasets by implementing occlusion culling to discard hidden particle chunks from the rendering pipeline, based on the depth information obtained in the previous frame.
%Lindow et al. [11] followed-up the work of Lampe et al. by using instanced rendering of entire protein structures instead. 
%For each molecule type, they precompute a 3D grid containing the atomic structure and which is stored on the GPU.
%Upon the drawing of a protein the bounding box is drawn first, then during the per-fragment operation rays are cast through the grid, in order to find the first atom which is hit by the ray.
%Falk et al. [3] further refined the method by introducing depth culling and level-of-detail for the grid structures to achieve faster rendering performance for even larger scenes.
%Up to this point, the rendering methods presented in the literature have reached unprecedented levels of performance, in terms of size of supported dataset and rendering speed.
%However, the minimum requirement for a comfortable user experience is between 20 and 60 Hz on average games, and higher than 60 Hz for VR content, which is still out of reach for very large datasets (10 billions atoms at 4 fps).
%Moreover, the presented solutions were specifically designed for efficient rendering of macro molecules only, such as proteins.
%None of the techniques mentioned above have proved to efficiently support other types of molecular structures that exhibit much more complex organization, such as lipid membranes, nucleic acids or fibers, which ought to be taken into account for a truer depiction of molecular landscapes.
%Indeed, the properties of these structures can represent a challenge, especially for dynamic data because the assembling blocks or these structures are considerably smaller and also more numerous than with protein data.
%Several work focus on the modelling and visualization of such structures, life explorer [] is a visualization software designed for interactive modelling and viewing of DNA strands, lipid wrapper[] is a tool designed to generate lipid membranes based on arbitrary meshes.
%However, none of these tools feature a visualization method that has the ambition to support overwhelmingly large datasets such as entire cells and which we deem crucial to achieve interactive and explanatory visualization of molecular biology.
%
%Moreover, macro-molecular elements are usually densely arranged inside compartments (see figure), which may results in serious occlusion problems when interactively exploring a dataset.
%This visualization challenge has not yet been explored in the context of molecular visualization although it is crucial to address it to improve the quality of the user experience.
%
%This review of the literature clearly underlines the lack of an efficient and integrated solution that would:
%a) Allow real-time rendering of datasets up to the size of a single cell
%b) Support all type of molecular structures efficiently
%c) Implement efficient occlusion management
%d) Provide tools that are embedded with animation or game creation package for optimized collaboration with various actors of the illustration process.

%----------------------
%Several molecular surface computation techniques have been developed, each one with their own drawbacks and advantages.
%SAS[] and Gaussian[] are easy to compute but offer less informative details.
%SES[] and MSS[] are more complex to compute and also provide more surface details which can facilitate the cavity and pocket exploration.
%----------------------

%\textit{and also because smooth surfaces have a much lower overhead than highly tessellated Van-der-Waals surfaces}
%While meshes are often used because of practical reason, they also have a significant computational overhead when featuring a high level of asperity details such as with SES or Van-der-Walls because of the large number of polygon needed to smoothly discretize a complex surface.
%In the visualization literature it exists a few techniques that were developed to overcome this limitation by using a ray-casting approach and grid-based supporting structures.
%Since the recent democratization of domestic and massively parallel computing processors (GPU) it has been possible to implement efficient SES computing and rendering methods [LBPH10,KGE11].
%These methods are able to render the SES for dynamic data sets of around 100k atoms interactively on current hardware.
%Subsequently, Krone et al. implemented a method allowing smooth rendering of Gaussian surfaces for large and dynamic molecular data based on a massively parallel GPU algorithm. 
%%Atoms are stored in a 3D grid, which allows fast accumulation of Gaussian density for each voxel.
%While their technique only supports Gaussian surfaces, which offer less details than SES, they are also able to render much larger dynamic datasets composed of several million atoms.
%Based on the idea that highly detailed molecular surfaces are only needed when closely observing a protein, Parulek et al. introduced the concept of continuous level-of-detail for molecular surfaces.
%They seamlessly transit from highly detailed (SES) to less detailed (Gaussian) surface rendering based on the distance to the camera.
%Because the most complex operation is only performed for a small region of the overall protein, this method provides faster rendering speeds while also reducing visual complexity caused by unnecessary surface details, in the context area.

%The trend in recent molecular surface visualization methods clearly leans towards faster and highly optimized GPU implementations.
%The supporting structures are three dimensional grids because operations performed on such structures can be easily parallelized, thus more efficient.
%The use of uniform spatial partitioning schemes however limits the overall size of the scene, as the complexity and memory usage increases exponentially as the resolution of 3D grid increases, which is why such techniques were not implemented in 3D animation packages and employed by scientific illustrators.
%To overcome this size limitation of the scene, De Hera Chomski[] implemented a CPU-based ray tracing software based on a bounding volume hierarchy (BVH) instead.
%The use of such acceleration structure enable real-time rendering of large and complex scenes, however dynamic data requires constant update of the BVH which adds additional complex work on top of the rendering and limits the real-time capability of the method.

% which is why illustrators often use smooth and simpler surface representations such as the Gaussian instead.

%\textbf{Processes}

%Modeling systems biology processes 
% calls for efficient visualization methods to cope with the complexity nature of the data resulting from the simulation.

%The principle of systems biology is to reproduce complex physiological processes \textit{in silico} in order to better understand how living organisms work.
%Firstly, the models are designed throughout descriptions of molecular interactions that are laid down in 2D reaction networks and digitalized.
%Subsequently, the models are used as input for computational simulations and finally, simulation results are used for further analysis and hypothesis verification.
%It exists a few popular visualization software that aim at connecting the modeling, simulation, and data analysis in a single framework to facilitate the task of biologists, such as CellDesigner[] TinkerCell[] or VCell[].
%These tools usually cover non-spatial models (kinetic modelling), except VCell which also supports the use of external particle-based simulation modules such as Smoldyn[].
%Although the simulation data may feature 3D particle trajectories, the framework do not include a visualization module for observing the behaviour of individual 3D particles, simply because it would have only very little value to most experts as they are more interested in studying changes in species concentration.
%However, in few specific cases, such as the study of signalling pathways for example, the 3D visualization of particle-based simulation results is thought to be informative to scientists that are interested in studying the spatial distribution of certain species over time.
%To a certain extent, this information could also have an informative value for the laymen because it depicts complex processes in the form of a 3D animation, similarly to an explanatory movie.
%Specific visualization tools where developed to produce 3D animations out of such particle-based simulations, supporting either playback of pre-computed data [CellBlender][Falk] or in-situ visualization of simulation computed in real-time [ZigCell].
%Naive visualization of the raw trajectory data, however, may results in an overly cluttered view due to the large number of elements moving in random directions, and it is therefore crucial to intuitively guide the viewer to ensure a maximum degree of clarity.
%Falk et al.[] added to the raw visualization the possibility to track single elements throughout time by tracing the trajectory of a particle and highlight the cascade of reactions which is relevant for the study of signalling pathways.
%Although previous works attempted to improve the raw visualization of particle-based simulation with useful overlaid information, there is still left to address the grand challenge which is due to time-scale discrepancies between entire processes and individual reactions, and which is described as follows: 
%On the one hand, observing the particle-based simulation at a slow pace to be able to keep track of individual elements and see interactions, would require hours to explore the entire process.
%On the other hand, observing the simulation in fast-forward would shorten the viewing length but it would become impossible to observe important reaction events because of the very fast pace at which elements would be moving.
%Furthermore, the real-time limitations of particle-based simulation tools still has to be addressed in order to deliver a smooth educational user experience.
%Indeed, the simulation of particle-based systems is very demanding in terms of computation and is usually precomputed prior to the visualization, which prohibit online interaction with the simulation parameters and real-time exploration of multiple simulation scenario.
%The most-optimized simulation tools leverage the power of GPU computing to speed-up the computation and enable \textit{in-situ} visualization [ZigCell].
%However, even most optimized particle-based simulation methods are still not able to compute complex models in a reasonable amount of time to enable a smooth and interactive experience.
%Other modeling methods, such as kinetic modeling, do not feature 3D dimensional data but are much faster to compute, and therefore quantitative information could also be used to interactively drive 3D animated particle data for explanatory purposes.

%***********************************************************
%Say that the next step-up in scientific illustration is interaction/real-time graphics.
%
%Talk about the currently existing interactive things (Games, tools, installation, VR/AR).
%
%Talk about the importance of interactivity (Faster content creation, non deterministic story telling).
%
%Talk about the limitation is terms of time needed for asset creation.
%
%Talk about the vision, which is an interactive platform that would utilize the data available to  
%***************************************************************


%-----------------------------------------------

%\textbf{Towards a dynamic and interactive model of a cell (E.Coli)}
%
%\textbf{Requirement analysis}
%
%The heart of this thesis is joining together structural and functional data, and the introduction of interactive techniques for the production of biological illustrations.
%
%We see the introduction of interactive illustration as two fold:
%
%.Gain time in producing illustration (render/animation times)
%.Offer a more engaging user experience (multiple scenario exploration VR)
%
%%Dealing with mutliple local scales
%The challenge in rendering lies in rendering multiple scales when dealing with atomic level data.
%
%%Dealing with mutliple temporal scales
%The challenge in animation lies in how to simultaneously show processes operation at different time scales.
%
%%Dealing with interaction and scenario changes
%The challenge in interaction lies in the fact that simulation of spatial data is very slow
%
%%Dealing with occlusion
%Large amount of data which need to be rendering when observing data o

%-----------------------------------------------

%Illustration help to understand but they are time consuming (gathering data, and realization).
%
%Computer graphics programs already help illustrators to accelerate their work (Maya, Cinema4D)
%
%But there is still a lot of work to do.
%
%Constant improvement in game technologies in often applied across field to accelerate the artists job in the movie industry (real-time simulation, real-time rendering, interaction)
%
%This help scientific illustrators too, but often they are using dedicated tools which are not mainstream and therefore only very few process is made for them.
% 
%Structure:
%
%Dealing with Large Amount of Data 
%
%Dealing with Multiple Scales.
%
%Dealing with Stochasticity / Introducing interaction
%
%Dealing with Occlusion.

%Part C the visualization (occlusion)
%Part E spatial navigation 
%Part F temporal navigation 
%Part D scenario interaction 

\section{Using Computation Data for Storytelling}

\section{Efficient Rendering of Large Structures}

\section{Efficient Spatial Composition and Occlusion Management}

\section{Conclusion}

%Since \LaTeX\ is widely used in academia and industry, there exists a plethora of freely accessible introductions to the language.
%Reading through the guide at \url{https://en.wikibooks.org/wiki/LaTeX} serves as a comprehensive overview for most of the functionality and is highly recommended before starting with a thesis in \LaTeX.
%
%\section{Installation}
%
%A full \LaTeX\ distribution\index{distribution} consists of not only of the binaries that convert the source files to the typeset documents, but also of a wide range of packages and their documentation.
%Depending on the operating system, different implementations are available as shown in Table~\ref{tab:distrib}.
%\textbf{Due to the large amount of packages that are in everyday use and due to their high interdependence, it is paramount to keep the installed distribution\index{distribution} up to date.}
%Otherwise, obscure errors and tedious debugging ensue.
%
%\section{Editors}
%
%A multitude of \TeX\ \glspl{texteditor} are available differing in their editing models, their supported operating systems and their feature sets.
%A comprehensive overview of \glspl{texteditor} can be found at the Wikipedia page  \url{https://en.wikipedia.org/wiki/Comparison_of_TeX_editors}.
%TeXstudio (\url{http://texstudio.sourceforge.net/}) is recommended.
%
%\section{Compilation}
%
%Modern editors usually provide the compilation programs to generate \gls{pdf} documents and for most \LaTeX\ source files, this is sufficient.
%More advanced \LaTeX\ functionality, such as glossaries and bibliographies, needs additional compilation steps, however.
%It is also possible that errors in the compilation process invalidate intermediate files and force subsequent compilation runs to fail.
%It is advisable to delete intermediate files (\verb|.aux|, \verb|.bbl|, etc.), if errors occur and persist.
%All files that are not generated by the user are automatically regenerated.
%To compile the current document, the steps as shown in Table~\ref{tab:compile} have to be taken.