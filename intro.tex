\newacronym{ctan}{CTAN}{Comprehensive TeX Archive Network}
\newacronym{faq}{FAQ}{Frequently Asked Questions}
\newacronym{pdf}{PDF}{Portable Document Format}
\newacronym{svn}{SVN}{Subversion}
\newacronym{wysiwyg}{WYSIWYG}{What You See Is What You Get}

\newglossaryentry{texteditor}
{
	name={editor},
	description={A text editor is a type of program used for editing plain text files.}
}

\chapter{Abstract}


This is where the first chapter begins...


\chapter{Related Publications}

This is where the first chapter begins...

\chapter{Introduction}

\section{Introduction}

%\textbf{Importance of visualisation in the physiology}
%
%Physiology describes how livings organism work, and it spans across multiple scientific fields.
%
%It is therefore important to be able to communicate scientific advances efficiently between experts with various scientific background.
%
%Additionally, there is also a growing interest from the general audience to understand how their own body function.
%
%Explaining such complex processes is usually very challenging without a supporting illustration.
%
%Although some illustrators are still doing hand-made illustrations, many of them are using tools inspired from movie production to accelerate their work.
%
%Despite that, modeling a single scenario is still very time consuming (gathering data and realization).
%
%Moreover, illustrations are usually static, (still images and movies) and do not offer much interactivity.


\textbf{Importance of Visualisation in Biology}


%Printed illustrations have the clear advantage to be easily accessible by the viewer, via a school textooks, scientific magazines, encyclopedia, or simply via the web. 

%To being with this ambitious enterprise it is necessary to starting studying how processes work on the atomic level, the smallest level which we  understand to this day.

%It also exists different types of supports for visual communication other than still images, such as animated movies (animated and non-interactive) and educational games (animated and interactive).

%Due to the popularity of digital effects used in the movie industry over the last decades, 3D animation packages and online training became widely available thus popularizing the use of these tools by scientific illustrators.

%The storytelling is a key element of an illustration as it is responsible of conveying the core message.

%Animated content is very powerful in illustration because it efficiently engages the viewer into the story (stupid comparison of people preferring movies to books).

%Due to the , 3D animation packages and online training became widely available thus popularizing the use of these tools by scientific illustrators.
%which became widely available thanks to the popularity of digital effects used in the movie industry over the last decades

%In order to understand how we work we must first understand the complete cascade of events down from atomic level and up to microscopic and macroscopic levels.

Biochemistry lies at the root of complex physiological processes that allow living organisms to function, and in order to understand how we work we must first understand the complete cascade of events that is happening of the atomic level.
Because physiology spans across multiple scientific fields, it is crucial to communicate advances in biochemistry efficiently between experts with various scientific background.
Moreover, there is also a growing interest from the general audience to understand how their own biological machinery work.

Visual communication is without a doubt a very efficient way to educate a non-expert audience about the functioning of physiological processes.
A quick glance at physiological textbooks is enough to realize that they would be close to useless without any illustrations.
Illustrations, such as the ones made by David Goodsell, are often used to illustrate cellular biology textbooks.
The illustrator likes to depict entire sceneries on the mesoscale levels, such as ...(figure X) that would be impossible to observe otherwise in such detail using current optical instruments.
The work of the illustrator is much appreciated because it exhibits a high level of scientific accuracy and clarity, which makes them accessible to a large audience. 
The realization of such illustrations, however, is very laborious and also demands highly skilled individuals.
The first step of the creation process consists of gathering knowledge from the scientific literature in order to thoroughly understand the process that is to be depicted.
Based on this knowledge, the illustrator will decide how to compose the scene, i.e., which macromolecular structures should be present, where should they be located, in which quantities, and what behaviour should they exhibit.
This task demands a strong understanding of biology as scientific articles are intended to be read by experts and peers.
The second step of the creation process is the drawing. 
It is important not to confuse the work of a scientific illustrator with the work of an artist.
Although they both aim at conveying a message or an idea, the artist has the freedom to hide his message behind an abstraction curtain in order to challenge the viewer.
On the contrary, the task of an illustrator is to make the communication process as clear as possible in order to expose scientific concepts to an uninformed audience.
This concretely means respective a set of logical guidelines in terms of composition, lighting, color-coding and storytelling.

While some illustrators prefer working with paper and pencil or 2D composition software such as Photoshop, the new generation of illustrators grew with 3D rendering and animation packages such as Maya, Cinema4D or Blender, made mainstream by the popularization of digital effects in the movie industry.
The use of such computer aided design tools greatly facilitates the drawing operation of three dimensional data.
Perspective and lighting effects, for instance, are automatically handled by the software leaving artists more time to work on other aspects such as material design, composition and post-processing.
Since less time is spent working on single images it is also less cumbersome to produce animated stories.
Consequently, 3D animation is became increasingly popular among scientific illustrators to depict the machinery of life, as it is a great way to engage the viewer into a storyline.
One notorious animated educational material is "The Inner Life of the Cell" realized in 2008 by the XVIVO medical illustration studio and appointed by Harvard University.
This short animated video beautifully depicts in few sequences the logical cascade of events that describe complex biochemical processes of a living cell. (such as)
The structures of the different actors is based on real structural information available from public databases and their behaviour based on the most recent knowledge of cell biology.
Furthermore, the surrounding of each event is also shown to provide information about the location and scale.
This masterpiece of scientific animation took the whole creation team 14 month to produce, which is a good average production time for a 5 to 10 minutes film of this quality.
Unfortunately, on top of being time consuming, the production of such films is also very costly, which somehow limits the accessibility and availability of such visualization.

%Over the last years, the net revenues of the computer game industry have largely outperformed those of the movie industry, which was not the case a few years ago.
%
%This trend clearly indicates the growing interest of the public in interactive entertainment and  storytelling. 
%
%Moreover, the recent emergence of a new generation of virtual and augmented reality hardware such as the Occulus Rift, HTC Vive, Microsoft Hololens, are now paving the way towards the next generation of computer games which promise to be even more immersive, realistic and engaging.
%
%Given the raising interest of the general audience in this type of interactive experience we can imagine that this could be hugely effective for edutainment and scientific dissemination of cell biology.



Another way to engage the subject even deeper into a scenario is interaction, as in computer games.

By embedding the subject into the presented content, he becomes an actor of the virtual world and his attention naturally increases. 

Immune Attack or Sim Cell are two famous examples of edutainment titles whose goal is to reveal the functioning of living cells through successful accomplishment of actions that are part of the physiological processes.

These titles are based on the traditional reward system present in many computer games that ensures to keep the player engaged with educational content.

The motivation of a player, however, does not have to be solely driven by the high-scores.

We can observe a recent trend among AAA game titles towards interactive storytelling rather than pure gameplay.

Those titles usually feature cinematic sequences, interleaved between interactive sessions where the player has to successfully achieve a task.

The player motivation is then rather driven by his curiosity to unravel the storyline than high-scores.

Given the raising interest of the general audience for interactive storytelling we can foresee that this type of media could have a hugely effective impact on scientific dissemination of cell biology.

The realization of high quality interactive content, however, is usually considered to be quite challenging compared to animated film production.

Indeed, due to fast framerates needed in computer games (20 to 60 fps even more for VR) it is difficult to reproduce entire complex environment we may see in movies, which may affect the overall communication impact. 

Additionally the creation team also requires more people e.g. computer programmers, which may increase the production costs and times. 

Given that the creation of a high quality explanatory video about cell biology may already take up to several months or even years, it is clear that the next generation of interactive scientific illustrations is going to need new efficient tools to:

A) Improve the complexity of the showcased content
B) Assist the content creation by reusing data from computational biology

 improve the content creation.

In order to design new tools to interactively illustrate the machinery of life it is also vital to explore all types of data already available from computational biology and that could help the content creation process.

Furthermore, it is important to improve the real-time rendering capabilities of interactive content creation in order to showcase content with a similar size and compexity and seen in movies.



%In games, this scenario would typically be invented and then manually scripted in the game.
%
%In physiology, however, it exists many processes that are available in computerized form as simulation and which could by used to generate stories on-the-fly.
%
%Additionally, should the simulation run in real-time 

%Unlike with movie, interactive content offer the possibility to explore multiple scenario.




%As the goal is to accurately depict 

%What we need is:
%a. Faster tools to render
%	Rendering several hundreds of thousands of molecular agents can take up to dozens of minutes to render a single image depending of the image quality presets such as resolution or lighting effects.
%
%b. Faster tools to compose
%	Creating large structures requires assembling thousands of proteins or lipids together based on available knowledge present in scientific literature.
%
%c. Faster tools to generate story telling
%	A great part to the creation process is spent understanding a given process in order to depict the clearest visual explanation as possible.
%	Creating an animated scenario for an entire living organisms, such as the E.Coli bacteria would require spending a lot of time collecting, reading and understanding all the physiological processes that allow the cell to function.

%c. Faster tools to animate
%	Animation of a large amount of proteins, requires using physics simulation tools that similarly to rendering software are very time consuming for large scenes.

\section{Background}

\textbf{Computational Biology to Assist Scientific Illustration}

%In biology the cost of wet lab experiments is significantly higher than the cost of computer-based simulations.
%
%Such simulation have a great value because they provide important insight about the functioning of a system. 
%
%Over the last years the use of computer-based simulation have significantly increase in biology.

In biochemistry, it exists two methods to conduct analysis and understand how processes work, called dry and wet laboratories.
Wet laboratories are laboratories where chemical agents are phyiscally manipulated and then observed.
Dry laboratories are laboratories where computational or mathematical methods are employed for the modelling and analysis of biochemical processes. 
Over the last decades, the use of in-silico (dry lab) modeling and analysis has significantly increased due to development new software and the decreasing costs of super computers.
\textit{The goal of a model is to approximate a given process, a complete and accurate model might simply too complex to describe of to simulate, also some aspects may sometimes be unknown}
\textit{Isolate a single part of the entire machinery to easily study its mechanism.}
Because it may often impractical to accurately model a process in its entirety, dry laboratory experiments are often criticized for their lack of precision.
The analysis of such models, although often inaccurate, is nonetheless still quite valuable for researchers.
In 2013 Martin Karplus, Michael Levitt and Arieh Warshel were awarded the Nobel prize in Chemistry for their work of theoretical modelling for complex chemical system.
Their work showcase the importance of theoretical modelling as a tool to complement experimental techniques.
Indeed the collected information is served to formulate new hypothesis which can be later on verified in wet laboratories.
Wet-lab experiments are usually expensive to conduct, and therefore the analysis of theoretical modeling may bring guidance to researchers and save the time and money needed to run too many wet lab experiments.
As a result of the increase in popularity of dry lab experiments, a significant amount of simulation data has been gathered (input) and produced (output).
The data most frequently stored in digital format and can be easily shared to other peers via online databases.
Available data may comprise structural information (how things look) or procedural information (what to they do) for a large number of biochemical agents.
\textbf{What is structure information ?? give concrete example}
\textbf{New mesocale models are being developed to better understand the pysiology of viruses and cells.}
%Right now only static models exists, but thanks to our technology we are paving the way toward interactive visualization of larger dynamic structures.
\textbf{What is procedural information ?? give concrete example}
At this stage they have very limited ways to see how these models of physiology behave. 
Such a visual form is an abstraction that is hard to relate to what is visually observed in experiments. 
In interdisciplinary physiological sciences this might hamper communication of results. 
Based on these two information, however, it would be theoretically possible to digitally produce a visualization that would illustrate a given biochemical process.
This technology could save countless hours of work to illustrators because interesting scenario could be simply queried and fetched from a database and automatically scripted into digital storyline.
Furthermore assuming that the rendering and simulation to be real-time capable, the manipulation of the simulation parameters would provide the means to alter the course of the scenario on-the-fly, thus offering a interactive and engaging experience for the viewer.
\textbf{Add a concrete example to physiological process which could be interesting to interact with}.
This might be the technology that would revolutionize how to effectively communicate how these processes work.
\textit{Biology, medicine, and other sciences can strongly profit from a visualization of physiology in order to gain, verify, and communicate the knowledge and the hypotheses in this field.}
\textbf{A common framework to bridge real-time simulation and meaningful 3D visualization together}

%\textbf{Microscope analogy ??}
%
%\textbf{Mention analogy between the microscope as a tool for viewing experimental data, but the lack of unified tool for viewing theoretical data.}
%
%\textbf{Ant-farm analogy ??}

\subsection{Biological Systems}

\textbf{What is a process ? how to model it ?}

One important aspect to consider if we wish to improve how visual communication is made is the storytelling creation process.
The creation of animated illustration is a very time-consuming enterprise, it demands a lot of knowledge acquisition about physiology to come up with an accurate and satisfying storyboard.
Moreover, this process is not streamlined and it could greatly benefit from better and more automated tools.
For example there are several procedural descriptions of life function and some of these descriptions are also available in computerized form as simulations.
The procedural descriptions provide information about how a given system works and this information is typically used by illustrator to manually animate the cascade of reactions that describe of process (see apoptosis).
However the simulated data contains additionally information about how would the species actually behave throughout time, given a set of initial parameters such as which elements are present, in which quantities and also the reaction rate which may depend on the location or temperature.
We are therefore convinced that efficiently using available data from computational biology could be the key to automatize the challenging task of storyboarding.
The branch of biology which focuses of the study of procedural models is system biology.
System biology models describe a complete or partial \textit{biochemical} biological process in order to simulate and analyse its functioning in more details.
The scientist who simulate those models are interested in studying the evolution in concentration of certain species, and also sometimes their spatial distribution.
Most commonly they use computer-based simulation programs to gauge how those properties change over time.
Based on this information and their own knowledge they are able to formulate hypothesis which helps them to broaden their understanding of the machinery of life. 
The models, as we understand them, describe the different interaction between species, which elements react with each other, what is the product of the reaction, and how fast do they react.
Additionally the concentration of each individual species defines the initial state of the simulation.
%It is common to describe these interactions as a reaction network, called pathway, to visually communicate given processes in biology books for instance.
Complex reaction networks can also be described with a custom markup language (such as SBML) and then stored in a digital file and easily shared with other researchers.
Digital models descriptions are also widely supported by simulation programs to facilitate the initialization procedure.

It exists several simulation methods to compute the number of species present in a system at a given time, they can be classified into two groups according to their modus operanti which can be either deterministic or stochastic.
The deterministic methods relies on the use of differential equation solvers to deduce the concentration at a given time. 
The result depends mostly on initial species concentration and the reaction rates.
This type of simulation was first introduced and still remains very popular among system biologists because it is easy to compute and reliable also.
Another type of simulation method is agent-based modeling.
This computational method quite differs from the strictly mathematical approach, as it aims at reproducing the original reaction-diffusion behaviour of biochemical agents in three dimensional space.
This technology was primarily developed to study and understand complex migration pattern among animal of human population.
This concept was naturally transposed study the behaviour of chemical species as more capable computer hardware became available and affordable.

With agent-based modeling, actors of the studied physiochemical phenomenon are virtually represented as a 3D point in space and subject to constant diffusion motion inspired from the diffusion speeds which was observed in vitro.
New elements are introduced or removed following reaction events, and the scientific analysis are based on the current number of species at a given time.
The reaction between two potential reactant is based on their local proximity and the reaction rate.
When an elements is located closely enough, the system would evaluate the reaction probability based on the reaction rate which was observed in-vitro.
Should a reaction event happen between one, two or more potential actors of a reaction the resulting species of the reaction will be introduced and the reactants removed.
\textbf{(show figure)}
In few cases also, such as modeling of signalling processes, deterministic methods do not produce reliable results because of the very low concentration of reactive agents, and agents-based modeling would be preferred.
However a disadvantage of agent-based modeling, is the complexity of the computation.
While solving a ordinary differential equation system if very straightforward, agent-based models take into account and simulate each single reacting entity.
The results of such simulation can take up to days or week for larger systems, even of super computer and for this reason this methods still remain less popular than the simpler quantitative models.
An advantage of agent-based modelling for automated storytelling however, is the availability of spatial information, which quantitative does not provide.
In the following sections we will introduce two different techniques especially developed for automated storytelling and which make use of simulated data from deterministic and stochastic models, respectively.

\subsection{Biological Structures}

Structural biology is the branch of molecular biology which mainly focuses on studying the structure of proteins and nucleic acids (the form, how they look), and how they acquire their form.
Data acquisition methods such as x-ray crystallography are widely used to read in details the atomic structure of proteins, i.e, the positions or atoms, their type and the type of bonds between them.
Based on this information it is possible to decrypt the sequence of amino acids that forms a protein.
The PDB was created to facilitate the sharing crystallography results of proteins, and now features an impressive collection of protein structures which is easily available to anyone for free.
This information is vital to biologist as it allows them to understand the functioning of proteins, the structure is also to run molecular dynamics, which aims are reproducing in a computer simulation the interactions and forces that drive atom behaviour in reality.
The shapes of proteins are tightly related to their function and therefore accurate atomic structure is also important for scientific illustrators to accurately depict the shapes of proteins.
A few set of tools, (in the form of plugins to amination package) were developed to streamline the illustration process and generate a mesh-based representation simply from a protein description file.
X-ray crystallography is limited in a sense that it is not capable of capturing large and complex structures such as organelles, viruses of cells in their entirety.
Electron microscopy imaging on the other hand still does not offer enough resolution to capture the type of individual atoms which make the segmentation task between proteins extremely challenging.
So far the spatial arrangement of protein that form greater structures such as organelles, viruses and cells was a manual a cumbersome task. 
To fill the mesoscale gap between atomic level data acquisition, and cell level data acquisition, scientists from the Scripps Institute in San Diego have developed new sets of tools that able to procedurally construct large mesoscale structures based on input parameters, such as the shape and location of the compartment, and important information about the proteins that are present such as the shape, the number, the location and the spatial distribution.
Their goal is to generate multi-scale models that could be used an input to run large simulation (MD or BD) for a complete or partial organism.
Additionally the generated structure can also be used to generate accurate 3D models in animation packages, thus preventing illustrator to manually place and arrange proteins when depicting a larger protein ensemble.
This project was a step in the direction of our vision which is to improve how visual communication of molecular biology is done.
However the overwhelmingly large number of elements that compose large mesoscale structures begin to truly challenge animation packages, that were not design with such constraint in mind.
While it is still possible to render still images in very high quality, real-time visualization of these models is challenging, even with simplified protein structures meshes. 
This affects the productivity or those create the models, as well as those who are using it for illustration purposes.
Also it this limitation somehow compromise the transition to the next generation of interactive scientific illustrations for molecular biology as current visualization fail at delivery the minimum requirement in terms of frame rates for showcasing interactive content.
In the following sections we will present two work that aim at enabling real-time rendering and compositing of large mesoscale models using state of the art computer graphics and GPGPU techniques.

\section{Related Work}

Our goal is to improve visual communication of molecular biology, and ultimately to promote interactive showcasing as a standard media for educating the masses about biology.
Valuable data from computation biology is constantly produced by experts and is often publicly available. 
It contains thorough description about how things look (structures) and how things work (processes), and we intend to make extensive use of this data to enable the next generation of scientific illustration.
The visualization literature already comprises a few work that are going in a similar direction and we will distinguish between the related work related to structural visualization in the first section and follow-up with related work about visualization of biochemical processes.

\textbf{Structures}

Structural biology is the branch of molecular biology that focuses on studying the structure of macromolecular elements in order to better understand their function.
Visualization is an important component of this discipline because atoms are arranged and assembled in 3 dimensional space and therefore a visual representation is very often required.
Scientist are using different types of representation to illustrate molecular structures, and we will describe the most commonly used ones.
The simplest representation is the sticks model, where each bonds are represented as a line and color coding is used to indicate the atom type at the line extremities or joints.
A popular representation among structural biologists is the secondary structure or ribbon diagram, it is used to reveal properties of the proteins backbone such as sheets or helices.
The molecular surface representation is used to show a continuous surface that closely surround atoms of a protein and that also close small holes between atoms that are not accessible by small solvent molecules.
This method was first introduced to reveal information that is not salient enough with other types of representation, such as the presence of pockets and cavities buried in the protein structure and that can potentially host important reaction sites.
In popular molecular visualization packages such as VMD[] or PyMol[], molecular surfaces are stored as polygon meshes, for the simple reason that meshes are widely supported by graphics APIs.
Another representation which is commonly understood by a larger audience is the Van der Waals surface, which simply represent atoms as spheres whose radius corresponds to the atomic radius.
Compared to molecular surfaces, the Van der Waals representation provides important information about atomic composition of proteins which is not clearly visible with molecular surfaces.
The Van-der-Waals surfaces are usually represented as meshes, but because the surface features highly detailed asperities it often requires a high polygon count to obtain a decent mesh quality, and which may overload the rendering pipeline when a large number of proteins is displayed.
Molecular surfaces are therefore a commonly used representation among illustrators for depicting large and complex scenes because they can easily import and render meshes with 3D animation packages, and also because surfaces meshes are more lightweight than Van der Waals meshes.
BioBlender[], Molecular Maya[], ePMV[], are examples of plugins for animation packages that were specifically developed to ease the loading and rendering of molecular surface meshes.
To improve rendering performances of highly detailed molecular structures, Tarini et al. introduced a novel visualization technique to render Van der Waals surfaces with a much lower polygon count than with surface meshes.
The rendering method is inspired from 2D billboards, a concept commonly used in computer graphics and games, and consists of drawing camera-facing 2D sphere impostors rather than tessellated 3D spheres when rendering individual atoms, thus considerably reducing the polygon count.
With their approach, they were able to interactively render large datasets (up to xxxk atoms) with very high quality details.
This method inspired several follow-up works that address the challenge of the constantly increasing size of available datasets in molecular biology.
Lampe et al. [8] pioneered real-time rendering of large-scale atomic data on consumer level hardware. 
They extended the fast billboard-based approach introduced by Tarini et al. [19] by leveraging the geometry shader to instantiate entire atom-blocks rather than single atom, thus reducing the memory bandwidth usage and GPU driver overhead by invoking fewer draw operations. 
Grottel at al. proposed to improve the rendering speed of large particle-based datasets by implementing occlusion culling to discard hidden particle chunks from the rendering pipeline, based on the depth information obtained in the previous frame.
Lindow et al. [11] followed-up the work of Lampe et al. by using instanced rendering of entire protein structures instead. 
For each molecule type, they precompute a 3D grid containing the atomic structure and which is stored on the GPU.
Upon the drawing of a protein the bounding box is drawn first, then during the per-fragment operation rays are cast through the grid, in order to find the first atom which is hit by the ray.
Falk et al. [3] further refined the method by introducing depth culling and level-of-detail for the grid structures to achieve faster rendering performance for even larger scenes.
Up to this point, the rendering methods presented in the literature have reached unprecedented levels of performance, in terms of size of supported dataset and rendering speed.
However, the minimum requirement for a comfortable user experience is between 20 and 60 Hz on average games, and higher than 60 Hz for VR content, which is still out of reach for very large datasets (10 billions atoms at 4 fps).
Moreover, the presented solutions were specifically designed for efficient rendering of macro molecules only, such as proteins.
None of the techniques mentioned above have proved to efficiently support other types of molecular structures that exhibit much more complex organization, such as lipid membranes, nucleic acids or fibers, which ought to be taken into account for a truer depiction of molecular landscapes.
Indeed, the properties of these structures can represent a challenge, especially for dynamic data because the assembling blocks or these structures are considerably smaller and also more numerous than with protein data.
Several work focus on the modelling and visualization of such structures, life explorer [] is a visualization software designed for interactive modelling and viewing of DNA strands, lipid wrapper[] is a tool designed to generate lipid membranes based on arbitrary meshes.
However, none of these tools feature a visualization method that has the ambition to support overwhelmingly large datasets such as entire cells and which we deem crucial to achieve interactive and explanatory visualization of molecular biology.

Moreover, macro-molecular elements are usually densely arranged inside compartments (see figure), which may results in serious occlusion problems when interactively exploring a dataset.
This visualization challenge has not yet been explored in the context of molecular visualization although it is crucial to address it to improve the quality of the user experience.

This review of the literature clearly underlines the lack of an efficient and integrated solution that would:
a) Allow real-time rendering of datasets up to the size of a single cell
b) Support all type of molecular structures efficiently
c) Implement efficient occlusion management
d) Provide tools that are embedded with animation or game creation package for optimized collaboration with various actors of the illustration process.

%----------------------
%Several molecular surface computation techniques have been developed, each one with their own drawbacks and advantages.
%SAS[] and Gaussian[] are easy to compute but offer less informative details.
%SES[] and MSS[] are more complex to compute and also provide more surface details which can facilitate the cavity and pocket exploration.
%----------------------

%\textit{and also because smooth surfaces have a much lower overhead than highly tessellated Van-der-Waals surfaces}
%While meshes are often used because of practical reason, they also have a significant computational overhead when featuring a high level of asperity details such as with SES or Van-der-Walls because of the large number of polygon needed to smoothly discretize a complex surface.
%In the visualization literature it exists a few techniques that were developed to overcome this limitation by using a ray-casting approach and grid-based supporting structures.
%Since the recent democratization of domestic and massively parallel computing processors (GPU) it has been possible to implement efficient SES computing and rendering methods [LBPH10,KGE11].
%These methods are able to render the SES for dynamic data sets of around 100k atoms interactively on current hardware.
%Subsequently, Krone et al. implemented a method allowing smooth rendering of Gaussian surfaces for large and dynamic molecular data based on a massively parallel GPU algorithm. 
%%Atoms are stored in a 3D grid, which allows fast accumulation of Gaussian density for each voxel.
%While their technique only supports Gaussian surfaces, which offer less details than SES, they are also able to render much larger dynamic datasets composed of several million atoms.
%Based on the idea that highly detailed molecular surfaces are only needed when closely observing a protein, Parulek et al. introduced the concept of continuous level-of-detail for molecular surfaces.
%They seamlessly transit from highly detailed (SES) to less detailed (Gaussian) surface rendering based on the distance to the camera.
%Because the most complex operation is only performed for a small region of the overall protein, this method provides faster rendering speeds while also reducing visual complexity caused by unnecessary surface details, in the context area.

%The trend in recent molecular surface visualization methods clearly leans towards faster and highly optimized GPU implementations.
%The supporting structures are three dimensional grids because operations performed on such structures can be easily parallelized, thus more efficient.
%The use of uniform spatial partitioning schemes however limits the overall size of the scene, as the complexity and memory usage increases exponentially as the resolution of 3D grid increases, which is why such techniques were not implemented in 3D animation packages and employed by scientific illustrators.
%To overcome this size limitation of the scene, De Hera Chomski[] implemented a CPU-based ray tracing software based on a bounding volume hierarchy (BVH) instead.
%The use of such acceleration structure enable real-time rendering of large and complex scenes, however dynamic data requires constant update of the BVH which adds additional complex work on top of the rendering and limits the real-time capability of the method.

% which is why illustrators often use smooth and simpler surface representations such as the Gaussian instead.

\textbf{Processes}

%Modeling systems biology processes 
% calls for efficient visualization methods to cope with the complexity nature of the data resulting from the simulation.

The principle of systems biology is to reproduce complex physiological processes \textit{in silico} in order to better understand how living organisms work.
Firstly, the models are designed throughout descriptions of molecular interactions that are laid down in 2D reaction networks and digitalized.
Subsequently, the models are used as input for computational simulations and finally, simulation results are used for further analysis and hypothesis verification.
It exists a few popular visualization software that aim at connecting the modeling, simulation, and data analysis in a single framework to facilitate the task of biologists, such as CellDesigner[] TinkerCell[] or VCell[].
These tools usually cover non-spatial models (kinetic modelling), except VCell which also supports the use of external particle-based simulation modules such as Smoldyn[].
Although the simulation data may feature 3D particle trajectories, the framework do not include a visualization module for observing the behaviour of individual 3D particles, simply because it would have only very little value to most experts as they are more interested in studying changes in species concentration.
However, in few specific cases, such as the study of signalling pathways for example, the 3D visualization of particle-based simulation results is thought to be informative to scientists that are interested in studying the spatial distribution of certain species over time.
To a certain extent, this information could also have an informative value for the laymen because it depicts complex processes in the form of a 3D animation, similarly to an explanatory movie.
Specific visualization tools where developed to produce 3D animations out of such particle-based simulations, supporting either playback of pre-computed data [CellBlender][Falk] or in-situ visualization of simulation computed in real-time [ZigCell].
Naive visualization of the raw trajectory data, however, may results in an overly cluttered view due to the large number of elements moving in random directions, and it is therefore crucial to intuitively guide the viewer to ensure a maximum degree of clarity.
Falk et al.[] added to the raw visualization the possibility to track single elements throughout time by tracing the trajectory of a particle and highlight the cascade of reactions which is relevant for the study of signalling pathways.
Although previous works attempted to improve the raw visualization of particle-based simulation with useful overlaid information, there is still left to address the grand challenge which is due to time-scale discrepancies between entire processes and individual reactions, and which is described as follows: 
On the one hand, observing the particle-based simulation at a slow pace to be able to keep track of individual elements and see interactions, would require hours to explore the entire process.
On the other hand, observing the simulation in fast-forward would shorten the viewing length but it would become impossible to observe important reaction events because of the very fast pace at which elements would be moving.
Furthermore, the real-time limitations of particle-based simulation tools still has to be addressed in order to deliver a smooth educational user experience.
Indeed, the simulation of particle-based systems is very demanding in terms of computation and is usually precomputed prior to the visualization, which prohibit online interaction with the simulation parameters and real-time exploration of multiple simulation scenario.
The most-optimized simulation tools leverage the power of GPU computing to speed-up the computation and enable \textit{in-situ} visualization [ZigCell].
However, even most optimized particle-based simulation methods are still not able to compute complex models in a reasonable amount of time to enable a smooth and interactive experience.
Other modeling methods, such as kinetic modeling, do not feature 3D dimensional data but are much faster to compute, and therefore quantitative information could also be used to interactively drive 3D animated particle data for explanatory purposes.

%It exists several modeling techniques that were developed to study the dynamism of a given process.
%
%Kinetic modeling take into account the concentration and reaction rates and predict the number of element present at a given time via numerical integration (ODE).
%
%Although no spatial information is taken into account the variation rates in concentration is very valuable to the biologists.
%
%
%Another type of physiological modeling is agent-based modeling.
%
%Agent-based modeling was initially developed for studying the behaviour of simple thinking digital entities (Animal population growth/decay)
%
%In the biological each agent represent a reactant and has a defined set of possible actions which are defined by the pathway.
%
%Unlike with kinetic modeling, the simulation is based on stochastic rather than determinism.
%
%The whole system is mimicking the reaction-diffusion principle, where each elements are diffusing randomly and occasionally reacting when in contact with a potentinal reaction partner.
%
%This type of simulation are designed to closely resemble how the process actually behaves, and therefore it would be ideal to exploit this type of results and to introduce it in our virtual microscope.
%
%\textbf{Microscopes cannot zoom as deep or slow down time but we potentially could}
%
%From visualizing such system come a few challenges, like exploration or across multiple spatial or temporal scales or occlusion management.
%
%
%
%The computation of agent-based simulation is very slow, the entire simulation must be precomputed prior to the visualization which can take hours or days.
%
%Unfortunately, no interactive change of scenario is possible with this type of simulation methods.
%
%On the other hands kinetic modeling is very fast to compute but does no provide spatial information.



%In biology an increasing number of experiments are conducted in silico because of the gain in time and running costs of in vivo experiements.
%
%Simulation are used to formulate hypothesis.
%Explain the different simulation methods particle based and kinetic models and how do they differ.
%
%The microscope was invented to observe the results of in vivo experiments
%
%\textbf{A microscope can observe cross sections...}
%
%It both shows what how things look and how things work.
%
%The results of computation often need to be visualized in order to be better understood.
%
%Visualization needs a good framework that would serve the same purpose as the microscope.
%
%Computational biology offers currently offer structural and functional information about organisms such as E.Coli.
%
%By integrating the two aspects together it would be possible to emulate processes that are even not observable as in vivo.
%
%With the certain advantage that the viewer would no longer be bound in either space or time. 

%
%\textbf{Procedural information available in public databases (pathway) could be used in conjunction with structural information to automatically generate animated scenario that would save a scientific illustrators a lot of time}.
%
%Rather than taking few months to create a single scenario, illustrators could potentially generate a plethora of scenario simply based on simulated models.
%
%Additionally, if the simulation is fast enough it would also be possible to generate a scenario that would not be predefined and be interactively changed by the viewer.
%
%The key in building the tools for the next generation of biological illustrations is interactivity.
%
%On the one hand, interactivity engages the viewer into a more immersive story, not only by moving around but by also by modifying the course of the story by introducing or removing key elements in the system and observe the consequences.
%
%On the one hand introducing interactive illustrations and thus, developing rendering and simulation technique that could run in real-time also means facilitating the work of scientific illustrators 
%which are currently using tools designed for offline movie creation and are therefore very slow to compute (ray tracing, large physics simulation).


%***************************************************************

%Say that the next step-up in scientific illustration is interaction/real-time graphics.
%
%Talk about the currently existing interactive things (Games, tools, installation, VR/AR).
%
%Talk about the importance of interactivity (Faster content creation, non deterministic story telling).
%
%Talk about the limitation is terms of time needed for asset creation.
%
%Talk about the vision, which is an interactive platform that would utilize the data available to  

%***************************************************************


%-----------------------------------------------

%\textbf{Towards a dynamic and interactive model of a cell (E.Coli)}
%
%\textbf{Requirement analysis}
%
%The heart of this thesis is joining together structural and functional data, and the introduction of interactive techniques for the production of biological illustrations.
%
%We see the introduction of interactive illustration as two fold:
%
%.Gain time in producing illustration (render/animation times)
%.Offer a more engaging user experience (multiple scenario exploration VR)
%
%%Dealing with mutliple local scales
%The challenge in rendering lies in rendering multiple scales when dealing with atomic level data.
%
%%Dealing with mutliple temporal scales
%The challenge in animation lies in how to simultaneously show processes operation at different time scales.
%
%%Dealing with interaction and scenario changes
%The challenge in interaction lies in the fact that simulation of spatial data is very slow
%
%%Dealing with occlusion
%Large amount of data which need to be rendering when observing data o

%-----------------------------------------------

%Illustration help to understand but they are time consuming (gathering data, and realization).
%
%Computer graphics programs already help illustrators to accelerate their work (Maya, Cinema4D)
%
%But there is still a lot of work to do.
%
%Constant improvement in game technologies in often applied across field to accelerate the artists job in the movie industry (real-time simulation, real-time rendering, interaction)
%
%This help scientific illustrators too, but often they are using dedicated tools which are not mainstream and therefore only very few process is made for them.
% 
%Structure:
%
%Dealing with Large Amount of Data 
%
%Dealing with Multiple Scales.
%
%Dealing with Stochasticity / Introducing interaction
%
%Dealing with Occlusion.

%Part C the visualization (occlusion)
%Part E spatial navigation 
%Part F temporal navigation 
%Part D scenario interaction 

\section{Using Computation Data for Storytelling}

\section{Efficient Rendering of Large Structures}

\section{Efficient Spatial Composition and Occlusion Management}




\section{Conclusion}

\chapter{Research Publications}


%
%
%
%
%
%Since \LaTeX\ is widely used in academia and industry, there exists a plethora of freely accessible introductions to the language.
%Reading through the guide at \url{https://en.wikibooks.org/wiki/LaTeX} serves as a comprehensive overview for most of the functionality and is highly recommended before starting with a thesis in \LaTeX.
%
%\section{Installation}
%
%A full \LaTeX\ distribution\index{distribution} consists of not only of the binaries that convert the source files to the typeset documents, but also of a wide range of packages and their documentation.
%Depending on the operating system, different implementations are available as shown in Table~\ref{tab:distrib}.
%\textbf{Due to the large amount of packages that are in everyday use and due to their high interdependence, it is paramount to keep the installed distribution\index{distribution} up to date.}
%Otherwise, obscure errors and tedious debugging ensue.
%
%\section{Editors}
%
%A multitude of \TeX\ \glspl{texteditor} are available differing in their editing models, their supported operating systems and their feature sets.
%A comprehensive overview of \glspl{texteditor} can be found at the Wikipedia page  \url{https://en.wikipedia.org/wiki/Comparison_of_TeX_editors}.
%TeXstudio (\url{http://texstudio.sourceforge.net/}) is recommended.
%
%\section{Compilation}
%
%Modern editors usually provide the compilation programs to generate \gls{pdf} documents and for most \LaTeX\ source files, this is sufficient.
%More advanced \LaTeX\ functionality, such as glossaries and bibliographies, needs additional compilation steps, however.
%It is also possible that errors in the compilation process invalidate intermediate files and force subsequent compilation runs to fail.
%It is advisable to delete intermediate files (\verb|.aux|, \verb|.bbl|, etc.), if errors occur and persist.
%All files that are not generated by the user are automatically regenerated.
%To compile the current document, the steps as shown in Table~\ref{tab:compile} have to be taken.